% Chapter Template

\chapter{Background Information \& Related Work}\label{chap:background_n_related}
% ROUGH DRAFT

James Somers puts down the modern dictionaries by saying \enquote{The definitions are these desiccated little husks of technocratic meaningese, as if a word were no more than its coordinates in semantic space.}~\cite{somers_youre_2014}.
Even though the author criticises the efficient of the dictionary definitions, we will build the thesis on the idea that we can represent senses using their dictionary definitions.

\section{Word Embeddings}%
\label{sec:word_embeddings}

Recent studies have been using word representations, commonly known as \emph{word embeddings}.
Word embeddings are real valued, dense feature vectors for words.
They are induced in order to map a lexicon to a multidimensional latent space.
This representation allows researchers access to the tools of a broad literature in linear algebra and machine learning.
Since the embeddings and their respective words (labels) can be saved to the disk, researchers have been sharing their models on the internet for other researchers to simply download and use them on their own applications.
Word embeddings acquired this way are often called \emph{pre-trained}.

In this section, we will present a brief history of word embeddings.
At the end of the section, we will study our selected model, \emph{fasttext}~\cite{mikolov2018advances}.

Word embeddings is a sprawling subject that has been built upon ideas from probabilistic, statistical and neural network models.
We have omitted approaches that are not used for our study and constrained ourselves only to the literature that lead up to the model we will use. % dilimiz dondugunce

\subsection{History of Word Representations}%
\label{sub:history_of_word_representations}

In order to talk about how words can be mapped to a multidimensional space, first we should talk about how the idea that they can has been theorized.

\subsubsection{Linguistic Background}%
\label{ssub:linguistic_background}

In his \citeyear{harris_distributional_1954} article, \textcite{harris_distributional_1954} introduced his ideas which later came to known as \emph{distributional hypothesis} in the field of linguistics.
He argued that similar words appear within similar contexts.
The famous quote by \textcite{firth_synopsis_1957} captures the idea as;
\enquote{You shall know a word by the company it keeps!}
For instance, the semantic similarity between the terms \emph{jacket} and \emph{coat} can be theoretically proven since they will be accompanied by similar verbs, such as \emph{wear}, \emph{dry clean} or \emph{hang}, and similar adjectives such as \emph{warm} or \emph{leather}.
% is leather an adjective?
However, for a researcher to extract these rules by hand would have been infeasible.

Even though \citeauthor{harris_distributional_1954} argued that \enquote{language is not merely a bag of words}, using unordered collection of word counts to capture the semantic information will be used in the literature and be known as the \emph{bag-of-words} hypothesis.

\subsubsection{Vector Space Model}%
\label{ssub:vector_space_model}

The vector space models first appeared in the information retrieval field.
Initial vector space model developed by \textcite{salton_vector_1975} and presented in \citetitle{salton_vector_1975}.
It was the first application of bag-of-words hypothesis on a corpus to extract semantic information~\cite{turney_frequency_2010}.
\citeauthor{salton_vector_1975} presented the novel idea of a \emph{document space}, consisting of fixed sized vectors as the columns of a term document matrix.
The dimensions of the vectors were the whole vocabulary of the corpus.

In this space, a document $D_i$ is represented using $t$ distinct terms as a row vector;
\begin{displaymath}
    D_{i} = (d_{i1}, d_{i2}, \ldots, d_{it})
\end{displaymath}
The weights for the index terms are calculated by using the \tfidf{} measure introduced by \textcite{jones_statistical_1972}.
\tfidf{} is the multiplication of two metrics;
\begin{description}
    \item[\emph{tf}] the number of times a term $k$ occurs in a document
    \item[\emph{idf}] the inverse of the number of documents that contain $k$.
\end{description}

\citeauthor{salton_vector_1975} presented their particular weighting scheme where the term frequency is multiplied by the following inverse document frequency for the term $k$.
\begin{displaymath}
IDF_{k} = \ceil[\big]{\log_{2}n} - \ceil[\big]{\log_{2}d_{k}} + 1
\end{displaymath}
% simply log_2(n/d_{k})
Where $n$ is the number of documents in the collection and $d_k$ is the number of documents that consists the term $k$.
The weighting scheme was selected to \enquote{assign the largest weight to those terms which arise with high frequency in individual documents, but are at the same time relatively rare in the collection as a whole}.
Finally, they have cast their similarity function between documents $i$ and $j$, as the inner product between their vectors which corresponds to the cosine similarity.

The vector space model allowed \citeauthor{salton_vector_1975} to handle the similarity between documents as the angle between two vectors.
More importantly, they have shown that there is merit to handling documents as real valued vectors.

\subsubsection{Latent Semantic Analysis}%
\label{ssub:latent_semantic_analysis}

\textcite{deerwester_indexing_1990} identified a crucial problem with the vector space model.
According to the author, synonyms and homonyms cannot be handled by the naive term document matrix approach.
Synonymity is an issue because the query might have terms that have the same meaning as the target word.
On the other hand, homonyms might match with an unrelated word.
In order to address these issues, they introduced \emph{latent semantic analysis}.
Dimensionality reduction is used.
Sought higher order latent semantic structure.
For a term document matrix $X$ can be decomposed into;
\begin{displaymath}
    X = T_{0}S_{0}D_{0}'
\end{displaymath}

\begin{displayquote}
Each term or document is then characterized by a vector of weights indicating its strength of association with each of these underlying concepts.
That is, the \enquote{meaning} of a particular term, query, or document can be expressed by $k$ factor values, or equivalently, by the location of a vector in the $k$-space defined by the factors.
\end{displayquote}
They have used the obtained $\hat{X}$ in both document similarity task but more importantly to measure \emph{word similarity}.

So that the words can be represented by much lower dimensionality.
\citeauthor{deerwester_indexing_1990} has reported 100 dimensions.

\citeauthor{lund_producing_1996} provided a method to obtain feature vectors to represent the meaning of words building an co-occurance  information.
Their crucial contribution was to use a window to constrain the co-occurrence information to spatially close terms.

% \textcite{bengio_neural_2000} root word embedding paper.
\textcite{bengio_neural_2000} proposed the first neural network model.
Neural network models will be the centrepiece in word embedding research later.
They wanted to pursue the curse of dimensionality initially because the corpora were getting bigger and term document matrices were \ldots.
They claimed that "we use a continuous real-vector..."

The idea presented by \citeauthor{lund_producing_1996} similar words should have similar feature vectors is presented here.
\citeauthor{lund_producing_1996} has shown this hypothesis with co-occurrence vectors and \citeauthor{bengio_neural_2000} used a distributed feature vector, learned by a probability distribution.

\citeauthor{collobert_unified_2008} suggested a deep neural network model to solve various natural language processing tasks but relevant to our study, have proposed to explicitly learn the feature vectors at the same time.
% something about unsup. learning

They have also used a window that looked ahead and behind of the target word instead of previous methods which have traditionally only looked up to the word, sticking to $P(w_t | w_{t+1})$.
Jointly with \citeauthor{collobert_unified_2008}, \citeauthor{p._turian_word_2010} steered the work on word representations to the today's route.
Distributed word representations.
% TODO write with paper at hand
They showcased that word embeddings can indeed be used as ready made feature vectors and once trained, can be used for other applications by other researchers.
Important to note that they reported training times in the order of days, even weeks.

% \textcite{mikolov_distributed_2013} Word2Vec paper.
Word2Vec by \textcite{mikolov_distributed_2013} brought together the advancements and attractiveness that were brewing in the word embedding research.
First and foremost, they used an efficient loss function for their neural network architecture, the hierarchical softmax. % TODO ref here

With training time under manageable conditions \ldots.
Used negative subsampling, essentially a probability for a word to be discarded by inversely proportional to how frequent it is in the dataset.
Their most famous contribution is the quality of the vectors they have learned.
The theory set out by ? was empirically shown by \citeauthor{mikolov_distributed_2013} by demonstrating that countries and their capital cities exhibited a linear pattern on the PCA.
% TODO simplified graphic here, what is a PCA?

Also element-wise addition in section 5.
% TODO read section 5
They have been hosting their project open source but perhaps more importantly, they published an word2vec pretrained model on English on the internet.
Researchers and industry professionals have been using the embeddings since the semantic similarity between close words were relevant in numerous applications.
%Finally, word2vec lead up to \textcite{bajo}

\section{Bilingual Word Embeddings}%
\label{sec:bilingual_word_embeddings}

\section{Document Retrieval}%
\label{sec:document_retrieval}

\section{Matching}%
\label{sec:matching}

\section{Approaches in Wordnet Generation}%
\label{sec:approaches_in_wordnet_generation}

% from Leveraging Parallel Corpora and Existing Wordnets for Automatic Construction of the Slovene Wordnet
WordNet generation is broken down into 4 categories
\begin{enumerate}
    \item Expand model, \textcite{vossen_introduction_1998}, fixed synsets are translated from English to target language.
    \item Link English entries from machine-readable bilingual dictionaries to English Princeton WordNet senses~\textcite{knight_building_1994}.
    \item Taxonomy parsing~\textcite{farreres_using_1998}.
    \item Ontology matching~\textcite{farreres_towards_2004}
\end{enumerate}

\textcite{gordeev_unsupervised_2018} uses unsupervised cross-lingual embeddings to match cross-lingual product classifications.
Working on taxonomy matching, they use out of domain pre-trained embeddings due to small size of their corpora and investigate methods using untranslated and translated text.

\textcite{lesk_automatic_1986} represent words using their gloss. Relied upon traditional dictionaries.
\textcite{banerjee_adapted_2002} developed on lesk algorithm and included WordNet definitions.
\textcite{khodak_automated_2017} used word embeddings and WordNet.

\textcite{metzler_similarity_2007} talked about short text retrieval and lexical matching. They reported that lexical matching is good for finding semantically identical matches.

\textcite{sagot_building_2008} built a French wordnet.

\textcite{xiao_distributed_2014} another embedding paper.

\textcite{kusner_word_2015} is Word Mover's Distance.

\textcite{balikas_cross-lingual_2018} suggested using optimal transport for cross-lingual document retrieval.

\textcite{arora_simple_2016} simple but tough-to-beat baseline for sentence embeddings.

\textcite{klementiev_inducing_2012} base paper for cross lingual word embeddings?

\textcite{irvine_comprehensive_2017} used as a guideline on best practices.

%\textcite{jonker_shortest_1987} lapjv paper.
