% Chapter Template

\chapter{Background Information \& Related Work}%
\label{chap:background_n_related}

\citeauthor{somers_youre_2014} puts down modern dictionaries in their article \citetitle{somers_youre_2014};
\enquote{The definitions are these desiccated little husks of technocratic meaningese, as if a word were no more than its coordinates in semantic space.}~\cite{somers_youre_2014}.
From the perspective of an author, the efficiency of dictionaries might be worrisome but we will build this thesis on the presumption that dictionary definitions can indeed be represented in some semantic space.

\section{Word Embeddings}%
\label{sec:word_embeddings}

% Surveys used are
% \cite{turney_frequency_2010} -> VSM Survey
% Has the history but omits (rightfully) language modelling stuff (bengio and so on)
% \cite{camacho-collados_word_2018} -> Sense Embeddings Survey (Short section on word embeddings)
% \cite{almeida_word_2019} -> Brasil Survey (Very poorly written, don't want to cite if I can help it)
% \cite{ruder_survey_2017} -> Ruder Survey (Bilingual embeddings, short section on word embeddings)
% http://ruder.io/word-embeddings-1
% Used as 'what to look for', though some have conflicting information

\emph{Word embeddings} are real valued dense vectors for words.
Recently, language modelling studies have been focusing on explicitly learning word embeddings in order to show words or phrases as points on a low dimensional latent space.
On the other hand, earlier research had been obtaining what we can call feature vectors for words while studying natural language processing tasks such as named entity recognition or part of speech tagging~\cite{almeida_word_2019, collobert_unified_2008}.
The vector representation allows researchers access to the tools of the broad literature in linear algebra and machine learning, since they are intuitive for humans to interpret, more so for machines.
Vectors can be compared as a measure of semantic similarity or composed together to build more expansive sentence, paragraph or document representations.

Induced embeddings can be saved to the disk in matrix notation.
Each row is labelled with a token that is denoted by the following $n$ real numbers, as popularized by the open source package \emph{word2vec}.\footnote{\url{https://code.google.com/archive/p/word2vec/}}
Researchers have been sharing their models on the internet so that other researchers can simply download and use them in their own applications.
Word embeddings available in this manner are often called as \emph{pre-trained} models.
Examples of pre-trained word embeddings are \emph{word2vec}, \emph{GloVe}, \emph{Numberbatch} and \emph{fastText}.
% TODO if this line stays you need to cite them

In the following section, we will briefly present the history of word embeddings.
Word embeddings is a sprawling subject that researchers has been building upon using the ideas from probabilistic, statistical and neural network models.
We have omitted crucial contributions that optimized models and brought the literature where it is today due to space constraints, following a path that will lead into the preliminary behind the models we have chosen.

\subsection{History of Word Representations}%
\label{sub:history_of_word_representations}

In order to talk about how words can be mapped to a multidimensional space, we should first talk about how the idea that \emph{they can} has came about.

\subsubsection{Linguistic Background}%
\label{ssub:linguistic_background}

In his \citeyear{harris_distributional_1954} article, \textcite{harris_distributional_1954} introduced his ideas which later came to known as \emph{distributional hypothesis} in the field of linguistics.
He argued that similar words appear within similar contexts.
The famous quote by \textcite{firth_synopsis_1957} captures the idea as;
\enquote{You shall know a word by the company it keeps!}
For instance, the semantic similarity or relatedness between the terms \emph{jacket} and \emph{coat} can be theoretically shown since they will be accompanied by similar verbs, such as \emph{wear}, \emph{dry clean} or \emph{hang}, and similar adjectives such as \emph{warm} or \emph{leather}.

Early attempts to show words on a semantic space is studied in \textcite{osgood_measurement_1957}.
Authors suggested representing concepts using orthogonal scales and relying on human judgement to score meanings on the axes.
An example concept from their study is given in Figure~\ref{fig:early_vectors}.

\begin{figure}[htbp]
    \centering
    \incfig{early_semantic_vectors}
    \caption{Sense representation using human judgement scores for the concept \enquote{Father} adapted from \textcite{osgood_measurement_1957}.}%
    \label{fig:early_vectors}
\end{figure}

However, for a researcher to pick appropriate scales or have meaning extracted by hand would be infeasible for natural language processing tasks~\cite{lund_producing_1996}.

Even though \citeauthor{harris_distributional_1954} argued that \enquote{language is not merely a bag of words}, using unordered collection of word counts to capture the semantic information will be used in the literature and be known as the \emph{bag-of-words} model.

\subsubsection{Vector Space Model}%
\label{ssub:vector_space_model}

The history of word embeddings is tightly coupled with \emph{vector space models} that initially appeared in the field of information retrieval.
The intent was to extract vectors that represented \emph{documents}.
First vector space model developed by \textcite{salton_vector_1975} was presented in \citetitle{salton_vector_1975}.
% multiple sources say it's first, we should be fine
It was the first application of bag-of-words hypothesis on a corpus to extract semantic information~\cite{turney_frequency_2010}.
\citeauthor{salton_vector_1975} presented the novel idea of a \emph{document space}, that consist of fixed sized vectors as the columns of a term document matrix.
The dimensions of the row vectors were the whole vocabulary of the corpus.

In this space, a document $D_i$ is represented using $t$ distinct terms as a row vector;
\begin{displaymath}
    D_{i} = (d_{i1}, d_{i2}, \ldots, d_{it})
\end{displaymath}
The weights for each of the index terms $d_{ix}$ are calculated using the \tfidf{} measure introduced by \textcite{jones_statistical_1972}.
\tfidf{} is the multiplication of two metrics;
\begin{description}
    \item[\emph{tf}] the number of times a term $k$ occurs in a document
    \item[\emph{idf}] the inverse of the number of documents that contain $k$.
\end{description}

% \citeauthor{salton_vector_1975} presented their particular weighting scheme where the term frequency is multiplied by the following inverse document frequency for the term $k$.
% \begin{displaymath}
% IDF_{k} = \ceil[\big]{\log_{2}n} - \ceil[\big]{\log_{2}d_{k}} + 1
% \end{displaymath}
% simply log_2(n/d_{k})
% Where $n$ is the number of documents in the collection and $d_k$ is the number of documents that consists the term $k$.
The weighting scheme was selected to \enquote{assign the largest weight to those terms which arise with high frequency in individual documents, but are at the same time relatively rare in the collection as a whole}~\cite{salton_vector_1975}.
%They have cast their similarity function between documents as the inner product between their vectors which corresponds to the cosine similarity.

The vector space model allowed \citeauthor{salton_vector_1975} to handle the similarity between documents as the angle between two vectors.
Above all, they have shown that there is merit to handling documents as real valued vectors.

\subsection{Latent Semantic Analysis}%
\label{sub:latent_semantic_analysis}

\textcite{deerwester_indexing_1990} introduced latent semantic analysis in order to address a crucial problem with the vector space model.
They have identified that the term document matrix approach cannot handle synonyms and homonyms due to the fact that vector space model requires the words to match exactly between the two documents.
% TODO word overlap, you cited this on chapter 4?
Synonymity is an issue because the query can have terms that have the same meaning as the target word, without getting matched.
On the other hand, homonyms can match with an unrelated word.
In order to answer these issues, their model seeks the higher order latent semantic structure in order to learn the \emph{similarity between words}.

Latent semantic analysis starts with a word co-occurrence matrix $X$.
The terms of the matrix is weighted by some weighting scheme.
While original study used raw term frequencies, \tfidf{} is a possibility while \textcite{levy_improving_2015} reports pointwise mutual information (PMI)~\cite{church_word_1990} as a popular choice.
A term document matrix $X$ is then factorized into three matrices using singular value decomposition~\cite{forsythe_computer_1977};
% People don't cite this for some reason, is it wrong?
\begin{displaymath}
    X = T_{0}S_{0}D_{0}'
\end{displaymath}
Where the columns of $T_{0}$ and $D_{0}'$ are orthogonal to each other and $S_{0}$ is the diagonal matrix of singular values.
The singular values of $S_{0}$ can be ordered by size to keep only the $k$ largest elements, setting others to zero~\cite{deerwester_indexing_1990}.
The resulting matrix is shown as $S$.

\citeauthor{deerwester_indexing_1990} talk about the use for the resulting matrix as follows;
\begin{displayquote}
    Each term or document is then characterized by a vector of weights indicating its strength of association with each of these underlying concepts.
    That is, the \enquote{meaning} of a particular term, query, or document can be expressed by $k$ factor values, or equivalently, by the location of a vector in the $k$-space defined by the factors.
\end{displayquote}
They have used this technique to solve document similarity task and briefly mentioned \emph{word similarity}.
% TODO blockquoting doesn't look nice, paraphrase it
% levy_improving_2015 section 2.2 has a very short and nice explanation if you want
\textcite{landauer_solution_1997} later studied word similarity in full using latent semantic analysis.

\subsection{Building Upon Distributional Hypothesis}%
\label{sub:building_upon_distributional_hypothesis}

% \textcite{schutze_dimensions_1992} (intro to information retrieval co-author) is earlier but doesn't appear in any survey, A Survey on Word Representations of Meaning cites later works (Automatic word sense disambiguation), most just skip to lund
% "in order for the dimensions of meaning and the vector representations of words to reflect closeness in meaning faithfully, a global optimization of cooccurrence constraints is necessary, an operation so complex that only a supercomputer can perform it." that's why
% also used a context size of _characters_ because "few long words are better than many short words"
While \citeauthor{deerwester_indexing_1990} studied relatedness between words using vectors, their approach used the whole document for the co-occurrence information while the focus was clearly still on the document similarity.
\textcite{schutze_dimensions_1992} proposed \enquote{to represent the semantics of words and contexts in a text as vectors} and built upon word co-occurrence.
They theorized a context window of 1000 \emph{characters} in order to consider words that are close to the target word instead of the whole document in co-occurrence calculations.
% They used cosine similarity as a measure of semantic relatedness between word vectors.
However, they claimed that the computation power available was not suitable yet to fully tackle the task.

% Lund is not cited as much, is context window important?
% Why did Lund capture associativity instead of semantic relatedness?
% Collobert! This just predicts the next word, not the job of a word in a text
\textcite{lund_producing_1996} took the challenge and experimented upon 160 million \emph{words} taken from the Usenet, a precursor to the internet.
They used a context window of 10 words and provided a method to obtain feature vectors to represent the meaning of words.
% maybe talk about the method but seems redundant
However, intricate tuning of word co-occurrence generated associatively similar vectors instead of semantically similar ones.
To give an example, \emph{school} is related to \emph{student} while they are not similar.
% Figure 1 of this paper can be adapted here

% Count based approaches to predictive approaches
% Conventional VSM produces high dimensional representations, since the dims. correspond to words in the number can reach millions -> don't know where I got this, can't use it as it probably plagiarism
\subsection{Distributed Vector Representations}%
\label{sub:distributed_vector_representations}
% https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/
% Wish this post had an author, main takeaways;
% The main difference between these various models (neural language models and distributional semantic models) is the type of contextual information they use.
% LSA and topic models use documents as contexts, which is a legacy from their roots in information retrieval.
% Neural language models and distributional semantic models instead use words as contexts, which is arguably more natural from a linguistic and cognitive perspective.
% These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”).
% This very basic difference is too often misunderstood.

% \textcite{bengio_neural_2000} root word embedding paper.
% EVERYONE says they coined 'word embeddings', doesn't appear in the paper, don't know where people got that
% There is another paper called A Neural Probabilistic Language Model, that came out in 2000
% People pretend that it doesn't exist, we'll use the 2003 one as well, it's just the extended version I think

% So far we've been talking about IR, now this is language models
% For instance Turney Pantel (VSM Survey) doesn't mention bengio or any other word embeddings (2010)

% input layer -> emb. layer -> softmax layer
\textcite{bengio_neural_2003} proposed learning word representations using a feedforward neural network.
Their model would learn feature vectors for words using a predictive approach instead of counting based approaches we have presented until now.
Although neural networks have been proposed to learn a language model by \textcite{xu_can_2000}, the main contribution of \citeauthor{bengio_neural_2003} is to use an \emph{embedding layer}, in order to attack \emph{curse of dimensionality}.
For a corpus with vocabulary $V$, there are $|V|$ dimensions for the language model to learn and taking \emph{n-gram} representations into consideration, the problem grows exponentially.
Using $m$ dimensions in the embedding layer allowed \citeauthor{bengio_neural_2003} to represent words using manageable, more representative dimensions and the problem scaled linearly.
% $|V| >> m$

The setup for the neural network starts with the one hot encoded vector representation of the context for a word $w$.
This context window is similar to those used in statistical models that predicts the word $w_t$ using the words that lead up to $w_t$.
In other words, context window of $w_t$ is $T$ words on the left of $w_t$.

% TODO proper reference and check your notation
% this is from Bengio
\begin{displaymath}
    \hat{P}(w_1^T) = \prod_{t=1}^{T}\hat{P}(w_t | w_{1}^{t-1})
\end{displaymath}

The input layer is projected into an embedding layer, later to a softmax layer to get a probability distribution in order to minimize the following cost function.
% TODO check this sentence again, you might be remembering the details wrong
% TODO wrong, there's a tanh layer in the middle too
% http://ruder.io/word-embeddings-softmax/index.html has a beautiful softmax equation
\begin{equation}%
    \label{eqn:bengio_softmax}
    \hat{P}(w_t|w_{t-1}, \dots, w_{t-n+1}) = \frac{e^{y_{w_{t}}}}{\sum_{i}e^{y_{i}}}
\end{equation}
% TODO please check this after you've gotten some sleep, like what is y?
However, this formulation is too expensive computationally since all vocabulary needs to be considered for the sum in the denominator.
The curse of dimensionality problem were shifted to the final layer of the neural network which will be solved later using hierarchical softmax.
% TODO cite hierarchical softmax
% TODO talk about shifting the difficulty elsewhere, instead of full vocabulary dimensions, you just have last layer to worry about
Authors reported training times around 3 weeks using 3 to 5 context window sizes and vocabulary sizes around 17000.

\textcite{collobert_unified_2008} suggested a deep neural network model in order to learn feature vectors for various natural language processing tasks.
Their proposed approach for language model is important for our case since it explicitly learned distributed word representations or simply \emph{word embeddings}.
They have introduced two key ideas;
\begin{itemize}
    \item Instead of using a context window that used words left of the target word to estimate the probability of the target word, they have placed the context window \emph{on} the target window, using $n$ words for left and right of the target word.
    \item They introduced negative examples, where they randomly changed the middle word with a random one. This allowed them to use the ranking cost;
        \begin{displaymath}
            \sum_{s \in S} \sum_{w \in D} \text{max}\big( 0, 1 - f(s) + f(s^w)\big)
        \end{displaymath}
\end{itemize}
\textcite{collobert_unified_2008} claim that these additions allowed their system to learn the representation better rather than the probability.

% Turian 2010 doesn't appear much in any of the surveys or word embedding blogs
% but as far as I can tell, they're the first to say word embeddings are useful because they're off the shelf which I want to emphasize
% Like they literally say "With this contribution word embeddings can now be used off-the-shelf as word features, no tuning"

\subsection{Pre-trained Embeddings}%
\label{sub:pre_trained_embeddings}

\textcite{p._turian_word_2010} evaluated the performance of different word representations as word features you can include into an existing task.
They summarize their contribution as follows;
\begin{displayquote}
    Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner.
    These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. \\
    \textelp{} \\
    With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning.
\end{displayquote}
% TODO paraphrase don't blockquote

% Jointly with \citeauthor{collobert_unified_2008}, \citeauthor{p._turian_word_2010} steered the work on word representations to the today's route.
% They showcased that word embeddings can indeed be used as ready made feature vectors and once trained, can be used for other applications by other researchers.
% Important to note that they reported training times in the order of days, even weeks.

% \textcite{mikolov_distributed_2013} word2vec paper.
% This paper is legendary, talk about skip-gram, hierarchical softmax negative sampling
% Also this paper is cursed and I can't explain it to save my life.
% And the approaches are rabbit holes and I don't have time
% If need be, \cite{goldberg_word2vec_2014} -> word2vec explained

\subsection{Popularization of Word Embeddings}%
\label{sub:popularization_of_word_embeddings}

word2vec package~\cite{mikolov_efficient_2013,mikolov_distributed_2013,mikolov_linguistic_2013} popularized word embeddings.
There are two aspects of the work done by \citeauthor{mikolov_distributed_2013}  that contributed to the fact;
\begin{itemize}
    \item Their model captures the semantic and syntactic attributes of words and phrases on a large scale with good accuracy, trained on billions of words using a shallow neural network, keeping the computational cost down.
    \item They published their code as well as their pre-trained embeddings as an open source project\footnote{\url{https://code.google.com/archive/p/word2vec/}}.
\end{itemize}
The second point is self explanatory but in order to argue about the first one, we should report the algorithms behind word2vec.

The \emph{skip-gram model} introduced by \textcite{mikolov_efficient_2013} differs from the previous methods by predicting the surrounding words given the target word (Figure~\ref{fig:skipgram}).
\begin{figure}[htbp]
    \centering
    \incfig{skip_gram_2}
    \caption{Skipgram architecture courtesy of \textcite{mikolov_distributed_2013}}%
    \label{fig:skipgram}
\end{figure}
In \citetitle{mikolov_distributed_2013}, it is defined as follows;
\begin{equation}
    \frac{1}{T}\sum_{t=1}^{T}\sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j}|w_t)
\end{equation}
%The $w_{t-c}, w_{t-c+1}, \dots, w_{t+c-1}, w_{t+c}$ are the context of the word $w_t$.
The $w_{1}, w_{2}, \dots, w_{T}$ are the context of the word $w_t$.
%It includes $c$ words behind and $c$ words ahead of the word $w_t$.
We should note that, \textcite{levy_improving_2015} has identified that this window size is \emph{dynamic} in the open source implementation of word2vec, where the actual window size is sampled between $1$ and $T$.
%\footnote{Refer to \url{https://github.com/bollu/bollu.github.io} for a comprehensive analysis}
% TODO might not rely on some random guy's repo

% In the Equation~\ref{eqn:bengio_softmax}, we have reported on the \emph{costly} softmax function, used by \citeauthor{bengio_neural_2003}.
% \citeauthor{mikolov_distributed_2013} significantly improves their model's performance by using \emph{hierarchical softmax}.
% Hierarchical softmax~\cite{morin_hierarchical_2005} uses a binary tree to distribute the probability distribution among the leaves of the nodes.
% In short, the costly denominator of the softmax calculation that scaled exponentially with the size of vocabulary $|W|$, it scales in $\log(|W|)$.
% no need to talk about this actually delete later
% TODO paste the negative sampling and subsampling of frequent words to be honest

%The theory set out by ? was empirically shown by \citeauthor{mikolov_distributed_2013} by demonstrating that countries and their capital cities exhibited a linear pattern on the PCA.
% TODO simplified graphic here, what is a PCA -> principal component analysis, t-sne is a good substitue

% http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/
% Ruder says don't dwell on the specific embedding too much
% fasttext is trained on a huge corpus with tuned hyperparameters
% hence the choice of the specific model is not important
\textcite{levy_improving_2015} compared the performance of count based and prediction based word representation models.
Representation algorithms they considered are;
\begin{itemize}
    \item Positive pointwise mutual information (PPMI)~\cite{church_word_1990, bullinaria_extracting_2007}
    \item Singular Value Decomposition on PPMI Matrix (Latent Semantic Analysis)~\cite{deerwester_indexing_1990}
    \item Skip-Gram with Negative Sampling~\cite{mikolov_distributed_2013}
    \item Global Vectors for Word Representation~\cite{pennington_glove_2014}
\end{itemize}
They found out that choice of a particular algorithm played an insignificant role compared to choosing the right approach during training, mainly picking correct \emph{hyperparameters}.
Besides, the amount of data the models are trained on were more important than the model itself.
They used this finding to counter the results reported by \textcite{baroni_dont_2014}.
\citeauthor{baroni_dont_2014} claimed that predictive models outperformed count based models.
On the other hand, \citeauthor{levy_improving_2015} noted that \citeauthor{baroni_dont_2014} used count based models without hyperparameter tuning, denying them from \enquote{tricks} developed in the word representation literature.

\subsection{fastText}%
\label{sub:fasttext}

%Finally, word2vec lead up to \textcite{bajo}
% bag of character n-grams, no need to worry about mophological structure of the language, can induce oov words if parts of it are available,
% Ruder survey demonstrated how good at compositing they are *citation needed*
Armed with the fact that a good word representation model should tune their hyperparameters and should be trained on a large dataset, we set our sights on \emph{fastText}.
On their website, authors define fastText as (a) \enquote{Library for efficient text classification and representation learning}.
The ideas behind it are presented in \textcite{mikolov2018advances}.
Overall, it builds upon word2vec~\cite{mikolov_distributed_2013} by adding position dependent features presented in \textcite{mnih_learning_2013} and character n-grams suggested on \textcite{bojanowski_enriching_2016}.

Let us turn our focus towards \citetitle{bojanowski_enriching_2016}~\cite{bojanowski_enriching_2016}.
Instead of using a context window to learn the representation of the target word or predicting surrounding words given the centre word like in the the skip-gram model, \citeauthor{bojanowski_enriching_2016} learn representations for character n-grams.

\begin{figure}[htbp]
    \centering
    \includegraphics[page=1,width=\textwidth]{Figures/fasttext.pdf}
    \caption{Overview of fastText embedding model}%
    \label{fig:fasttext}
\end{figure}

With the subword vectors $z_g$ for every n-gram $g$ at hand, authors take a dictionary of n-grams of size G and for a given word $w$, they denote $G_{w} \subset {1, \dots, G}$ as the n-grams of $w$.
\begin{displaymath}
    s(w,c) = \sum_{g \in G_{w}}z_{g}^T v_c
\end{displaymath}
% TODO section 3.2 of bojanowski_enriching_2016, it is alright to do this, just reference

\textcite{grave_learning_2018} trained fastText model on different languages using Wikipedia and Common Crawl data.
Wikipedia is a curated encyclopaedia that can be used as a multilingual corpora with 28 languages that have over 100 million tokens and 82 languages with 10 million tokens~\cite{grave_learning_2018}.
Considering the original word2vec was trained on 100 billion tokens, \citeauthor{grave_learning_2018} also used Common Crawl, a non profit project that collocates web pages and publishes the data publicly.
This data can be inadvertently noisy which \citeauthor{grave_learning_2018} addressed using linewise language identification and by removing duplicate lines that often appear as leftover boilerplate on many cites.
While Wikipedia provided them with a curated signal, Common Crawl data helped with capturing as many contexts as possible to train their distributed model.
They are currently hosting the pre-trained word embeddings on their website~\footnote{\url{https://fasttext.cc/}}.

\subsection{ConceptNet Numberbatch}%
\label{sub:conceptnet_numberbatch}

ConceptNet\footnote{\url{http://conceptnet.io/}} is a knowledge graph.
Like the WordNet, it presents semantic relationships between concepts but the defined relationships are more fine grained.
The relationships are collocated from various resources like Wikimedia, English Princeton WordNet and knowledge databases like DBPedia.
%TODO return to this line
ConceptNet stylizes its relations with the \texttt{/r/Relationship} syntax.
For instance, being distinct members of a set relation is defined as \texttt{/r/DistinctFrom} which includes concepts like August and September.\footnote{\url{https://github.com/commonsense/conceptnet5/wiki/Relations}}.
Compared to WordNet, more subjective, human centric relations are defined such as \texttt{/r/MotivatedByGoal}, relationship between \emph{compete} and \emph{win} or \texttt{/r/ObstructedBy}, the relationship between \emph{sleep} and \emph{noise}.
Total number of relations that are available between two concepts is 36.
Moreover, these relationships are \emph{weighted} depending on how present they are.
For example, the concept \emph{run} is related to \emph{fast} with a weight of \texttt{9.42} but the similarity between \emph{race} and \emph{run} is weighted at \texttt{2.54}.
Finally, the ConcepNet is multilingual, encompassing 304 languages in total with 77 of them reported as having moderate support that can be used in downstream tasks.
% TODO NOPE, only 10 is given full approval, Italian is one, bg ro etc. are all lesser languages
Details of the knowledge graph and the procedure is explained by \textcite{speer_conceptnet_2017} in the paper \citetitle{speer_conceptnet_2017}.

\citeauthor{speer_conceptnet_2017} also present their resource in word embedding matrix form for ease of use in the current plug-and-play environment.
In order to get word embeddings from the knowledge graph, first they prepare a symmetric term-term matrix $X$ where an element $w_{i,j}$ is the sum of all edge weights between terms $i$ and $j$.
This is similar to a word co-occurrence matrix but instead of constructing the matrix using unstructured text, this approach builds upon semantic connections between senses.
\citeauthor{speer_conceptnet_2017} reports that their approach learns \emph{relatedness} more so than \emph{similarity} which was reported by the word co-occurrence approaches before.
%TODO cite here lund, deerwester?

With the term-term matrix at hand, \citeauthor{speer_conceptnet_2017} weigh the terms using positive pointwise mutual information (PPMI) as suggested by \textcite{levy_improving_2015} and reduce it to 300 dimensions, as is standard set by word2vec, using truncated SVD\@.
Resulting matrix is similar to approaches set by \textcite{deerwester_indexing_1990} or \textcite{pennington_glove_2014} but \citeauthor{speer_conceptnet_2017} enrich it further using \emph{retrofitting} as proposed by \textcite{faruqui_improving_2014}.
Pre-trained word2vec and GloVe embeddings are incorporated into the reduced term-term matrix to finally obtain embeddings that include both distributional and semantic relatedness signal.
Authors call their finalized model \emph{ConceptNet numberbatch}.
\citeauthor{speer_conceptnet_2017} reports state of the art results compared to word2vec embeddings on word relatedness and models built using their embeddings get scores equivalent to humans on SAT-style analogy tasks.

\section{Bilingual Word Embeddings}%
\label{sec:bilingual_word_embeddings}

Cross lingual embedding models optimize similar objectives.
Only source of variation is due to the data used and the monolingual regularization objectives employed~\cite{ruder_survey_2017}.

\section{Approaches in Wordnet Generation}%
\label{sec:approaches_in_wordnet_generation}

\subsection{History of Wordnet Generation}%
\label{sub:history_of_wordnet_generation}

We have mentioned the lexical database WordNet created by Princeton University.
To reiterate, WordNet is a lexical database with human annotated collection of senses and relationships among them.
The relationships are hierarchical so they can be followed along to reach new nodes due to the transitive property.
The format itself has become the standard for databases that present meanings and concepts~\cite{neale_survey_2018}.

Glosses or the definitions that go along with synsets were not initially part of the WordNet design.
Authors believed that \enquote{definition by synonymity} would be enough.
In other words, definition of a synset can be derived from the lemmas that make up the synset.
As the number of items in the WordNet grew, only then short glosses, followed by longer definitions got included in WordNet~\cite{fellbaum_wordnet_1998}.

\begin{table}[htbp]
    \centering
    \begin{tabulary}{\textwidth}{LL}
        \toprule%
        \textbf{Synset} & \textbf{Gloss} \\
        \midrule%
        \{glossary, gloss\} & an alphabetical list of technical terms in some specialized field of knowledge; usually published as an appendix to a text on that field \\
        \cmidrule(rl){1-2}
        \{dog, domestic dog, Canis familiaris\} & (a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds) \\
        \cmidrule(rl){1-2}
        \{university\} & (the body of faculty and students at a university) \\
        \cmidrule(rl){1-2}
        \{depository financial institution, bank, banking concern, banking company\} & (a financial institution that accepts deposits and channels the money into lending activities) \\
        \bottomrule %
    \end{tabulary}
    \caption{Example synsets and their respective glosses}%
    \label{tab:synset_gloss}%
\end{table}

WordNet has been used in various natural language processing applications over the years such as text summarization~\cite{ercan_using_2007}.
% TODO more usages here
Since the original WordNet was prepared for English over many years of work, efforts for creating an equivalent resource for other languages went under way.
Arguably, EuroWordNet set the standard for creating wordnets for languages other than English~\cite{vossen_introduction_1998, vossen_eurowordnet_2004}.

EuroWordNet\footnote{\url{http://projects.illc.uva.nl/EuroWordNet/}} project was initiated to introduce the benefits of English Princeton WordNet for other languages.
Additionally, an interlinked semantic network can be a research topic on lexicalization patterns of languages, finding conceptual clusters of vocabularies or cross lingual text retrieval~\cite{gonzalo_applying_1998, vossen_introduction_1998}.
EuroWordNet project included 7 wordnets for languages other than English and an adapted English wordnet.
Due to the effort needed to create a wordnet from scratch, \citeauthor{vossen_introduction_1998} averted the EuroWordNet project from creating full scale semantic lexicons and prioritized the connectivity between wordnets.
All in all, \textcite{vossen_introduction_1998} defines the aims as;
\begin{displayquote}
    \begin{enumerate}
        \item to create a multilingual database;
        \item to maintain language-specific relations in the wordnets;
        \item to achieve maximal compatibility across the different resources;
        \item to build the wordnets relatively independently (re)-using existing resources;
    \end{enumerate}
\end{displayquote}

A sense in one language might not have a direct equivalent in an other.
Cultural differences or linguistic differences between languages contribute to this fact~\cite{kitamura_cultural_2009}.
This phenomenon is called a lexical gap or untranslatability.
EuroWordNet addresses lexical gaps using Inter-Lingual-Index (ILI).
ILI is a higher order list of meanings just for wordnet synsets to align themselves to.
Elevating alignment from English Princeton WordNet to ILI allowed language specific structures to exist in wordnets while keeping the connections among themselves.

Two approaches for wordnet generation were defined;
\begin{description}
    \item[Merge Approach] where a wordnet structure is formed in the target language with synset selection and relation mapping. Then the connections between the new wordnet and English Princeton WordNet can be established.
    \item[Expand Approach] where English Princeton WordNet is (machine) translated to target language~\cite{knight_building_1994}, preserving connection information with a trade-off where the target language wordnet will be biased towards the relationships of the English Princeton WordNet and may not include target language specific lexical connections.
\end{description}

In order to maintain as much language specific lexical connections as possible while having a starting point for evaluation of target wordnets, EuroWordNet project offered \enquote{Base Concepts}.
This idea later evolved into 1000 and to 5000 core synsets that are compiled from most frequent, connected and representetive synsets to be used for evaluating wordnet generation~\cite{boyd-graber_adding_2006}.

\subsection{Examples of Wordnet Generation}%
\label{sub:examples_of_wordnet_generation}

Whether an Arabic wordnet is attainable or not using parallel corpora was explored by \textcite{diab_feasibility_2004}.
Arabic differs from other languages explored so far due to its unusual morphological nature.
Using their proposed method, they observed 52.3\% of words they processed are sufficient for a future Arabic wordnet.
They have also reiterated that semantic relationships in a language are transferable to a target language's wordnet.

% \textcite{ruiz-casado_automatic_2005} explored using online encyclopaedia Wikipedia to automatically discover potential synsets using the definitions in the term's pages.
% Just English
Further approaches using parallel corpora to align the target language with English Princeton WordNet were used by \textcite{sagot_building_2008} in order to create a production ready French wordnet and by \textcite{fiser_leveraging_2009} to create a Slovene wordnet.

Approaches that used machine translation to get potential synsets for the target language was explored by \textcite{lam_automatically_2014}.
They proposed two approaches for this task.
First approach uses a single bilingual dictionary to translate English Princeton WordNet lemmas to target language to form synsets.
Second approach translates existing wordnet synsets to English and then translates them to target language.

Our focus is to study approaches that proposed \emph{word embedding} based methods for the task.
\textcite{sand_wordnet_2017} used word embedding models to extend Norwegian wordnet with relationships on existing synsets or by introducing new synsets.
Cosine similarity is used to retrieve nearest neighbours.
Then a threshold value is used to cut off any potential synsets below a value.
They evaluated against accuracy, the percentage of relations that were correct.
Attachment was also used similar to recall scoring.
It is the percentage of considered synsets that were above the threshold value.

%\textcite{khodak_automated_2017} used word embeddings and WordNet.
% talk about Khodak -> Automated WordNet construction Using Word Embeddings
Arguably most relevant to our study, \textcite{khodak_automated_2017} proposed an unsupervised method for automated construction of wordnets.
In their paper \citetitle{khodak_automated_2017} they present 3 approaches and compare their precision, recall and coverage.

First off, they picked 200 French and Russian adjectives, nouns and verbs.
These words are selected based on if they have a translation in the core sense list provided by English Princeton WordNet.
Their approach is presented by handling a word $w$ in the selected list.

Initially, the presented method collects the possible translations of $w$ using a bilingual dictionary and uses them as lemmas to query English Princeton WordNet.
As we have mentioned, lemmas return synsets from English Princeton WordNet in the form \texttt{<lemma.pos.offset>}.
Furthermore, every synset includes possible lemmas that can represent the sense of the synset.
These lemmas form the set $S$.
By translating the English Princeton WordNet lemmas to target language, they obtain the set $T_S$
Elements of $T_S$ and $w$ are both in target language so a monolingual word embeddings can be used;
As a baseline, they calculate the average cosine similarity between word embeddings of every element of $S$ and $w$.
\begin{displaymath}
    \frac{1}{S}\sum_{w' \in S}v_{w} \cdot v_{w'}
\end{displaymath}

%Further, they improve upon the baseline by introducing sentence embeddings~\cite{arora_simple_2016}.
In order to improve the discriminative power of their target vectors, they use synsets relations, definitions and example sentences.
These are used to get a more representative vector $v_{L}$.
Given resources can be thought as sentences and sentence embeddings are calculated by getting a weighted average of word vectors of words that make up the sentence.
The weighting is called smooth inverse frequency by \textcite{arora_simple_2016}.
\begin{displaymath}
    v_{L} = \sum_{w' \in L}\frac{a}{a + P{w'}}v_{w'}
\end{displaymath}
We will talk about sentence embeddings in detail for our approach in Section~\ref{sec:linear_assignment_using_sentence_embeddings}.

Third approach is word sense induction~\cite{arora_linear_2018}.

\begin{enumerate}
    \item Expand model, \textcite{vossen_introduction_1998}, fixed synsets are translated from English to target language.
    \item Link English entries from machine-readable bilingual dictionaries to English Princeton WordNet senses~\textcite{knight_building_1994}.
\end{enumerate}

\textcite{gordeev_unsupervised_2018} uses unsupervised cross-lingual embeddings to match cross-lingual product classifications.
Working on taxonomy matching, they use out of domain pre-trained embeddings due to small size of their corpora and investigate methods using untranslated and translated text.

\textcite{lesk_automatic_1986} represent words using their gloss.
Relied upon traditional dictionaries.

\textcite{banerjee_adapted_2002} developed on lesk algorithm and included WordNet definitions.

\textcite{metzler_similarity_2007} talked about short text retrieval and lexical matching.
They reported that lexical matching is good for finding semantically identical matches.
