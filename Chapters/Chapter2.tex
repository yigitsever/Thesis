% Chapter Template

\chapter{Background Information \& Related Work}%
\label{chap:background_n_related}
% ROUGH DRAFT

James Somers puts down the modern dictionaries by saying \enquote{The definitions are these desiccated little husks of technocratic meaningese, as if a word were no more than its coordinates in semantic space.}~\cite{somers_youre_2014}.
Even though the author criticises the efficient of the dictionary definitions, we will build the thesis on the idea that we can represent senses using their dictionary definitions.

\section{Word Embeddings}%
\label{sec:word_embeddings}

% Surveys used are
% \cite{turney_frequency_2010} -> VSM Survey
% Has the history but omits (rightfully) language modelling stuff (bengio and so on)
% \cite{camacho-collados_word_2018} -> Sense Embeddings Survey (Short section on word embeddings)
% \cite{almeida_word_2019} -> Brasil Survey (Very poorly written, don't want to cite if I can help it)
% \cite{ruder_survey_2017} -> Ruder Survey (Bilingual embeddings, short section on word embeddings)
% http://ruder.io/word-embeddings-1/index.html
% Used as 'what to look for', though some has conflicting information

Recent studies have been using word representations, commonly known as \emph{word embeddings}.
Word embeddings are real valued, dense feature vectors for words.
They are induced in order to map a lexicon to a multidimensional latent space.
This representation allows researchers access to the tools of a broad literature in linear algebra and machine learning.
Since the embeddings and their respective words (labels) can be saved to the disk, researchers have been sharing their models on the internet for other researchers to simply download and use them on their own applications.
Word embeddings acquired this way are often called \emph{pre-trained}.

In this section, we will present a brief history of word embeddings.
At the end of the section, we will study our selected model, \emph{fastText}~\cite{mikolov2018advances}.

Word embeddings is a sprawling subject that has been built upon ideas from probabilistic, statistical and neural network models.
We have omitted approaches that are not used for our study and constrained ourselves only to the literature that lead up to the model we will use. % dilimiz dondugunce

\subsection{History of Word Representations}%
\label{sub:history_of_word_representations}

In order to talk about how words can be mapped to a multidimensional space, first we should talk about how the idea that they can has been theorized.

\subsubsection{Linguistic Background}%
\label{ssub:linguistic_background}

In his \citeyear{harris_distributional_1954} article, \textcite{harris_distributional_1954} introduced his ideas which later came to known as \emph{distributional hypothesis} in the field of linguistics.
He argued that similar words appear within similar contexts.
The famous quote by \textcite{firth_synopsis_1957} captures the idea as;
\enquote{You shall know a word by the company it keeps!}
For instance, the semantic similarity between the terms \emph{jacket} and \emph{coat} can be theoretically proven since they will be accompanied by similar verbs, such as \emph{wear}, \emph{dry clean} or \emph{hang}, and similar adjectives such as \emph{warm} or \emph{leather}.
% is leather an adjective?
However, for a researcher to extract these rules by hand would have been infeasible.

Even though \citeauthor{harris_distributional_1954} argued that \enquote{language is not merely a bag of words}, using unordered collection of word counts to capture the semantic information will be used in the literature and be known as the \emph{bag-of-words} hypothesis.

\subsubsection{Vector Space Model}%
\label{ssub:vector_space_model}

The history of word embeddings is tightly coupled with vector space models.
The vector space models first appeared in the information retrieval field.
Initial vector space model developed by \textcite{salton_vector_1975} and presented in \citetitle{salton_vector_1975}.
It was the first application of bag-of-words hypothesis on a corpus to extract semantic information~\cite{turney_frequency_2010}.
\citeauthor{salton_vector_1975} presented the novel idea of a \emph{document space}, consisting of fixed sized vectors as the columns of a term document matrix.
The dimensions of the vectors were the whole vocabulary of the corpus.

In this space, a document $D_i$ is represented using $t$ distinct terms as a row vector;
\begin{displaymath}
    D_{i} = (d_{i1}, d_{i2}, \ldots, d_{it})
\end{displaymath}
The weights for the index terms are calculated by using the \tfidf{} measure introduced by \textcite{jones_statistical_1972}.
\tfidf{} is the multiplication of two metrics;
\begin{description}
    \item[\emph{tf}] the number of times a term $k$ occurs in a document
    \item[\emph{idf}] the inverse of the number of documents that contain $k$.
\end{description}

\citeauthor{salton_vector_1975} presented their particular weighting scheme where the term frequency is multiplied by the following inverse document frequency for the term $k$.
\begin{displaymath}
IDF_{k} = \ceil[\big]{\log_{2}n} - \ceil[\big]{\log_{2}d_{k}} + 1
\end{displaymath}
% simply log_2(n/d_{k})
Where $n$ is the number of documents in the collection and $d_k$ is the number of documents that consists the term $k$.
The weighting scheme was selected to \enquote{assign the largest weight to those terms which arise with high frequency in individual documents, but are at the same time relatively rare in the collection as a whole}.
Finally, they have cast their similarity function between documents $i$ and $j$, as the inner product between their vectors which corresponds to the cosine similarity.

The vector space model allowed \citeauthor{salton_vector_1975} to handle the similarity between documents as the angle between two vectors.
More importantly, they have shown that there is merit to handling documents as real valued vectors.

\subsubsection{Latent Semantic Analysis}%
\label{ssub:latent_semantic_analysis}

\textcite{deerwester_indexing_1990} introduced latent semantic analysis in order to address a crucial problem with the vector space model.
They have identified that synonyms and homonyms cannot be handled by the naive term document matrix approach due to the fact that vector space model requires the words to match exactly between the two documents.
Synonymity is an issue because the query might have terms that have the same meaning as the target word.
On the other hand, homonyms might match with an unrelated word.
Their model seeks the higher order latent semantic structure in order to learn the \emph{similarity between words}.

Latent semantic analysis starts with a word co-occurrence matrix $X$.
The terms of the matrix is weighted by some weighting scheme.
While original study used raw term frequencies, \tfidf{} is a possibility while \textcite{levy_improving_2015} reports pointwise mutual information (PMI)~\cite{church_word_1990} as a popular choice.
A term document matrix $X$ is then factorized into three matrices using singular value decomposition~\cite{forsythe_computer_1977};
% People don't cite this for some reason, is it wrong?
\begin{displaymath}
    X = T_{0}S_{0}D_{0}'
\end{displaymath}
Where the columns of $T_{0}$ and $D_{0}'$ are orthogonal to each other and $S_{0}$ is the diagonal matrix of singular values.
The singular values of $S_{0}$ can be ordered by size to keep only the $k$ largest elements, setting the others to zero~\cite{deerwester_indexing_1990}.

Their use for the resulting matrix is as follows;
\begin{displayquote}
    Each term or document is then characterized by a vector of weights indicating its strength of association with each of these underlying concepts.
    That is, the \enquote{meaning} of a particular term, query, or document can be expressed by $k$ factor values, or equivalently, by the location of a vector in the $k$-space defined by the factors.
\end{displayquote}
They have used this technique to solve document similarity task and touched upon \emph{word similarity}.
% TODO I can't talk about this paper in a concise way or any way really, return later
% levy_improving_2015 section 2.2 has a very short and nice explanation if you want
Using latent semantic analysis to represent word similarity is later studied by \textcite{landauer_solution_1997}

\subsubsection{Building Upon Distributional Hypothesis}%
\label{ssub:building_upon_distributional_hypothesis}

% \textcite{schutze_dimensions_1992} (intro to information retrieval co-author) is earlier but doesn't appear in any survey, A Survey on Word Representations of Meaning cites later works (Automatic word sense disambiguation), most just skip to lund
% "in order for the dimensions of meaning and the vector representations of words to reflect closeness in meaning faithfully, a global optimization of cooccurrence constraints is necessary, an operation so complex that only a supercomputer can perform it." that's why
% also used a context size of _characters_ because "few long words are better than many short words"
While \citeauthor{deerwester_indexing_1990} studied relatedness between words using vectors, their approach used the whole corpus for the co-occurrence information and the focus was still on the document similarity.

\textcite{schutze_dimensions_1992} proposed \enquote{to represent the semantics of words and contexts in a text as vectors} and built upon word co-occurrence.
They theorized a context window of 1000 characters in order to not consider the whole corpus but only words that are close to the target word to be considered in co-occurrence calculations.
% They used cosine similarity as a measure of semantic relatedness between word vectors.
However, they claimed that the computation power available was not suitable yet to fully tackle the task.

% Lund is not cited as much, is context window important?
% Why did Lund capture associativity instead of semantic relatedness?
% Collobert! This just predicts the next word, not the job of a word in a text
\textcite{lund_producing_1996} took the challenge and experimented upon 160 million words taken from the internet.
They used a context size of 10 words and provided a method to obtain feature vectors to represent the meaning of words.
% maybe talk about the method but seems redundant
However, intricate tuning of word co-occurrence generated associatively similar vectors instead of semantically similar ones.
% Figure 1 of this paper can be adapted here

% Count based approaches to predictive approaches
% Conventional VSM produces high dimensional representations, since the dims. correspond to words in the number can reach millions -> don't know where I got this, can't use it as it probably plagiarism
\subsubsection{Distributed Vector Representations}%
\label{ssub:distributed_vector_representations}
% https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/
% Wish this post had an author, main takeaways;
% The main difference between these various models (neural language models and distributional semantic models) is the type of contextual information they use.
% LSA and topic models use documents as contexts, which is a legacy from their roots in information retrieval.
% Neural language models and distributional semantic models instead use words as contexts, which is arguably more natural from a linguistic and cognitive perspective.
% These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”).
% This very basic difference is too often misunderstood.

% \textcite{bengio_neural_2000} root word embedding paper.
% EVERYONE says they coined 'word embeddings', doesn't appear in the paper, don't know where people got that
% There is another paper called A Neural Probabilistic Language Model, that came out in 2000
% People pretend that it doesn't exist, we'll use the 2003 one as well, it's just the extended version I think

% So far we've been talking about IR, now this is language models
% For instance Turney Pantel (VSM Survey) doesn't mention bengio or any other word embeddings (2010)

% input layer -> emb. layer -> softmax layer
\textcite{bengio_neural_2003} proposed learning word representations using a feedforward neural network.
Their model would learn feature vectors for words using a predictive approach instead of counting based approaches we have presented until now.
Although neural networks have been proposed to learn a language model~\cite{xu_can_2000}, the main contribution of \citeauthor{bengio_neural_2003} is to use an \emph{embedding layer}, in order to attack \emph{curse of dimensionality}.
For a corpus of $V$ words, there are $|V|$ dimensions for the language model to learn and taking \emph{n-gram} representations into consideration, the problem grows exponentially.
Using $m$ dimensions in the embedding layer allowed \citeauthor{bengio_neural_2003} to represent words using manageable dimensions.
% $|V| >> m$

The setup for the neural network starts with the one hot encoded vector representation of the context for a word $w$.
This context is not a window but similar to statistical models that predicts the word $w$ using the words that lead up to $w$, $w_{}$.
% TODO something about statistical models here
The input layer is projected into an embedding layer, later to a softmax layer to get a probability distribution.
% TODO softmax formula here

However, the softmax layer is computationally expensive to be viable.
% TODO why
Authors reported training times around 3 weeks using 3 to 5 context window sizes and vocabulary sizes around 17000.
% TODO talk about shifting the difficulty elsewhere, instead of full vocabulary dimensions, you just have last layer to worry about

\citeauthor{collobert_unified_2008} suggested a deep neural network model in order to learn feature vectors for various natural language processing tasks.
Their proposed approach for language model is important for our case since it explicitly learned distributed word representations or simply word embeddings.
They have introduced two key ideas;
\begin{itemize}
    \item Instead of using a context window that used words left of the target word to estimate the probability of the target word, they have placed the context window \emph{on} the target window, using $n$ words for left and right of the target word.
    \item They introduced negative examples, where they randomly changed the middle word with a random one. This allowed them to use the ranking cost;
        \begin{displaymath}
            \sum_{s \in S} \sum_{w \in D} \text{max}\big( 0, 1 - f(s) + f(s^w)\big)
        \end{displaymath}
\end{itemize}
\citeauthor{collobert_unified_2008} claim that these additions allowed their system to learn the representation better rather than the probability.

% Turian 2010 doesn't appear much in any of the surveys or word embedding blogs
% but as far as i can tell, they're the first to say word embeddings are useful because they're off the shelf which I want to emphasize
% Like they literally say "With this contribution word embeddings can now be used off-the-shelf as word features, no tuning"

\textcite{p._turian_word_2010} evaluated the performance of different word representations as word features you can include into an existing task.
They summarize their contribution as follows;
\begin{displayquote}
  Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner.
  These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems.
  \textelp{} % TODO want this on it's own line
  With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning.
\end{displayquote}

% Jointly with \citeauthor{collobert_unified_2008}, \citeauthor{p._turian_word_2010} steered the work on word representations to the today's route.
% They showcased that word embeddings can indeed be used as ready made feature vectors and once trained, can be used for other applications by other researchers.
% Important to note that they reported training times in the order of days, even weeks.

% \textcite{mikolov_distributed_2013} word2vec paper.
% This paper is legendary, talk about skip-gram, hierarchical softmax negative sampling
% If need be, \cite{goldberg_word2vec_2014} -> word2vec explained

word2vec~\footnote{\url{https://github.com/tmikolov/word2vec}} package~\cite{mikolov_distributed_2013,mikolov_linguistic_2013,mikolov_exploiting_2013} popularized the word embeddings.
There are two aspects of the work done by \citeauthor{mikolov_distributed_2013}  that contributed to the fact;
\begin{itemize}
    \item Their model captures the semantic and syntactic attributes of words and phrases on a large scale with good accuracy, trained on billions of words.
    \item Their code and the pre-trained embeddings have been published as open source.
\end{itemize}

by \textcite{mikolov_distributed_2013} brought together the advancements and attractiveness that were brewing in the word embedding research.
First and foremost, they used an efficient loss function for their neural network architecture, the hierarchical softmax. % TODO ref here

With training time under manageable conditions \ldots.
Used negative subsampling, essentially a probability for a word to be discarded by inversely proportional to how frequent it is in the dataset.
Their most famous contribution is the quality of the vectors they have learned.
The theory set out by ? was empirically shown by \citeauthor{mikolov_distributed_2013} by demonstrating that countries and their capital cities exhibited a linear pattern on the PCA.
% TODO simplified graphic here, what is a PCA?

Also element-wise addition in section 5.
% TODO read section 5
They have been hosting their project open source but perhaps more importantly, they published an word2vec pretrained model on English on the internet.
Researchers and industry professionals have been using the embeddings since the semantic similarity between close words were relevant in numerous applications.

% http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/
% Ruder says don't dwell on the specific embedding too much
% fasttext is trained on a huge corpus with tuned hyperparameters
% hence the choice of the specific model is not important
\textcite{levy_improving_2015} compared the performance of count based and prediction based word representation models.
Representation algorithms they considered are;
\begin{itemize}
    \item Positive pointwise mutual information (PPMI)~\cite{church_word_1990, bullinaria_extracting_2007}
    \item Singular Value Decomposition on PPMI Matrix (Latent Semantic Analysis)~\cite{deerwester_indexing_1990}
    \item Skip-Gram with Negative Sampling~\cite{mikolov_distributed_2013}
    \item Global Vectors for Word Representation~\cite{pennington_glove_2014}
\end{itemize}
They found out that choice of a particular algorithm played an insignificant role compared to choosing the right \emph{hyperparameters}.
They used this finding to counter the results reported by \textcite{baroni_dont_2014} which claimed that predictive models outperformed count based models.
On the other hand, \citeauthor{levy_improving_2015} noted that \citeauthor{baroni_dont_2014} used count based models without hyperparameter tuning, denying them from \enquote{tricks} developed in the word representation literature.

\subsubsection{fastText}%
\label{ssub:fasttext}

%Finally, word2vec lead up to \textcite{bajo}
% bag of character n-grams, no need to worry about mophological structure of the language, can induce oov words if parts of it are available,
% Ruder survey demonstrated how good at compositing they are *citation needed*
Armed with the fact that a good word representation model should have tuned hyperparameters and should be trained on a large dataset, we set our sights on \emph{fastText}~\footnote{\url{https://fasttext.cc/}}.
From their website, fastText is (a) \enquote{Library for efficient text classification and representation learning}.
Ideas behind it are presented in \textcite{mikolov2018advance}.
Overall, it builds upon word2vec~\cite{mikolov_distributed_2013} by adding position dependent features presented in \textcite{mnih_learning_2013} and character n-grams suggested on \textcite{bojanowski_enriching_2016}.
On their website, they are currently hosting pre-trained word embeddings for 157 languages, built from \emph{Common Crawl} and \emph{Wikipedia} data.

\section{Bilingual Word Embeddings}%
\label{sec:bilingual_word_embeddings}

Cross lingual embedding models optimize similar objectives.
Only source of variation is due to the data used and the monolingual regularization objectives employed~\cite{ruder_survey_2017}.

\section{Document Retrieval}%
\label{sec:document_retrieval}

\section{Approaches in Wordnet Generation}%
\label{sec:approaches_in_wordnet_generation}

% from Leveraging Parallel Corpora and Existing Wordnets for Automatic Construction of the Slovene Wordnet
WordNet generation is broken down into 4 categories
\begin{enumerate}
    \item Expand model, \textcite{vossen_introduction_1998}, fixed synsets are translated from English to target language.
    \item Link English entries from machine-readable bilingual dictionaries to English Princeton WordNet senses~\textcite{knight_building_1994}.
    \item Taxonomy parsing~\textcite{farreres_using_1998}.
    \item Ontology matching~\textcite{farreres_towards_2004}
\end{enumerate}

\textcite{sagot_building_2008} built a French wordnet.

\textcite{gordeev_unsupervised_2018} uses unsupervised cross-lingual embeddings to match cross-lingual product classifications.
Working on taxonomy matching, they use out of domain pre-trained embeddings due to small size of their corpora and investigate methods using untranslated and translated text.

\textcite{lesk_automatic_1986} represent words using their gloss.
Relied upon traditional dictionaries.
\textcite{banerjee_adapted_2002} developed on lesk algorithm and included WordNet definitions.
\textcite{khodak_automated_2017} used word embeddings and WordNet.

\textcite{metzler_similarity_2007} talked about short text retrieval and lexical matching.
They reported that lexical matching is good for finding semantically identical matches.

\textcite{xiao_distributed_2014} another embedding paper.

\textcite{kusner_word_2015} is Word Mover's Distance.

\textcite{balikas_cross-lingual_2018} suggested using optimal transport for cross-lingual document retrieval.

\textcite{arora_simple_2016} simple but tough-to-beat baseline for sentence embeddings.

\textcite{klementiev_inducing_2012} base paper for cross lingual word embeddings?
