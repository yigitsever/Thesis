% Chapter Template

\chapter{Background Information \& Related Work}\label{chap:background_n_related}

James Somers puts down the modern dictionaries by saying \enquote{The definitions are these desiccated little husks of technocratic meaningese, as if a word were no more than its coordinates in semantic space.}~\cite{somers_youre_2014}.
From his perspective as an author, the efficient descriptions of the dictionaries is a bother but we will build the thesis on the idea that we can represent words, senses using their dictionary definitions.

% ROUGH DRAFT

\section{Approaches in Wordnet Generation}%
\label{sec:approaches_in_wordnet_generation}

% from Leveraging Parallel Corpora and Existing Wordnets for Automatic Construction of the Slovene Wordnet
WordNet generation is broken down into 4 categories
\begin{enumerate}
    \item Expand model, \textcite{vossen_introduction_1998}, fixed synsets are translated from English to target language.
    \item Link English entries from machine-readable bilingual dictionaries to English Princeton WordNet senses~\textcite{knight_building_1994}.
    \item Taxonomy parsing~\textcite{farreres_using_1998}.
    \item Ontology matching~\textcite{farreres_towards_2004}
\end{enumerate}

\section{Word Embeddings}%
\label{sec:word_embeddings}

Recent literature has been using existing pre-trained word representations, commonly known as \emph{word embeddings}.
Word embeddings are real valued dense feature vectors for words.
They are induced in order to map a lexicon to a multidimensional latent space.
This representation allows researchers access to the tools of a broad literature in linear algebra and machine learning.
Since the embeddings and their accompanying words (labels) can be saved to the disk and shared on the internet, researchers have been using existing (often called pre-trained) models in their own applications by simply downloading them off of the internet.

In this section, we will present a brief history of word embeddings.
We will finish the section by presenting our selected model, \emph{fasttext}~\cite{mikolov2018advances}.
Word embeddings is a sprawling subject that borrowed ideas from probabilistic, statistical and neural network models.
We have omitted approaches that are not used for our study and constrained ourselves only to the literature that lead up to the model we will use. % dilimiz dondugunce

\subsection{History of Word Representations}%
\label{sub:history_of_word_representations}

In order to talk about how words can be mapped to a multidimensional space, first we should talk about how the semantic similarity between words has been theorized.

\subsubsection{Linguistic Background}%
\label{ssub:linguistic_background}

With his \citeyear{harris_distributional_1954} article, \textcite{harris_distributional_1954} introduced the distributional hypothesis for the field of linguistics.
He argued that similar words appear within similar contexts.
For instance, the semantic similarity between the terms \emph{jacket} and \emph{coat} can be empirically proven since they will be accompanied by similar verbs, such as \emph{wear}, \emph{dry clean} or \emph{hang}, and similar adjectives such as \emph{warm} or \emph{leather}.
% is leather an adjective?

\subsubsection{Vector Space Model}%
\label{ssub:vector_space_model}

Using vector representation in natural language processing tasks stemmed from information retrieval and the inception of vector space model can be traced back to \citetitle{salton_vector_1975} by \textcite{salton_vector_1975}.
They presented the novel idea of a \emph{document space}, consisting of fixed sized vectors.
In this space, a document $D_i$ is represented using $t$ number of weights so that;
\begin{displaymath}
    D_{i} = (d_{i1}, d_{i2}, \ldots, d_{it})
\end{displaymath}

Treating documents as vectors and queries as pseudo-vectors was intuitive for both researchers and machines.

Alongside this revolutionary idea, the most important leverage from mathematical vectors have followed.
\citeauthor{salton_vector_1975} suggested using cosine similarity, cast as inner product between two vectors after they are normalized to unit length.
For the weights of the index terms, they identified term frequency with IDF~\cite{jones_statistical_1972}.
% Get idf from turney
%A statistical interpretation of term specificity and its application in retrieval

1 being the smoothing term so that division by zero doesn't occur of tf-idf.
The weighting scheme was selected to that "\ldots will assign the largest weight".

However, one shortcoming was immediately obvious as the theory cemented in practice, using large corpora lead to vocabularies getting larger as well.
\citeauthor{deerwester_indexing_1990} introduced latent semantic analysis to attack the ever-increasing dimensions of vocabulary on their 1990 study.
They have used representations of 100 dimensions.
They start with a term-document matrix.
Then, SVD is applied.

% SVD here

So that the words can be represented by much lower dimensionality.
\citeauthor{deerwester_indexing_1990} has reported 100 dimensions.

\citeauthor{lund_producing_1996} provided a method to obtain feature vectors to represent the meaning of words building an co-occurance  information.
Their crucial contribution was to use a window to constrain the co-occurance information to spatially close terms.

% \textcite{bengio_neural_2000} root word embedding paper.
\textcite{bengio_neural_2000} proposed the first neural network model.
Neural network models will be the centrepiece in word embedding research later.
They wanted to pursue the curse of dimensionality initially because the corpora were getting bigger and term document matrices were \ldots.
They claimed that "we use a continuous real-vector..."

The idea presented by \citeauthor{lund_producing_1996} similar words should have similar feature vectors is presented here.
\citeauthor{lund_producing_1996} has shown this hypothesis with cooccurence vectors and \citeauthor{bengio_neural_2000} used a distributed feature vector, learned by a probability distribution.

\citeauthor{collobert_unified_2008} suggested a deep neural network model to solve various natural language processing tasks but relevant to our study, have proposed to explicitly learn the feature vectors at the same time.
% something about unsup. learning

They have also used a window that looked ahead and behind of the target word instead of previous methods which have traditionally only looked up to the word, sticking to $P(w_t | w_{t+1})$.
Jointly with \citeauthor{collobert_unified_2008}, \citeauthor{p._turian_word_2010} steered the work on word representations to the today's route.
Distributed word reprs.
% TODO write with paper at hand
They showcased that word embeddings can indeed be used as ready made feature vectors and once trained, can be used for other applications by other researchers.
Important to note that they reported training times in the order of days, even weeks.

% \textcite{mikolov_distributed_2013} Word2Vec paper.
Word2Vec by \textcite{mikolov_distributed_2013} brought together the advancements and attractiveness that were brewing in the word embedding research.
First and foremost, they used an efficient loss function for their neural network architecture, the hierarchical softmax. % TODO ref here

With training time under manageable conditions \ldots.
Used negative subsampling, essentially a probability for a word to be discarded by inversely proportional to how frequent it is in the dataset.
Their most famous contribution is the quality of the vectors they have learned.
The theory set out by ? was empirically shown by \citeauthor{mikolov_distributed_2013} by demonstrating that countries and their capital cities exhibited a linear pattern on the PCA.
% TODO simplified graphic here, what is a PCA?

Also element-wise addition in section 5.
% TODO read section 5
They have been hosting their project open source but perhaps more importantly, they published an word2vec pretrained model on English on the internet.
Researchers and industry professionals have been using the embeddings since the semantic similarity between close words were relevant in numerous applications.
%Finally, word2vec lead up to \textcite{bajo}

\section{Bilingual Word Embeddings}%
\label{sec:bilingual_word_embeddings}

\section{Document Retrieval}%
\label{sec:document_retrieval}

\section{Matching}%
\label{sec:matching}

\textcite{gordeev_unsupervised_2018} uses unsupervised cross-lingual embeddings to match cross-lingual product classifications.
Working on taxonomy matching, they use out of domain pre-trained embeddings due to small size of their corpora and investigate methods using untranslated and translated text.

\textcite{lesk_automatic_1986} represent words using their gloss. Relied upon traditional dictionaries.
\textcite{banerjee_adapted_2002} developed on lesk algorithm and included WordNet definitions.
\textcite{khodak_automated_2017} used word embeddings and WordNet.

\textcite{metzler_similarity_2007} talked about short text retrieval and lexical matching. They reported that lexical matching is good for finding semantically identical matches.

\textcite{sagot_building_2008} built a French wordnet.

\textcite{xiao_distributed_2014} another embedding paper.

\textcite{kusner_word_2015} is Word Mover's Distance.

\textcite{balikas_cross-lingual_2018} suggested using optimal transport for cross-lingual document retrieval.

\textcite{arora_simple_2016} simple but tough-to-beat baseline for sentence embeddings.

\textcite{klementiev_inducing_2012} base paper for cross lingual word embeddings?

\textcite{irvine_comprehensive_2017} used as a guideline on best practices.

\textcite{jonker_shortest_1987} lapjv paper.
