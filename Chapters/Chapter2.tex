% Chapter Template

\chapter{Background Information \& Related Work}\label{chap:background_n_related}
% ROUGH DRAFT

James Somers puts down the modern dictionaries by saying \enquote{The definitions are these desiccated little husks of technocratic meaningese, as if a word were no more than its coordinates in semantic space.}~\cite{somers_youre_2014}.
Even though the author criticises the efficient of the dictionary definitions, we will build the thesis on the idea that we can represent senses using their dictionary definitions.

\section{Word Embeddings}%
\label{sec:word_embeddings}

Recent studies have been using word representations, commonly known as \emph{word embeddings}.
Word embeddings are real valued, dense feature vectors for words.
They are induced in order to map a lexicon to a multidimensional latent space.
This representation allows researchers access to the tools of a broad literature in linear algebra and machine learning.
Since the embeddings and their respective words (labels) can be saved to the disk, researchers have been sharing their models on the internet for other researchers to simply download and use them on their own applications.
Word embeddings acquired this way are often called \emph{pre-trained}.

In this section, we will present a brief history of word embeddings.
At the end of the section, we will study our selected model, \emph{fasttext}~\cite{mikolov2018advances}.

Word embeddings is a sprawling subject that has been built upon ideas from probabilistic, statistical and neural network models.
We have omitted approaches that are not used for our study and constrained ourselves only to the literature that lead up to the model we will use. % dilimiz dondugunce

\subsection{History of Word Representations}%
\label{sub:history_of_word_representations}

In order to talk about how words can be mapped to a multidimensional space, first we should talk about how the idea that they can has been theorized.

\subsubsection{Linguistic Background}%
\label{ssub:linguistic_background}

In his \citeyear{harris_distributional_1954} article, \textcite{harris_distributional_1954} introduced his ideas which later came to known as \emph{distributional hypothesis} in the field of linguistics.
He argued that similar words appear within similar contexts.
The famous quote by \textcite{firth_synopsis_1957} captures the idea as;
\enquote{You shall know a word by the company it keeps!}
For instance, the semantic similarity between the terms \emph{jacket} and \emph{coat} can be theoretically proven since they will be accompanied by similar verbs, such as \emph{wear}, \emph{dry clean} or \emph{hang}, and similar adjectives such as \emph{warm} or \emph{leather}.
% is leather an adjective?
However, for a researcher to extract these rules by hand would have been infeasible.

Even though \citeauthor{harris_distributional_1954} argued that \enquote{language is not merely a bag of words}, using unordered collection of word counts to capture the semantic information will be used in the literature and be known as the \emph{bag-of-words} hypothesis.

\subsubsection{Vector Space Model}%
\label{ssub:vector_space_model}

The history of word embeddings is tightly coupled with vector space models.
The vector space models first appeared in the information retrieval field.
Initial vector space model developed by \textcite{salton_vector_1975} and presented in \citetitle{salton_vector_1975}.
It was the first application of bag-of-words hypothesis on a corpus to extract semantic information~\cite{turney_frequency_2010}.
\citeauthor{salton_vector_1975} presented the novel idea of a \emph{document space}, consisting of fixed sized vectors as the columns of a term document matrix.
The dimensions of the vectors were the whole vocabulary of the corpus.

In this space, a document $D_i$ is represented using $t$ distinct terms as a row vector;
\begin{displaymath}
    D_{i} = (d_{i1}, d_{i2}, \ldots, d_{it})
\end{displaymath}
The weights for the index terms are calculated by using the \tfidf{} measure introduced by \textcite{jones_statistical_1972}.
\tfidf{} is the multiplication of two metrics;
\begin{description}
    \item[\emph{tf}] the number of times a term $k$ occurs in a document
    \item[\emph{idf}] the inverse of the number of documents that contain $k$.
\end{description}

\citeauthor{salton_vector_1975} presented their particular weighting scheme where the term frequency is multiplied by the following inverse document frequency for the term $k$.
\begin{displaymath}
IDF_{k} = \ceil[\big]{\log_{2}n} - \ceil[\big]{\log_{2}d_{k}} + 1
\end{displaymath}
% simply log_2(n/d_{k})
Where $n$ is the number of documents in the collection and $d_k$ is the number of documents that consists the term $k$.
The weighting scheme was selected to \enquote{assign the largest weight to those terms which arise with high frequency in individual documents, but are at the same time relatively rare in the collection as a whole}.
Finally, they have cast their similarity function between documents $i$ and $j$, as the inner product between their vectors which corresponds to the cosine similarity.

The vector space model allowed \citeauthor{salton_vector_1975} to handle the similarity between documents as the angle between two vectors.
More importantly, they have shown that there is merit to handling documents as real valued vectors.

\subsubsection{Latent Semantic Analysis}%
\label{ssub:latent_semantic_analysis}

\textcite{deerwester_indexing_1990} introduced latent semantic analysis in order to address a crucial problem with the vector space model.
They have identified that synonyms and homonyms cannot be handled by the naive term document matrix approach due to the fact that vector space model requires the words to match exactly between the two documents.
Synonymity is an issue because the query might have terms that have the same meaning as the target word.
On the other hand, homonyms might match with an unrelated word.
Their model seeks the higher order latent semantic structure in order to learn the \emph{similarity between words}.

Latent semantic analysis starts with a word co-occurrence matrix $X$.
The terms of the matrix is weighted by an arbitrary weighting scheme, such as \tfidf{}, pointwise mutual information\cite{church_word_1990} or raw term frequencies as used in the original study.
% Ruder Survey claims pmi weights, original paper uses raw frequencies, says it'll be chosen per application anyway
A term document matrix $X$ is then factorized into three matrices using singular value decomposition~\cite{forsythe_computer_1977};
% People don't cite this for some reason, is it wrong?
\begin{displaymath}
    X = T_{0}S_{0}D_{0}'
\end{displaymath}
Where the columns of $T_{0}$ and $D_{0}'$ are orthogonal to each other and $S_{0}$ is the diagonal matrix of singular values.
The singular values of $S_{0}$ can be ordered by size to keep only the $k$ largest elements, setting the others to zero~\cite{deerwester_indexing_1990}.

Their use for the resulting matrix is as follows;
\begin{displayquote}
Each term or document is then characterized by a vector of weights indicating its strength of association with each of these underlying concepts.
That is, the \enquote{meaning} of a particular term, query, or document can be expressed by $k$ factor values, or equivalently, by the location of a vector in the $k$-space defined by the factors.
\end{displayquote}
They have used this technique to solve document similarity task and touched upon \emph{word similarity}.
% TODO I can't talk about this paper in a concise way or any way really, return later
Using latent semantic analysis to represent word similarity is later studied by \textcite{landauer_solution_1997}

\subsubsection{Building Upon Distributional Hypothesis}%
\label{ssub:building_upon_distributional_hypothesis}

% \textcite{schutze_dimensions_1992} (intro to information retrieval co-author) is earlier but doesn't appear in any survey, A Survey on Word Representations of Meaning cites later works (Automatic word sense disambiguation), most just skip to lund
% "in order for the dimensions of meaning and the vector representations of words to reflect closeness in meaning faithfully, a global optimization of cooccurrence constraints is necessary, an operation so complex that only a supercomputer can perform it." that's why
% also used a context size of _characters_ because "few long words are better than many short words"
While \citeauthor{deerwester_indexing_1990} studied relatedness between words using vectors, their approach used the whole corpus for the co-occurrence information and the focus was still on the document similarity.

\textcite{schutze_dimensions_1992} proposed \enquote{to represent the semantics of words and contexts in a text as vectors} and built upon word co-occurrence.
They theorized a context window of 1000 characters in order to not consider the whole corpus but only words that are close to the target word to be considered in co-occurrence calculations.
% They used cosine similarity as a measure of semantic relatedness between word vectors.
However, they claimed that the computation power available was not suitable yet to fully tackle the task.

% Lund is not cited as much, is context window important?
\textcite{lund_producing_1996} took the challenge and experimented upon 160 million words taken from the internet.
They used a context size of 10 words and provided a method to obtain feature vectors to represent the meaning of words.
% maybe talk about the method but seems redundant
However, intricate tuning of word co-occurrence generated associatively similar vectors instead of semantically similar ones.
% Figure 1 of this paper can be adapted here

\subsubsection{Neural Network Models}%
\label{ssub:neural_network_models}

% \textcite{bengio_neural_2000} root word embedding paper.
% There is another paper called A Neural Probabilistic Language Model, that came out in 2000
% People pretend that it doesn't exist
\textcite{bengio_neural_2000} proposed the first neural network model.
Neural network models will be the centrepiece in word embedding research later.
They wanted to pursue the curse of dimensionality initially because the corpora were getting bigger and term document matrices were \ldots.
They claimed that "we use a continuous real-vector..."

The idea presented by \citeauthor{lund_producing_1996} similar words should have similar feature vectors is presented here.
\citeauthor{lund_producing_1996} has shown this hypothesis with co-occurrence vectors and \citeauthor{bengio_neural_2000} used a distributed feature vector, learned by a probability distribution.

\citeauthor{collobert_unified_2008} suggested a deep neural network model to solve various natural language processing tasks but relevant to our study, have proposed to explicitly learn the feature vectors at the same time.
% something about unsup. learning

They have also used a window that looked ahead and behind of the target word instead of previous methods which have traditionally only looked up to the word, sticking to $P(w_t | w_{t+1})$.
Jointly with \citeauthor{collobert_unified_2008}, \citeauthor{p._turian_word_2010} steered the work on word representations to the today's route.
Distributed word representations.
% TODO write with paper at hand
They showcased that word embeddings can indeed be used as ready made feature vectors and once trained, can be used for other applications by other researchers.
Important to note that they reported training times in the order of days, even weeks.

% \textcite{mikolov_distributed_2013} Word2Vec paper.
Word2Vec by \textcite{mikolov_distributed_2013} brought together the advancements and attractiveness that were brewing in the word embedding research.
First and foremost, they used an efficient loss function for their neural network architecture, the hierarchical softmax. % TODO ref here

With training time under manageable conditions \ldots.
Used negative subsampling, essentially a probability for a word to be discarded by inversely proportional to how frequent it is in the dataset.
Their most famous contribution is the quality of the vectors they have learned.
The theory set out by ? was empirically shown by \citeauthor{mikolov_distributed_2013} by demonstrating that countries and their capital cities exhibited a linear pattern on the PCA.
% TODO simplified graphic here, what is a PCA?

Also element-wise addition in section 5.
% TODO read section 5
They have been hosting their project open source but perhaps more importantly, they published an word2vec pretrained model on English on the internet.
Researchers and industry professionals have been using the embeddings since the semantic similarity between close words were relevant in numerous applications.
%Finally, word2vec lead up to \textcite{bajo}
% http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/
% Ruder says don't dwell on the specific embedding too much
% fasttext is trained on a huge corpus with tuned hyperparameters
% hence the choice of the specific model is not important


\section{Bilingual Word Embeddings}%
\label{sec:bilingual_word_embeddings}

\section{Document Retrieval}%
\label{sec:document_retrieval}

\section{Matching}%
\label{sec:matching}

\section{Approaches in Wordnet Generation}%
\label{sec:approaches_in_wordnet_generation}

% from Leveraging Parallel Corpora and Existing Wordnets for Automatic Construction of the Slovene Wordnet
WordNet generation is broken down into 4 categories
\begin{enumerate}
    \item Expand model, \textcite{vossen_introduction_1998}, fixed synsets are translated from English to target language.
    \item Link English entries from machine-readable bilingual dictionaries to English Princeton WordNet senses~\textcite{knight_building_1994}.
    \item Taxonomy parsing~\textcite{farreres_using_1998}.
    \item Ontology matching~\textcite{farreres_towards_2004}
\end{enumerate}

\textcite{gordeev_unsupervised_2018} uses unsupervised cross-lingual embeddings to match cross-lingual product classifications.
Working on taxonomy matching, they use out of domain pre-trained embeddings due to small size of their corpora and investigate methods using untranslated and translated text.

\textcite{lesk_automatic_1986} represent words using their gloss. Relied upon traditional dictionaries.
\textcite{banerjee_adapted_2002} developed on lesk algorithm and included WordNet definitions.
\textcite{khodak_automated_2017} used word embeddings and WordNet.

\textcite{metzler_similarity_2007} talked about short text retrieval and lexical matching. They reported that lexical matching is good for finding semantically identical matches.

\textcite{sagot_building_2008} built a French wordnet.

\textcite{xiao_distributed_2014} another embedding paper.

\textcite{kusner_word_2015} is Word Mover's Distance.

\textcite{balikas_cross-lingual_2018} suggested using optimal transport for cross-lingual document retrieval.

\textcite{arora_simple_2016} simple but tough-to-beat baseline for sentence embeddings.

\textcite{klementiev_inducing_2012} base paper for cross lingual word embeddings?

\textcite{irvine_comprehensive_2017} used as a guideline on best practices.

%\textcite{jonker_shortest_1987} lapjv paper.
