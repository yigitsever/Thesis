% Chapter Template
\chapter{Unsupervised Matching}%
\label{chap:unsupervised_matching}

%%% ROUGH FIRST DRAFT

\section{Linear Assignment Using Sentence Embeddings}%
\label{sec:linear_assignment_using_sentence_embeddings}

Using word embeddings to obtain embeddings for longer pieces of text has been studied with implementations like doc2vec~\cite{le_distributed_2014} that builds upon the word2vec~\cite{mikolov_distributed_2013} model in order to learn paragraph embeddings.
However, there is an assumption of a continuous text for the given model.
When the text that we would like to show on a latent space is not part of a longer piece of text but \emph{discrete} pieces, that presumption does not hold.
With the dictionary definitions, we have such a case.
Our dictionary definitions are comprised of 10 to 11 words and there is no relation from one distinct dictionary definition to another.
% TODO reference the wordnet statistics table when you decide on where to put it
In other words, they are not continuous.
One other case where a similar situation occur is \emph{twitter}.
\emph{Tweets} are short pieces of text due to the 280 character constraint imposed by the platform.
With such short pieces of text, instead of paragraph embeddings, we can talk about \emph{sentence embeddings}.
A sentence embedding model should ideally capture the collective meaning of the short text where every word is potentially informative.

\textcite{zhao_ecnu_2015} used two approaches for SemEval-2015 Task 2: Semantic Textual Similarity~\footnote{\url{http://alt.qcri.org/semeval2015/task2/}}.
First, for a sentence $S = (w_{1}, w_{2}, \dots, w_{s})$ where the length of the presumably small sentence is $|S| = s$ and the word embedding of a $w_t$ is $v_t$;
\begin{itemize}
    \item They summed up the word embeddings of the sentence $\sum_{t \in S}v_{t}$
    \item Used information content~\cite{saric_takelab_2012} to weigh each word's LSA vector $\sum_{t \in S} I(w_t) v_{t}$
\end{itemize}
Both approaches results in a vector that is in the same dimensions $R^{d}$ as the original word representations.

\textcite{edilson_a._correa_nilc-usp_2017} expanded upon this simple yet effective idea to tackle the SemEval-2017 Task 4\footnote{\url{http://alt.qcri.org/semeval2017/task4}}, Sentiment Analysis in Twitter.
In order to acquire embeddings that represented \emph{tweets}, they weighed the word embeddings that made up a tweet; $\text{tweet}_i = (w_{i1}, w_{i2}, \dots, w_{im})$ with the \tfidf{} weights.
For the \tfidf{} calculation, they cast individual weights as documents so that term frequency become the term count in a single tweet while document frequency become the number of tweets the term $w_t$ occurs.

We have mentioned that our dictionary definitions are not continuous.
Yet, we advocate using \tfidf{} weights to weigh our word embeddings to get sentence embeddings.
In order to clarify, let us present Table~\ref{tab:en_it_examples}.

\noindent\fbox{%
    \parbox{\textwidth}{%
        % \caption{Example English Princeton WordNet definitions}
        turn red, as if in embarrassment or shame \\
        a feeling of extreme joy \\
        a person who charms others (usually by personal attractiveness) \\
        so as to appear worn and threadbare or dilapidated \\
        a large indefinite number \\
        distributed in portions (often equal) on the basis of a plan or purpose \\
        a lengthy rebuke
    }%
}%
% TODO maybe a fbox with more line width but captions don't work

\begin{table}
    \centering
    \caption{Some definitions from English Princeton WordNet}%
    \label{tab:en_it_examples}
    \begin{tabular}{l}
        \toprule
        turn red, as if in embarrassment or shame \\
        a feeling of extreme joy \\
        a person who charms others (usually by personal attractiveness) \\
        so as to appear worn and threadbare or dilapidated \\
        a large indefinite number \\
        distributed in portions (often equal) on the basis of a plan or purpose \\
        a lengthy rebuke \\
        \bottomrule
    \end{tabular}
\end{table}
% TODO this table is ugly can we do better?

% For sentence embeddings, first a \tfidf{} matrix is constructed.
For the \tfidf{} calculations, we followed a similar approach.
The term frequency is the raw count of a term in a dictionary definition.
While the document frequency is the number of dictionary definitions where $w_t$ occurs.

Then, with the term-embedding matrix at hand, we have calculated definition embeddings using;
\begin{equation}
    S_{\text{emb}}(S) = \sum_{w_{i} \in S} \varB{tf_{w_{i},S}-idf_{w_i}} \cdot Emb_{w}(w_{i})
\end{equation}
Every word that makes up a definition is scaled by its vector in ${\rm I\!R}^n$, then concatenated to form sentence embeddings on ${\rm I\!R}^n$.

% Bipartite graph or matrix
% Give the assumption, defintions are one-to-one
Given the N vectors from source and target language, we hypothesize that there exists a matching where every source definition vector is perfectly mapped to one target vector.
Given that this problem naively iterates over $N!$ matchings, we have looked into an algorithm.

%%% TODO lapjv %%%
% https://blog.sourced.tech/post/lapjv/
%
\section{Jonker Volgenant Algorithm}%
\label{sec:jonker_volgenant_algorithm}
% Ok jonker volgenant is super complicated
% can I just say I'm using linear assignment? maybe talk about hungarian algortihm a bit

\section{Results}%
\label{sec:results}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \multicolumn{3}{c}{Percentage of Correctly Matched Definitions} \\
\cmidrule(lr){2-4}
        \textbf{Language} & \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
        \midrule
        bg & 0.39 & 0.41 & 0.19 \\
        el & 0.37 & 0.38 & 0.14 \\
        it & 0.28 & 0.28 & 0.36 \\
        ro & 0.39 & 0.39 & 0.20 \\
        sl & 0.15 & 0.15 & 0.06 \\
        sq & 0.55 & 0.54 & 0.27 \\
        \bottomrule
    \end{tabular}
    \caption{Linear Assignment Using 2000 Definitions}%
    \label{tab:lapjv_2000}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \multicolumn{3}{c}{Percentage of Correctly Matched Definitions} \\
\cmidrule(lr){2-4}
        \textbf{Language} & \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
        bg & 0.35 & 0.36 & 0.18 \\
        el & 0.36 & 0.36 & 0.12 \\
        it & 0.25 & 0.25 & 0.32 \\
        ro & 0.36 & 0.37 & 0.19 \\
        sl & 0.11 & 0.11 & 0.05 \\
        sq & 0.39 & 0.40 & 0.19 \\
        \bottomrule
    \end{tabular}
    \caption{Linear Assignment Using 3000 Definitions}%
    \label{tab:lapjv_3000}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
\midrule
        Best & 0.55 & 0.54 & 0.36 \\
        Worst & 0.11 & 0.11 & 0.05 \\
        Average & 0.33 & 0.33 & 0.19 \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Linear Assignment}%
    \label{tab:lapjv_summary}
\end{table}
