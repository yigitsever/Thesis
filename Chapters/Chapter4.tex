% Chapter Template

\chapter{Dictionary Alignment as Pseudo-Document Retrieval}%
\label{chap:retrieval}

Document retrieval is the prototypical information retrieval task.
\textcite{bush_as_1945} theorized the possibilities of the automatic information retrieval by machines in his essay titled \citetitle{bush_as_1945}.
On his \citetitle{singhal_modern_2001}, \textcite{singhal_modern_2001} gives due credit to \textcite{luhn_statistical_1957} for the initial suggestion of using word overlap in document retrieval.

Modern information retrieval techniques are out of the scope of this thesis, our short definitions are arguably not even documents.
Yet, we can still benefit from the tried and tested methods of the early information retrieval.
Considering the small collection of documents at hand, first we will investigate if we can handle the task using approaches that were available to the researchers when the size of corpora that were available to them were small as well~\cite{singhal_modern_2001}.

\section{Google Translate Monolingual Baseline}%
\label{sec:monolingual_retrieval}

First of all, by collecting English Princeton WordNet definitions and target wordnet definitions, we created a corpora suitable for the task by translating the target language's definitions to English using Google Translate.
We used the Google Cloud API\footnote{\url{https://cloud.google.com/translate}} in order to automate the process.

\begin{table}[htbp]
    \centering
    \begin{tabulary}{\textwidth}{LR}
        \toprule%
        \textbf{Original Definition} & \textbf{Translated Definition} \\
        \midrule%
        bila pri starih Grkih in Rimljanih vojna ladja s tremi vrstami vesel & with the ancient Greeks and Romans, a three-wheeled war ship was happy \\
        \cmidrule(rl){1-2}
        znanstvena veja matematike, ki se ukvarja s prostorskimi lastnostmi teles in njihovimi medsebojnimi odnosi & the scientific branch of mathematics, which deals with the spatial properties of the bodies and their interrelations \\
        \cmidrule(rl){1-2}
        circumstanțe, stări de fapte, lucruri luate în calcul într + o discuție sau dezbatere & circumstances, facts, things taken into account in a discussion or discussion \\
        \cmidrule(rl){1-2}
        Fază a lunii în care este complet iluminată & Phase of the month in which it is fully illuminated \\
        \cmidrule(rl){1-2}
        Persoană care se ocupă cu pescuitul și uneori cu conservarea peștelui pescuit & A person who deals with fishing and sometimes with the conservation of fish fishing \\
        \cmidrule(rl){1-2}
        E bëj diçka që të shkojë e të përputhet me një qëllim të caktuar, i bëj ndryshimet e ndreqjet e nevojshme, që t'u përgjigjet kushteve e rrethanave të caktuara. & I do something to go and match a certain purpose, make the necessary adjustments and adjustments, to respond to certain conditions and circumstances. \\
        \cmidrule(rl){1-2}
        pohoj, a paraqit dikujt një gjë për ta parë, për t'u njohur me të a për të gjykuar për të; zbuloj diçka që të shihet; tregoj & I assure you, did someone present a thing to see, to know him or to judge him? I find something to be seen; show \\
        \bottomrule %
    \end{tabulary}%
    \caption{Example definitions and translations}%
    \label{tab:google_translate_lol}
\end{table}

Table~\ref{tab:google_translate_lol} presents some definitions and their automated translations to English.
While the translations can inform the user about the general sense of the word, sentence structure is flawed at times.
We can also see word duplication when synonyms collapse into single words.
These shortcomings are also reported by \textcite{groves_friend_2015}.

With the English Princeton WordNet definitions and 6 target wordnet definitions at hand, we can handle the task as \emph{monolingual document retrieval}.
As usual, we present our methods with a single wordnet which is repeated for 6 wordnets trivially.
We will use the vector space model that was mentioned in Chapter~\ref{chap:background_n_related} to have real valued vectors that represent the definitions.

\subsection{Term Document Matrix}%
\label{sub:term_document_matrix}

With the corpora of two wordnets in English, we have can use a single vocabulary $V$.
With the definition collection and the vocabulary at hand, we create a \emph{term-document} matrix.
In this instance of the term-document matrix, rows represent the documents and columns denote individual vocabulary entries.
Such a matrix $C$ is $m$ by $n$ while $m$ is the number of documents and $n$ is the size of the vocabulary $|V|$.
Our definitions are used as short documents so we use the terms \emph{definition} and \emph{document} interchangeably with the added benefit of using $d$ to denote either of them.

For any element of $C$, $c_{i,j}$ is the number of times $j$ occurs in the document $i$.
This matrix is sparse since documents use a fraction of the available vocabulary $V$.
Our case with definitions is no different with an average of 10.62 words per definition with vocabulary sizes well over 10,000.

\subsection{Term Weighting}%
\label{sub:term_weighting}

Elements of a term-document matrix is scaled in order to assign weights to elements according to their discriminative power~\cite{manning_introduction_2009}.
Stopwords like \enquote{the}, \enquote{and} have virtually no significance and terms that are common in the corpora's domain will have no discriminating power either~\cite{manning_introduction_2009}.

\textbf{What is a good scaling method for term weighing?}
\tfidf{} is composed of two scores; term frequency \emph{tf} and inverse document frequency \emph{idf}.
\emph{idf} was initially suggested by \textcite{jones_statistical_1972} and is highlighted by \textcite{manning_introduction_2009}.
Term frequency $tf_{w,d}$ is the number of times term $w$ occurs in a document $d$.
Inverse document frequency $idf_{w}$ is calculated using the number of documents that contain the term $w$ which is also known as the document frequency of $w$, $df_w$~\cite{manning_introduction_2009}
We have used the smooth variant of idf as follows;
\begin{equation}
    idf_w = \log{\frac{N + 1}{df_w + 1}}
\end{equation}
Where $N$ is the number of documents in the corpus. $\log(\cdot)$ is used to dampen the effect of the $idf$.
Using \emph{idf} ensures that terms that appear rarely have a bigger influence and coupled with \emph{tf}, the highest weights are given to rare terms when they appear numerous times in a document.
On the other hand, terms that appear commonly in documents or even in every document lose their influence~\cite{manning_introduction_2009}.
By weighing our term-document matrix $C$, for any term $x_{i,j}$
\begin{equation}
x_{i,j} = \text{tf}_{td} \cdot \text{idf}_{t} \end{equation}
term $x_{i,j}$ shows the importance of term $t$ with relation to its general significance throughout the corpus.
Our choice of \tfidf{} weighting is further motivated by \textcite{ruizcasadoAutomatic2005} in which they claim superior results in discovering correct wordnet definitions using \tfidf{} weights.

\subsection{Similarity Measure}%
\label{sub:similarity_measure}

With our term-document matrix ready, similarity between two documents can be calculated by assigning a scoring scheme that uses document vectors.
\emph{Cosine similarity} is a measure that assigns to find the cosine of the angle between two vectors as their similarity score and scales to multidimensional case trivially.
Moreover, since the majority of the dimensions between the vectors are zero, cosine similarity ignores these dimensions and implicitly considers the non-zero dimensions.

For the row vector $\vec{d_t}$ and $\vec{d_p}$, cosine similarity between definitions $t, p$ is
\begin{equation}
    \cos(\theta) = \text{sim}(d_t, d_p) = \frac{\vec{d_t} \cdot \vec{d_p}}{|\vec{d_t}||\vec{d_p}|}
\end{equation}

\subsection{Retrieval}%
\label{sub:retrieval}

With definition vectors and a similarity measure at hand, we split the \tfidf{} weighted term-document matrix $C'$ as English Princeton WordNet definitions and target wordnet definitions.
The target wordnet definitions were used as pseudo-queries and English Princeton WordNet definitions were used as the document collection to retrieve definitions from.
In Figure~\ref{fig:term_document}, we show the overview of the process.
As we have mentioned in Chapter~\ref{chap:unsupervised_matching}, the golden corpora we use includes the same number of definitions on both collections.
This number is shown as $k$ in Figure~\ref{fig:term_document}.

\begin{figure}[htbp]
    \centering
    \incfig{term_document}
    \caption{Term document matrix to queries and documents}%
    \label{fig:term_document}
\end{figure}

In order to retrieve and rank definitions given the query definition, cosine similarity between query and each corpus definition is calculated.
The similarity scores that range between 0 and 1 are sorted in descending order with the definition index attached.
The retrieved documents are ranked according to their similarity score.
Since the definitions are aligned across wordnets, a correctly retrieved definition will have the same index as the query definition.
By sorting the retrieved indices of the definitions, we have a ranking system.

\subsection{Evaluation}%
\label{sub:monolingual_evaluation}

Information retrieval literature usually uses \emph{precision} and \emph{recall} as the evaluation metric with the assumption that the user wants to receive as many relevant documents as possible~\cite{salton_state_1992}.
However, we only have one correct definition we are interested in and cannot access relevance any further for any other retrieved definitions anyway.
The only information our corpora provides is that \emph{correct} definition to retrieve has the same index as the query.
So, we used a tweaked version of \emph{Mean reciprocal rank} (MRR) as introduced by \textcite{voorhees_trec-8_1999}.
In their work, mean reciprocal rank was used in order to evaluate a question answering track where a singular correct answer was sought among several answers and is rewarded according to the rank of the correct item.
In other words, mean reciprocal rank evaluates a system's retrieval score with respect to the rank of a single correct answer, averaged over all the queries.
Considering that we have $k$ definitions that we cast as queries, the MRR is calculated as

\begin{equation}\label{eq:mrr}
    \text{MRR} = \frac{1}{k} \sum_{q = 1}^{k} \frac{1}{R(q)}
\end{equation}

Where rank$_q$ is the ranking of the \emph{correct} definition for the query $q$.

\begin{table}[htbp]
    \centering
    \begin{tabulary}{\textwidth}{LR}
        \toprule%
        \textbf{English Princeton WordNet Definition} & \textbf{Translated Definition} \\
        \midrule%
        ancient Greek or Roman galley or warship having three tiers of oars on each side & with the ancient Greeks and Romans, a three-wheeled war ship was happy \\
        \cmidrule(rl){1-2}
        the pure mathematics of points and lines and curves and surfaces & the scientific branch of mathematics, which deals with the spatial properties of the bodies and their interrelations \\
        \cmidrule(rl){1-2}
        everything stated or assumed in a given discussion & circumstances, facts, things taken into account in a discussion or discussion \\
        \cmidrule(rl){1-2}
        the time when the Moon is fully illuminated & Phase of the month in which it is fully illuminated \\
        \cmidrule(rl){1-2}
        someone whose occupation is catching fish & A person who deals with fishing and sometimes with the conservation of fish fishing \\
        \cmidrule(rl){1-2}
        make fit for, or change to suit a new purpose & I do something to go and match a certain purpose, make the necessary adjustments and adjustments, to respond to certain conditions and circumstances. \\
        \cmidrule(rl){1-2}
        admit (to a wrongdoing) & I assure you, did someone present a thing to see, to know him or to judge him? I find something to be seen; show \\
        \bottomrule %
    \end{tabulary}%
    \caption{English Princeton WordNet definitions and the target wordnet definitions we want to match}%
    \label{tab:pwn_translated}
\end{table}

% temporary
% Following the monolingual retrieval, we leveraged the power of word embeddings to include the semantic similarity between words.
% A famous example for the inadequacy of \tfidf{} is illustrated by~\cite{kusner_word_2015}.
% For two snippets of text; \emph{Obama speaks to the media in Illinois} and \emph{The President greets the press in Chicago} Kusner argues that while they convey the same information, they would be near orthogonal in a bag of words setting.
% Yet before moving forward with WMD, we wanted to test sentence embeddings.

\section{Cross Lingual Document Retrieval}%
\label{sec:cross_lingual_document_retrival}

In this section, we will talk about the methods that allowed us to use bilingual word embeddings to retrieve definitions while keeping them in their own language.

\subsection{Word Movers Distance}%
\label{sub:word_movers_distance}

Following the popularization of the word2vec~\cite{mikolovDistributed2013}, \textcite{kusner_word_2015} introduced \emph{word mover's distance}.
In order to explain the algorithm, first we should talk about the motivation behind it.

We have studied bag of words representation for document retrieval tasks, using \tfidf{} weights.
\citeauthor{kusner_word_2015} gives a brilliant example where relying on word overlap falls short;

\begin{tabular}{l l}
    \midrule
    Obama speaks to the media in Illinois & The President greets the press in Chicago \\
    \midrule
\end{tabular}

After the stop words are removed, the resulting tokens \{Obama, speaks, media, Illinois\} and \{President, greets, press, Chicago\} have no overlap so their vector representations would be orthogonal, showing maximum dissimilarity.
Yet they convey the same meaning semantically.
By calculating the distances between individual words of documents, it can be proven that the case above is basically the same sentence.
Using this case as their motivation, \citeauthor{kusner_word_2015} adapted the optimal transport theorem~\cite{kantorovitch_translocation_1958}, an optimization technique like the one discussed in Chapter~\ref{chap:unsupervised_matching}.
This theorem deals with the energy cost of transporting one arbitrary mass to another, often explained with transporting piles of dirt analogy.
Their idea starts by casting documents as probability distributions defined over word embeddings of the words they are written with.
First, words of a document is thought as the elements of a document's probability distribution.
We now know that relying on sameness or word overlap is not feasible so the words are instead represented by their $d$ dimensional word embeddings.
The distance between two words can be represented by the Eucledian distance between them over $k$ dimensions, or as the distance metric $c(\cdot)$;
\begin{displaymath}
    c(w_a, w_b) = || w_a - w_b ||_2
\end{displaymath}
\textcite{kusner_word_2015} calls this the travel cost between word $w^a$ and $w^b$.
Now that the distance or the cost between words can be shown, two documents $d_i$ and $d_j$, respectively written using words such that $d_i = \{w^{i}_1, w^{i}_2 \dots w^{i}_n\}$ and $d_j = \{w^{j}_1, w^{j}_2 \dots w^{j}_n\}$, a \emph{flow matrix} can be formulated using the distances between every $w^{i}_x$ and $w^{j}_y$ pair.
This flow matrix is called $T$ and an element $T_{i,j}$ is how much of the distance needs to be travelled when starting from word $i$ to reach word $j$ in order to move $d_i$ to $d_j$.
The $T$ is optimized to find the minimum distance possible between $d_i$ and $d_j$ using linear programming.
Mainly, the following constraints are solved in order to find the $T$ with the least overall sum~\cite{kusner_word_2015};

\begin{gather}
    \sum_{i,j}^{n}T_{i,j}c(i,j) \\
    \sum_{j=1}^{n}T_{i,j} = d_i \forall i \in {1, \dots, n} \\
    \sum_{i=1}^{n}T_{i,j} = d'_j \forall j \in {1, \dots, n}
\end{gather}

This optimization is solved by \emph{choosing} which word pairs to use to travel from one document to another and when cast over a whole corpus.
Similar documents will have lower minimal costs on their $T$ matrices depending if their words are easily \emph{moveable} across each other.

\subsection{Cross Lingual Word Mover's Distance}%
\label{sub:clemd}

\textcite{balikasCrosslingual2018} extended the word mover's distance with two additions.
First, instead of normalized bag of words representation for the documents, they used term frequency and \tfidf{} to weigh the document representations.
Second, they introduced \emph{entropic regularization} which allowed them to use Sinkhorn-Knopp algorithm~\cite{sinkhorn_concerning_1967} to solve the linear assignment between word embeddings of the source and the target documents.

We have modified their open source project\footnote{\url{https://github.com/balikasg/WassersteinRetrieval}} to accompany our task.
% For document $d^{i}$ in language $i$, written using words $w_{x}^{i} \subset d^{i}$ which belong to vocabulary $V^{i}$.
% The same principle extends to language $j$ such that document $d^{j}$ is built with using $w_{x}^{j}$ that belong to $V^{j}$.
% Word embeddings of two languages that share a space is used such that $v_{w_{x}^{i}} \in W^{i}$ and $v_{w_{x}^{j}} \in W^{j}$.
% By using word embeddings that were mapped to the same latent space, they achieved cross lingual document retrival.

\subsection{Evaluation}%
\label{sub:cldr_evaluation}

In order to evaluate cross lingual pseudo document retrieval approaches, mean reciprocal rank that was presented in Equation~\ref{eq:mrr} is reported here as well.
Mean reciprocal rank gives an overall insight about the performance of the retrieval system by giving penalty to cases that retrieved the correct definition in a lower rank.
However, in the context of dictionary alignment, percentage score of \emph{precision at one} is also reported since in a real life application, retrieving the correct definition on the top spot is more important.
