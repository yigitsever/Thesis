% Chapter Template

\chapter{Dictionary Alignment as Pseudo-Document Retrieval}%
\label{chap:retrieval}

Document retrieval is the prototypical information retrieval task.
\textcite{bush_as_1945} theorized the possibilities of the automatic information retrieval by machines in his essay titled \citetitle{bush_as_1945}.
On his \citetitle{singhal_modern_2001}, \textcite{singhal_modern_2001} gives due credit to \textcite{luhn_statistical_1957} for the initial suggestion of using word overlap in document retrieval.
% this can be expanded or moved to chapter 2

Modern information retrieval techniques are far from the scope of this thesis.
Yet, we can still benefit from the tried and tested methods of the early information retrieval.
Considering the small collection of documents at hand, first we will investigate if we can handle the task using approaches that were available to the researchers when the size of corpora that were available to them were small as well~\cite{singhal_modern_2001}.
However, we will get leverage from a state of the art tool from the modern computer science; \emph{Google Translate}.
% then, we will look into cldr

\section{Monolingual Retrieval}

First of all, we created a corpora suitable for the task by translating the target language's definitions to English using Google Translate.
We used the Google Cloud API\footnote{\url{https://cloud.google.com/translate}} in order to automate the process.
% TODO when the preparing the wordnets section is ready, refer there from here

% TODO figure out how to render bulgarian and greek text here and add them
\begin{table}[htbp]
    \centering
    \begin{tabulary}{\textwidth}{LR}
        \toprule%
        \textbf{Original Definition} & \textbf{Translated Definition} \\
        \midrule%
        bila pri starih Grkih in Rimljanih vojna ladja s tremi vrstami vesel & with the ancient Greeks and Romans, a three-wheeled war ship was happy \\
        \cmidrule(rl){1-2}
        znanstvena veja matematike, ki se ukvarja s prostorskimi lastnostmi teles in njihovimi medsebojnimi odnosi & the scientific branch of mathematics, which deals with the spatial properties of the bodies and their interrelations \\
        \cmidrule(rl){1-2}
        circumstanțe, stări de fapte, lucruri luate în calcul într + o discuție sau dezbatere & circumstances, facts, things taken into account in a discussion or discussion \\
        \cmidrule(rl){1-2}
        Fază a lunii în care este complet iluminată & Phase of the month in which it is fully illuminated \\
        \cmidrule(rl){1-2}
        Persoană care se ocupă cu pescuitul și uneori cu conservarea peștelui pescuit & A person who deals with fishing and sometimes with the conservation of fish fishing \\
        \cmidrule(rl){1-2}
        E bëj diçka që të shkojë e të përputhet me një qëllim të caktuar, i bëj ndryshimet e ndreqjet e nevojshme, që t'u përgjigjet kushteve e rrethanave të caktuara. & I do something to go and match a certain purpose, make the necessary adjustments and adjustments, to respond to certain conditions and circumstances. \\
        \cmidrule(rl){1-2}
        pohoj, a paraqit dikujt një gjë për ta parë, për t'u njohur me të a për të gjykuar për të; zbuloj diçka që të shihet; tregoj & I assure you, did someone present a thing to see, to know him or to judge him? I find something to be seen; show \\
        \bottomrule %
    \end{tabulary}%
    \caption{Example definitions and translations}%
    \label{tab:google_translate_lol}
\end{table}

Table~\ref{tab:google_translate_lol} presents some definitions and their automated translations to English.
While the translations can inform the user about the general sense of the word, sentence structure is flawed at times.
We can also see word duplication when synonyms collapse into single words.
These shortcomings are also reported by \textcite{groves_friend_2015}.

With the English Princeton WordNet definitions and 6 target wordnet definitions at hand, we can handle the task as \emph{monolingual document retrieval}.
% We have presented the preliminary knowledge for the vector space representation in Chapter~\ref{chap:background_n_related}.
We will use the vector space model that was mentioned in Chapter~\ref{chap:background_n_related} to have real valued vectors that represent the definitions.

To begin with, the definitions that were extracted from English Princeton WordNet and the target wordnet have been collocated to form a corpora.
This process includes one target wordnet at a time and experiments were simply repeated for each wordnet.
The vocabulary of this corpora is shared between the wordnet pairs as they have been joined in a single language.

\subsection{Term Document Matrix}%
\label{sub:term_document_matrix}

With the definition collection and the vocabulary at hand, we created a \emph{term-document} matrix.
In a term-document matrix, rows represent the documents and columns denote individual vocabulary entries.
Such a matrix $C$ is $m$ by $n$ while $m$ is the number of documents and $n$ is the size of the vocabulary $|V|$.
Our definitions are just short documents so we use the terms \emph{definition} and \emph{document} interchangeably with the added benefit of using $d$ for both of them.

For any element of $C$, $C_{i,j}$ is the number of times $j$ occurs in the document $i$.
This matrix is sparse since documents use a fraction of the available vocabulary $V$.
Our case with definitions is no different with an average of 10.62 words per definition.

\subsection{Term Weighting}%
\label{sub:term_weighting}

Elements of a term-document matrix is scaled in order to assign weights to elements according to their discriminative power~\cite{manning_introduction_2009}.
Stopwords like \enquote{the}, \enquote{and} have virtually no significance and terms that are common in the corpora's domain will have no discriminating power either, as explained by \textcite{manning_introduction_2009}.
We have opted out of stopword removal considering our short definitions.

What is a good scaling method for term weighing?
\tfidf{} was initially suggested by \textcite{jones_statistical_1972} and is highlighted by \textcite{manning_introduction_2009}.
% TODO idf was suggested by jones_statistical_1972 not tfidf
% \cite{robertson_understanding_2004} can be cited for full tfidf
\tfidf{} is composed of two scores; term frequency \emph{tf} and inverse document frequency \emph{idf}.
Term frequency $tf_{w,d}$ is the number of times term $w$ occurs in a document $d$.
Inverse document frequency $idf_{w}$ is calculated using the number of documents that contain the term $w$ which is also known as the document frequency of $w$, $df_w$.
We have used the smooth variant of idf as follows;
\begin{equation}
    idf_w = \log{\frac{N + 1}{df_w + 1}}
\end{equation}
Where $N$ is the number of documents in the corpus. $\log(\cdot)$ is used to dampen the effect of the $idf$.

\citeauthor{manning_introduction_2009} explains the intuition behind the \tfidf{} in his \citetitle{manning_introduction_2009} as follows;
\begin{displayquote}
    \textelp{}, tf-idf$_{t,d}$ assigns to term $t$ a weight in document $d$ that is
    \begin{enumerate}
        \item highest when $t$ occurs many times within a small number of documents (thus lending high discriminating power to those documents);
        \item lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal);
        \item lowest when the term occurs in virtually all documents.
    \end{enumerate}
\end{displayquote}
By weighing our term-document matrix $C$, for any term $x_{i,j}$
\begin{equation}
x_{i,j} = \text{tf}_{td} \cdot \text{idf}_{t} \end{equation}
Such that term $x_{i,j}$ shows the importance of term $t$ with relation to its general significance throughout the corpus.

% textcite{ruiz-casado_automatic_2005} says tfidf + dot product was the best for their task

\subsection{Similarity Measure}%
\label{sub:similarity_measure}

Using the term-document matrix, similarity between two documents can be calculated by assigning a scoring scheme that uses rows as document vectors.
\emph{Cosine similarity} is such a measure that can be used to find the cosine of the angle between two vectors and scales to multidimensional case trivially.
Moreover, since the majority of the dimensions between the vectors are zero, cosine similarity ignores these dimensions and implicitly prioritizes the non-zero dimensions.
% TODO think about this / citation needed

For the row vector $\vec{d_t}$ and $\vec{d_p}$, cosine similarity between definitions $t, p$ is
\begin{equation}
    \cos(\theta) = \text{sim}(d_t, d_p) = \frac{\vec{d_t} \cdot \vec{d_p}}{|\vec{d_t}||\vec{d_p}|}
\end{equation}

\subsection{Retrieval}%
\label{sub:retrieval}

With definition vectors and a similarity measure at hand, we split the term-document matrix $C$ as English Princeton WordNet definitions and target wordnet definitions.
The target wordnet definitions were used as pseudo-queries and English Princeton WordNet definitions were used as the document collection to retrieve definitions from.
% TODO maybe a graphic here or one of those cool pseudo-codes
In order to retrieve and rank definitions given the query definition, cosine similarity between query and each corpus definition is calculated.
The similarity scores that range between 0 and 1 are sorted in descending order with the definition index attached.
The retrieved documents are ranked according to their similarity score.
Since the definitions are aligned across wordnets, a correctly retrieved definition will have the same index as the query definition.

\subsection{Evaluation}%
\label{sub:evaluation}

Information retrieval literature usually uses \emph{precision} and \emph{recall} as the evaluation metric with the assumption that the user wants to receive as many relevant documents as possible~\cite{salton_state_1992}.
However, we only have one correct definition we are interested in and cannot access relevance any further for any other retrieved definitions anyway.
As a result, we used a tweaked version of \emph{Mean reciprocal rank} (MRR) as introduced by \textcite{voorhees_trec-8_1999} in \citetitle{voorhees_trec-8_1999}.
In the report, mean reciprocal rank was used in order to evaluate a question answering track where a singular correct answer was sought among other answers and is rewarded according to the returned rank.
In other words, mean reciprocal rank evaluates a system's retrieval score with respect to the rank of a single correct answer. % cares about only the single highest ranked term
\begin{equation}
    \text{MRR} = \frac{1}{|Q|} \cdot \sum_{q}^{|Q|} \frac{1}{\text{rank}_{q}}
\end{equation}
% TODO only wikipedia has a equation and I can't cite wikipedia and I can't write an equation that doesn't resemble wikipedia's
Where rank$_q$ is the ranking of the \emph{correct} definition for the query $q$.

\begin{table}[htbp]
    \centering
    \begin{tabulary}{\textwidth}{LR}
        \toprule%
        \textbf{English Princeton WordNet Definition} & \textbf{Translated Definition} \\
        \midrule%
        ancient Greek or Roman galley or warship having three tiers of oars on each side & with the ancient Greeks and Romans, a three-wheeled war ship was happy \\
        \cmidrule(rl){1-2}
        the pure mathematics of points and lines and curves and surfaces & the scientific branch of mathematics, which deals with the spatial properties of the bodies and their interrelations \\
        \cmidrule(rl){1-2}
        everything stated or assumed in a given discussion & circumstances, facts, things taken into account in a discussion or discussion \\
        \cmidrule(rl){1-2}
        the time when the Moon is fully illuminated & Phase of the month in which it is fully illuminated \\
        \cmidrule(rl){1-2}
        someone whose occupation is catching fish & A person who deals with fishing and sometimes with the conservation of fish fishing \\
        \cmidrule(rl){1-2}
        make fit for, or change to suit a new purpose & I do something to go and match a certain purpose, make the necessary adjustments and adjustments, to respond to certain conditions and circumstances. \\
        \cmidrule(rl){1-2}
        admit (to a wrongdoing) & I assure you, did someone present a thing to see, to know him or to judge him? I find something to be seen; show \\
        \bottomrule %
    \end{tabulary}%
    \caption{English Princeton WordNet definitions and the target wordnet definitions we want to match}%
    \label{tab:pwn_translated}
\end{table}

\subsection{Results}%
\label{sub:results}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrrrr}
        \toprule%
        Language & MRR & Top 10 & Top 10 \% & Top 1 & Top 1 \% \\
        \midrule%
        bg & 0.087 & 674 & 0.337 & 403 & 0.202 \\
        el & 0.122 & 1006 & 0.503 & 709 & 0.355 \\
        it & 0.016 & 476 & 0.238 & 250 & 0.125 \\
        ro & 0.051 & 988 & 0.494 & 728 & 0.364 \\
        sl & 0.073 & 584 & 0.292 & 317 & 0.159 \\
        sq & 0.056 & 979 & 0.490 & 767 & 0.384 \\
        \bottomrule
    \end{tabular}
    \caption{Experiment results for monolingual retrieval}%
    \label{tab:monolingual_tfidf}
\end{table}

% temporary
% Following the monolingual retrieval, we leveraged the power of word embeddings to include the semantic similarity between words.
% A famous example for the inadequacy of \tfidf{} is illustrated by~\cite{kusner_word_2015}.
% For two snippets of text; \emph{Obama speaks to the media in Illinois} and \emph{The President greets the press in Chicago} Kusner argues that while they convey the same information, they would be near orthogonal in a bag of words setting.
% Yet before moving forward with WMD, we wanted to test sentence embeddings.

\section{Cross Lingual Document Retrival}%
\label{sec:cross_lingual_document_retrival}

\subsection{Word Movers Distance}%
\label{sub:word_movers_distance}

% ROUGH DRAFT
Following the popularization of the word2vec~\cite{mikolov_distributed_2013}, \textcite{kusner_word_2015} introduced \emph{Word Mover's Distance}.
By adapting the optimal transport theorem into using distances between word embeddings and documents as probability distributions that are composed of words, he suggested the following;

For an embedding matrix $X \in R^{n \times d}$ $d$ is the dimension the embedding was learned at and $n$ is the size of the vocabulary.
% TODO paper is d times n, I always worked with n times d (rows are word embeddings), is that a problem?
In this configuration, a row $\vec{x_i} \in R^d$ of the matrix $X$ is the embedding for the $i^{\text{th}}$ word $w_i$.
% TODO we have a nth package use it

Given a a separate term-document matrix $C \in R^{m \times n}$, a row $\vec{d_j} \in R^n$ is the row vector for the $j^{\text{th}}$ document $d_j$.
In the original paper \citetitle{kusner_word_2015}, the elements of the term-document matrix $C$ are given as normalized bag-of-words representations.
Given the $i^{\text{th}}$ word in a document;
\begin{equation}
    d_i = \frac{tf_{}}{\sum_{j = 1}^{n}tf_{j}}
\end{equation}
Where tf$_{i,j}$ is the term frequency or the number of times term $i$ appears in document $j$.

We have studied the \tfidf{} weighted case for using term-document matrices for document retrieval tasks.
\citeauthor{kusner_word_2015} gives a brilliant example on a case where word overlap scoring nature of term-document matrices fall short;
\begin{displayquote}
    Recall the earlier example of two similar, but word-different sentences in one document: \enquote{Obama speaks to the media in Illinois} and in another: \enquote{The President greets the press in Chicago}. After stop-word removal, the two corresponding nBOW vectors \textbf{$d$} and \textbf{$d'$} have no common non-zero dimensions and therefore have close to maximum simplex distance, although their true distance is small.
\end{displayquote}
In order to use the innate similarity between words provided by word embeddings, \citeauthor{kusner_word_2015} first introduces a flow matrix $T \in R^{n \times n}$.

\section{\citeauthor{balikas_cross-lingual_2018} Implementation}%
\label{sec:balikas_cross-lingual_2018_implementation}



\subsection{Optimal Transport}%
\label{sub:optimal_transport}

\subsection{Sinkhorn}%
\label{sub:sinkhorn}
