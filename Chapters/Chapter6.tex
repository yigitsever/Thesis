% Chapter Template

\chapter{Experiments and Evaluation}%
\label{chap:experiments_and_evaluation}

\section{Mapping Word Embeddings}%
\label{sec:mapping_word_embeddings}

We experimented with in domain \emph{fasttext} embeddings.
We have trained Romanian and Bulgarian embeddings and ran the WMD and Sinkhorn experiments.
The performance dropped to the quarter of pre-trained embeddings so we have not repeated the experiments for other language corpora.

We have evaluated our embeddings using the standard bilingual lexicon extraction as a general measure for their performance.
Although \textcite{ruder_survey_2017} and \textcite{glavas_how_2019} both say you should evaluate them using downstream tasks.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Language} & \textbf{FastText 1M} & \textbf{FastText 500k} & \textbf{numberbatch} \\
        \midrule
        bg & 33.61 & 35.17 & 51.97 \\
        el & 37.37 & 39.58 & 30.35 \\
        it & 58.20 & 59.28 & 50.37 \\
        ro & 37.33 & 38.71 & 64.17 \\
        sl & 21.42 & 22.91 & 74.74 \\
        sq & 24.46 & 25.36 & 58.63 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy Scores of the Vectors Aligned Using VecMap}%
    \label{tab:accuracy_results}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Language} & \textbf{FastText 1M} & \textbf{FastText 500k} & \textbf{numberbatch} \\
        \midrule
        bg & 96.43 & 93.36 & 17.53 \\
        el & 94.44 & 90.28 & 12.15 \\
        it & 97.93 & 95.97 & 41.08 \\
        ro & 97.06 & 94.91 & 16.4 \\
        sl & 94.67 & 90.73 & 9.23 \\
        sq & 83.59 & 80.92 & 9.51 \\
        \bottomrule
    \end{tabular}
    \caption{Coverage Scores for the Vectors Aligned Using VecMap}%
    \label{tab:coverage_results}
\end{table}


\section{Results}%
\label{sec:chap3_results}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \multicolumn{3}{c}{Percentage of Correctly Matched Definitions} \\
\cmidrule(lr){2-4}
        \textbf{Language} & \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
        \midrule
        bg & 0.39 & 0.41 & 0.19 \\
        el & 0.37 & 0.38 & 0.14 \\
        it & 0.28 & 0.28 & 0.36 \\
        ro & 0.39 & 0.39 & 0.20 \\
        sl & 0.15 & 0.15 & 0.06 \\
        sq & 0.55 & 0.54 & 0.27 \\
        \bottomrule
    \end{tabular}
    \caption{Linear Assignment Using 2000 Definitions}%
    \label{tab:lapjv_2000}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \multicolumn{3}{c}{Percentage of Correctly Matched Definitions} \\
\cmidrule(lr){2-4}
        \textbf{Language} & \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
        bg & 0.35 & 0.36 & 0.18 \\
        el & 0.36 & 0.36 & 0.12 \\
        it & 0.25 & 0.25 & 0.32 \\
        ro & 0.36 & 0.37 & 0.19 \\
        sl & 0.11 & 0.11 & 0.05 \\
        sq & 0.39 & 0.40 & 0.19 \\
        \bottomrule
    \end{tabular}
    \caption{Linear Assignment Using 3000 Definitions}%
    \label{tab:lapjv_3000}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
\midrule
        Best & 0.55 & 0.54 & 0.36 \\
        Worst & 0.11 & 0.11 & 0.05 \\
        Average & 0.33 & 0.33 & 0.19 \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Linear Assignment}%
    \label{tab:lapjv_summary}
\end{table}

\subsection{Monolingual Retrieval Results}%
\label{sub:chap4_results}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrrrr}
        \toprule%
        Language & MRR & Top 10 & Top 10 \% & Top 1 & Top 1 \% \\
        \midrule%
        bg & 0.087 & 674 & 0.337 & 403 & 0.202 \\
        el & 0.122 & 1006 & 0.503 & 709 & 0.355 \\
        it & 0.016 & 476 & 0.238 & 250 & 0.125 \\
        ro & 0.051 & 988 & 0.494 & 728 & 0.364 \\
        sl & 0.073 & 584 & 0.292 & 317 & 0.159 \\
        sq & 0.056 & 979 & 0.490 & 767 & 0.384 \\
        \bottomrule
    \end{tabular}
    \caption{Experiment results for monolingual retrieval}%
    \label{tab:monolingual_tfidf}
\end{table}

\subsection{Cross Lingual Retrieval Results}%
\label{sub:cross_lingual_retrieval_results}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table}[]
\begin{tabular}{@{}lllllllllll@{}}
 &  &  & \multicolumn{2}{r}{WMD - tfidf} & \multicolumn{2}{r}{Sinkhorn tfidf} & \multicolumn{2}{r}{WMD - tf} & \multicolumn{2}{r}{Sinkhorn - tf} \\
Language & Vector & \multicolumn{1}{r}{Vocabulary Size} & MRR & Hits @ 1 & MRR & Hits @ 1 & MRR & Hits @ 1 & MRR & Hits @ 1 \\
\multirow{3}{*}{bg} & 1M Fasttext & \multicolumn{1}{r}{10532} & 0.5083 & 0.4150 & 0.5185 & 0.4265 & 0.4181 & 0.3395 & 0.4276 & 0.3460 \\
 & 500k Fasttext & \multicolumn{1}{r}{10457} & 0.5115 & 0.4190 & 0.5224 & 0.4300 & 0.4218 & 0.3415 & 0.4319 & 0.3480 \\
 & Numberbatch & \multicolumn{1}{r}{6028} & 0.2532 & 0.1745 & 0.2497 & 0.1715 & 0.1855 & 0.1230 & 0.1809 & 0.1135 \\
\multirow{3}{*}{el} & 1M Fasttext & \multicolumn{1}{r}{9307} & 0.4782 & 0.3905 & 0.4867 & 0.4015 & 0.4074 & 0.3285 & 0.4142 & 0.3340 \\
 & 500k Fasttext & \multicolumn{1}{r}{9168} & 0.4745 & 0.3845 & 0.4845 & 0.3995 & 0.4053 & 0.3255 & 0.4139 & 0.3355 \\
 & Numberbatch & \multicolumn{1}{r}{5127} & 0.1992 & 0.1315 & 0.2006 & 0.1340 & 0.1471 & 0.0995 & 0.1494 & 0.1000 \\
\multirow{3}{*}{it} & 1M Fasttext & \multicolumn{1}{r}{10025} & 0.4024 & 0.3115 & 0.4060 & 0.3145 & 0.3198 & 0.2350 & 0.3207 & 0.2340 \\
 & 500k Fasttext & \multicolumn{1}{r}{9975} & 0.4027 & 0.3115 & 0.4049 & 0.3130 & 0.3211 & 0.2365 & 0.3221 & 0.2350 \\
 & Numberbatch & \multicolumn{1}{r}{8875} & 0.4272 & 0.3330 & 0.4277 & 0.3335 & 0.3511 & 0.2670 & 0.3512 & 0.2680 \\
\multirow{3}{*}{ro} & 1M Fasttext & \multicolumn{1}{r}{12165} & 0.5130 & 0.4160 & 0.5195 & 0.4150 & 0.4420 & 0.3590 & 0.4506 & 0.3560 \\
 & 500k Fasttext & \multicolumn{1}{r}{12034} & 0.5128 & 0.4165 & 0.5237 & 0.4220 & 0.4385 & 0.3550 & 0.4514 & 0.3575 \\
 & Numberbatch & \multicolumn{1}{r}{6939} & 0.2768 & 0.1985 & 0.2770 & 0.1980 & 0.2157 & 0.1570 & 0.2186 & 0.1625 \\
\multirow{3}{*}{sl} & 1M Fasttext & \multicolumn{1}{r}{12185} & 0.2622 & 0.1780 & 0.2638 & 0.1805 & 0.2343 & 0.1510 & 0.2397 & 0.1560 \\
 & 500k Fasttext & \multicolumn{1}{r}{12020} & 0.2612 & 0.1780 & 0.2631 & 0.1795 & 0.2347 & 0.1515 & 0.2403 & 0.1580 \\
 & Numberbatch & \multicolumn{1}{r}{5870} & 0.0935 & 0.0480 & 0.0946 & 0.0500 & 0.0635 & 0.0290 & 0.0626 & 0.0285 \\
\multirow{3}{*}{sq} & 1M Fasttext & \multicolumn{1}{r}{8048} & 0.6566 & 0.5915 & 0.6397 & 0.5640 & 0.5647 & 0.4855 & 0.5694 & 0.4930 \\
 & 500k Fasttext & \multicolumn{1}{r}{7990} & 0.6561 & 0.5870 & 0.6442 & 0.5685 & 0.5661 & 0.4880 & 0.5705 & 0.4925 \\
 & Numberbatch & \multicolumn{1}{r}{4908} & 0.3107 & 0.2355 & 0.3106 & 0.2330 & 0.2431 & 0.1780 & 0.2474 & 0.1835 \\
 &  &  & 0.4000 & 0.3178 & 0.4021 & 0.3186 & 0.3322 & 0.2583 & 0.3368 & 0.2612 \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  &  \\ \cmidrule(r){1-2}
\end{tabular}
\end{table}


\section{Case Study}%
\label{sec:case_study}

In order to test our approach, we have acquired a general purpose Turkish dictionary~\footnote{We would like to thank Hülya Küçükaras once more}, to be used for research purposes.
The dictionary was in a proprietary format so we have parsed the terms alongside their definitions.
The parts of speech for the terms are also extracted.
All in all, 67351 headwords with 93062 definitions are parsed.

We have shown that the approaches we have presented so far are bound by their memory restrictions.
We have tried to overcome it by running the experiment on only nouns but the issue persisted for the case study as well.
As a result, we referred to \textcite{khodak_automated_2017} and constrained our scope to a list of core WordNet synsets.
Open Multilingual Wordnet hosts~\footnote{\url{http://compling.hss.ntu.edu.sg/omw/wn30-core-synsets.tab}} a list that identifies 4961 WordNet identifiers in the form of offset and part of speech that is compatible with the nltk library.
The list has been prepared in \textcite{boyd-graber_adding_2006} by human evaluators by selecting salient synsets from a list of frequent words.
By using a core WordNet, we picked ourselves a problem domain we can tackle.
We also deleted the identifiers for verbs and adjectives and worked only with nouns.
Then, the experiment set for the Turkish dictionary is selected by translating the lemmas that belong to core WordNet synsets to Turkish and using the resulting set to query the headwords of the Turkish dictionary.
Using this method, we obtained 702 Turkish definitions and 3280 WordNet definitions.

The methods we studied so far work with two corpora of the same size so we randomly picked 702 English WordNet definitions to go against 702 Turkish dictionary definitions.
We use our best performing set of approaches; word mover's distance using \tfidf{} weights, run on fastText embeddings mapped using VecMap.
The bilingual dictionary provided by OpenSubtitles used in order to map Turkish and English fastText embeddings.

While preparing the corpora for the word mover's distance, 101 Turkish definitions are dropped due to them having no words to be represented by fastText embeddings.
Same number of English WordNet definitions are also dropped to keep to the symmetric size constraint.
Then, word mover's distance is run over 601 Turkish and English definitions.

We do not know the ground truth for the mapping in this study hence we cannot report for any evaluation metric.
As a result, in Appendix~\ref{app:case_study}, we present 100 randomly selected pairs that were retrieved as the top result against the Turkish query.
