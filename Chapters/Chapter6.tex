% Chapter Template

\chapter{Experiments and Evaluation}%
\label{chap:experiments_and_evaluation}

\section{Preparing Wordnets}%
\label{sec:preparing_wordnets}

In order to run our experiments we need two sets of dictionary definitions from two different languages.
Open Multilingual Wordnet~\cite{bond_linking_2013} project hosts 34 wordnets with permissive licenses on their website.\footnote{\url{http://compling.hss.ntu.edu.sg/omw/}}
We have investigated the available wordnets and fortunately six of them included definitions, also known as glosses.
Since we do not use any other information related to wordnets (like semantic relationships) only definitions were extracted into a plain text corpora.
This intermediate corpora includes WordNet 3.0 synsets identifiers and the corresponding definitions in the target language.
Natural Language Toolkit~\cite{bird_natural_2009} provides an API for reading and retrieving English Princeton WordNet.
Using the synsets identifiers, it is possible to retrieve the exact synset which comes attached with an unique definition for the synset.
Finally, we have 6 aligned corpora for 6 wordnets we will run the experiments on.
Alignment here refers to definitions that represent same synset across languages appearing on the same index, a notation we will use throughout the chapter.
The statistics and the 2 letter language codes that we will commonly use to denote the wordnets for the rest of this chapter is presented in Table~\ref{tab:wordnet_stats}.

\begin{table}[hbtp]
    \centering
    \settowidth\tymin{\textbf{Language}}
    \setlength\extrarowheight{2pt}
    \begin{tabulary}{1.0\linewidth}{L L R R R R}
        \toprule
        Language Code & Language Name & Number of Definitions & Number of words & Average Words per Definition & Longest Definition \\ \midrule
        sq & Albanian & 4681 & 54980 & 11.75 & 101 \\
        bg & Bulgarian & 4959 & 63014 & 12.71 & 53 \\
        el & Greek & 18136 & 203924 & 11.24 & 89 \\
        it & Italian & 12688 & 93005 & 7.33 & 35 \\
        ro & Romanian & 58754 & 586304 & 9.98 & 105 \\
        sl & Slovene & 3144 & 39865 & 12.68 & 68 \\
        \bottomrule
    \end{tabulary}%
    \caption{Language codes and statistics for the target wordnets used in the thesis.}%
    \label{tab:wordnet_stats}
\end{table}

\section{Preparing Word Embeddings}%
\label{sec:preparing_word_embeddings}

In Chapter~\ref{chap:background_n_related}, we have mentioned the recent popularization of pre-trained word embeddings.
Initiated by word2vec\footnote{\url{https://code.google.com/archive/p/word2vec/}}, other sources for word embeddings are GloVe\footnote{\url{https://nlp.stanford.edu/projects/glove/}}, fastText\footnote{\url{https://fasttext.cc/}} and numberbatch\footnote{\url{https://github.com/commonsense/conceptnet-numberbatch}}.
Yet, word embeddings for languages other than English are scarce.
FastText hosts word embeddings for 157 languages so we used them as our primary source.\cite{grave_learning_2018}
These embeddings are trained using Common Crawl and Wikipedia data.

Numberbatch provides word embeddings for 304 languages.
However, 10 of the supported languages are presented as core languages with excellent support and 68 of them are tagged as common languages which is only given adequate support.
Referring to Table~\ref{tab:wordnet_stats}, Italian is among the core languages and the rest are in the common languages group.

The fastText embeddings include 2 million tokens out of the box.
In order to increase the efficient of the experiments, we have cut down the size of the embeddings into 1 million and 500 thousand.
The vectors are sorted according to their corpus frequency so the uppermost lines were used.

Numberbatch embeddings are distributed in one large file and does not include fixed number of tokens per language.
Hence after parsing the file and extracting the embeddings into individual files, the number of word vectors we are left with are presented in Table~\ref{tab:numberbatch_stats}.

\begin{table}[hbtp]
    \centering
    \begin{tabulary}{1.0\linewidth}{L R}
        \toprule
        Language Code & Number of Tokens \\
        \midrule
        bg & 20871 \\
        el & 16926 \\
        en & 417195 \\
        it & 91829 \\
        ro & 10874 \\
        sl & 11458 \\
        sq & 5512 \\
        \bottomrule
    \end{tabulary}
    \caption{The number of embeddings available in numberbatch}%
    \label{tab:numberbatch_stats}
\end{table}

These embeddings are monolingual, so they are on separate arbitrary latent spaces.
In order to represent them on the same latent space, we used VecMap~\cite{artetxe_robust_2018,artetxe_generalizing_2018,artetxe_learning_2017,artetxe_learning_2016}.\footnote{\url{https://github.com/artetxem/vecmap}}
According to \textcite{ruder_survey_2017}, bilingual or cross lingual embedding models optimize similar objectives and differences in performance is due to available data they are trained on.
\textcite{glavas_how_2019} supports this intuition and has empirically proven that common evaluation metrics like bilingual dictionary induction is not representative for the bilingual embedding's performance on downstream tasks.
Hence our preference of VecMap is highly influenced by it's availability as an open source framework and it's ease of training.

Best performing VecMap model available in the framework is supervised alignment.
It requires a bilingual dictionary, otherwise known as aligned word pairs for two languages.
We sourced our bilingual dictionary from Open Subtitles 2018\footnote{\url{http://www.opensubtitles.org/}} data as hosted by OPUS.\footnote{\url{http://opus.nlpl.eu/}}
The dictionary can be sorted by the confidence score of the translation pair, which we did so that pairs with high confidence scored swam to top.
The first 25000 translation pairs were shuffled and split into training and testing examples for 6 languages.
After supervised mapping of language specific word embedding and English word embedding, we have 6 pairs of vectors that share the same latent space.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Language} & \textbf{FastText 1M} & \textbf{FastText 500k} & \textbf{numberbatch} \\
        \midrule
        bg & 33.61 & 35.17 & 51.97 \\
        el & 37.37 & 39.58 & 30.35 \\
        it & 58.20 & 59.28 & 50.37 \\
        ro & 37.33 & 38.71 & 64.17 \\
        sl & 21.42 & 22.91 & 74.74 \\
        sq & 24.46 & 25.36 & 58.63 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy scores (in percentage) of the word embeddings aligned using VecMap}%
    \label{tab:accuracy_results}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Language} & \textbf{FastText 1M} & \textbf{FastText 500k} & \textbf{numberbatch} \\
        \midrule
        bg & 96.43 & 93.36 & 17.53 \\
        el & 94.44 & 90.28 & 12.15 \\
        it & 97.93 & 95.97 & 41.08 \\
        ro & 97.06 & 94.91 & 16.4 \\
        sl & 94.67 & 90.73 & 9.23 \\
        sq & 83.59 & 80.92 & 9.51 \\
        \bottomrule
    \end{tabular}
    \caption{Coverage scores (in percentage) of the word embeddings aligned using VecMap}%
    \label{tab:coverage_results}
\end{table}

Even though bilingual dictionary induction is not representative of a bilingual word embedding pair's performance on downstream tasks~\cite{ruder_survey_2017,glavas_how_2019}, we include the evaluation results obtained from VecMap framework as a quick measure for their quality on Table~\ref{tab:accuracy_results} for accuracy and Table~\ref{tab:coverage_results} for coverage.
Accuracy is the measure for correctly identifying the translation of a word given the test dictionary and coverage is the percentage of translation pairs that could be inducted.
From the results, it is apparent that fastText embeddings have much better coverage due to vast data they were trained on.
Yet, numberbatch exhibits better accuracy scores which might be a tradeoff of their low coverage.
Nevertheless, bilingual dictionary induction results are not indicative of word embedding's real life performance.


\section{Investigating Word Embedding Sources}%
\label{sec:investigating_word_embedding_sources}

We experimented with in domain \emph{fastText} embeddings in order to ask if pre-trained embeddings are better than embeddings trained on the experiment data.
Since Romanian wordnet has the most data available, we have trained Romanian embeddings on the Romanian wordnet definitions, mapped the embeddings to the same latent space using supervised VecMap and ran cross lingual document retrieval experiments using word mover's distance and Sinkhorn.
% TODO here
The performance dropped to the quarter of pre-trained embeddings so we have not repeated the experiments for other language corpora.


\section{Results}%
\label{sec:chap3_results}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \multicolumn{3}{c}{Percentage of Correctly Matched Definitions} \\
\cmidrule(lr){2-4}
        \textbf{Language} & \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
        \midrule
        bg & 0.39 & 0.41 & 0.19 \\
        el & 0.37 & 0.38 & 0.14 \\
        it & 0.28 & 0.28 & 0.36 \\
        ro & 0.39 & 0.39 & 0.20 \\
        sl & 0.15 & 0.15 & 0.06 \\
        sq & 0.55 & 0.54 & 0.27 \\
        \bottomrule
    \end{tabular}
    \caption{Linear Assignment Using 2000 Definitions}%
    \label{tab:lapjv_2000}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \multicolumn{3}{c}{Percentage of Correctly Matched Definitions} \\
\cmidrule(lr){2-4}
        \textbf{Language} & \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
        bg & 0.35 & 0.36 & 0.18 \\
        el & 0.36 & 0.36 & 0.12 \\
        it & 0.25 & 0.25 & 0.32 \\
        ro & 0.36 & 0.37 & 0.19 \\
        sl & 0.11 & 0.11 & 0.05 \\
        sq & 0.39 & 0.40 & 0.19 \\
        \bottomrule
    \end{tabular}
    \caption{Linear Assignment Using 3000 Definitions}%
    \label{tab:lapjv_3000}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrr}
        \toprule
& \textbf{fastText 1M} & \textbf{fastText 500k} & \textbf{Numberbatch} \\
\midrule
        Best & 0.55 & 0.54 & 0.36 \\
        Worst & 0.11 & 0.11 & 0.05 \\
        Average & 0.33 & 0.33 & 0.19 \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Linear Assignment}%
    \label{tab:lapjv_summary}
\end{table}

\subsection{Monolingual Retrieval Results}%
\label{sub:chap4_results}

\begin{table}[htbp]
    \centering
    \begin{tabular}{lrrrrr}
        \toprule%
        Language & MRR & Top 10 & Top 10 \% & Top 1 & Top 1 \% \\
        \midrule%
        bg & 0.087 & 674 & 0.337 & 403 & 0.202 \\
        el & 0.122 & 1006 & 0.503 & 709 & 0.355 \\
        it & 0.016 & 476 & 0.238 & 250 & 0.125 \\
        ro & 0.051 & 988 & 0.494 & 728 & 0.364 \\
        sl & 0.073 & 584 & 0.292 & 317 & 0.159 \\
        sq & 0.056 & 979 & 0.490 & 767 & 0.384 \\
        \bottomrule
    \end{tabular}
    \caption{Experiment results for monolingual retrieval}%
    \label{tab:monolingual_tfidf}
\end{table}

\subsection{Cross Lingual Retrieval Results}%
\label{sub:cross_lingual_retrieval_results}


\section{Case Study}%
\label{sec:case_study}

In order to test our approach, we have acquired a general purpose Turkish dictionary~\footnote{We would like to thank Hülya Küçükaras for providing the dictionary}, to be used for research purposes.
The dictionary was in a proprietary format so we have parsed the terms alongside their definitions.
The parts of speech for the terms are also extracted.
All in all, 67351 headwords with 93062 definitions are parsed.

We have shown that the approaches we have presented so far are bound by their memory restrictions.
We have tried to overcome it by running the experiment on only nouns but the issue persisted for the case study as well.
As a result, we referred to \textcite{khodak_automated_2017} and constrained our scope to a list of core WordNet synsets.
Open Multilingual Wordnet hosts~\footnote{\url{http://compling.hss.ntu.edu.sg/omw/wn30-core-synsets.tab}} a list that identifies 4961 WordNet identifiers in the form of offset and part of speech that is compatible with the nltk library.
The list has been prepared in \textcite{boyd-graber_adding_2006} by human evaluators by selecting salient synsets from a list of frequent words.
By using a core WordNet, we picked ourselves a problem domain we can tackle.
We also deleted the identifiers for verbs and adjectives and worked only with nouns.
Then, the experiment set for the Turkish dictionary is selected by translating the lemmas that belong to core WordNet synsets to Turkish and using the resulting set to query the headwords of the Turkish dictionary.
Using this method, we obtained 702 Turkish definitions and 3280 WordNet definitions.

The methods we studied so far work with two corpora of the same size so we randomly picked 702 English WordNet definitions to go against 702 Turkish dictionary definitions.
We use our best performing set of approaches; word mover's distance using \tfidf{} weights, run on fastText embeddings mapped using VecMap.
The bilingual dictionary provided by OpenSubtitles used in order to map Turkish and English fastText embeddings.

While preparing the corpora for the word mover's distance, 101 Turkish definitions are dropped due to them having no words to be represented by fastText embeddings.
Same number of English WordNet definitions are also dropped to keep to the symmetric size constraint.
Then, word mover's distance is run over 601 Turkish and English definitions.

We do not know the ground truth for the mapping in this study hence we cannot report for any evaluation metric.
As a result, in Appendix~\ref{app:case_study}, we present 100 randomly selected pairs that were retrieved as the top result against the Turkish query.
