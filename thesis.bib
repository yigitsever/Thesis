
@article{gouws_bilbowa_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.2455},
  primaryClass = {cs, stat},
  title = {{{BilBOWA}}: {{Fast Bilingual Distributed Representations}} without {{Word Alignments}}},
  url = {http://arxiv.org/abs/1410.2455},
  shorttitle = {{{BilBOWA}}},
  abstract = {We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.},
  date = {2014-10-09},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Statistics - Machine Learning},
  author = {Gouws, Stephan and Bengio, Yoshua and Corrado, Greg},
  file = {/home/yigit/Zotero/storage/48FPSG2K/Gouws et al. - 2014 - BilBOWA Fast Bilingual Distributed Representation.pdf;/home/yigit/Zotero/storage/C8VQXMWR/1410.html}
}

@article{mikolov_efficient_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  url = {http://arxiv.org/abs/1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  date = {2013-01-16},
  keywords = {Computer Science - Computation and Language},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  file = {/home/yigit/Zotero/storage/U2RB95RH/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/yigit/Zotero/storage/RM3ZK25B/1301.html}
}

@inproceedings{rapp_automatic_1999,
  location = {{Stroudsburg, PA, USA}},
  title = {Automatic {{Identification}} of {{Word Translations}} from {{Unrelated English}} and {{German Corpora}}},
  url = {https://doi.org/10.3115/1034678.1034756},
  doi = {10.3115/1034678.1034756},
  abstract = {Loading...},
  booktitle = {Proceedings of the 37th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} on {{Computational Linguistics}}},
  series = {{{ACL}} '99},
  publisher = {{Association for Computational Linguistics}},
  date = {1999},
  pages = {519--526},
  author = {Rapp, Reinhard},
  file = {/home/yigit/Zotero/storage/4ZKHIJ46/Rapp - 1999 - Automatic Identification of Word Translations from.pdf}
}

@article{coulmance_trans-gram_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.02502},
  primaryClass = {cs},
  title = {Trans-Gram, {{Fast Cross}}-Lingual {{Word}}-Embeddings},
  url = {http://arxiv.org/abs/1601.02502},
  abstract = {We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.},
  date = {2016-01-11},
  keywords = {Computer Science - Computation and Language},
  author = {Coulmance, Jocelyn and Marty, Jean-Marc and Wenzek, Guillaume and Benhalloum, Amine},
  file = {/home/yigit/Zotero/storage/GW3X94NP/Coulmance et al. - 2016 - Trans-gram, Fast Cross-lingual Word-embeddings.pdf;/home/yigit/Zotero/storage/75RPCD25/1601.html}
}

@article{bojanowski_enriching_2016,
  langid = {english},
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  url = {https://arxiv.org/abs/1607.04606},
  urldate = {2018-06-07},
  date = {2016-07-15},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  file = {/home/yigit/Zotero/storage/E95E7P5P/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf;/home/yigit/Zotero/storage/7QUSK2HT/1607.html}
}

@article{conneau_word_2017,
  langid = {english},
  title = {Word {{Translation Without Parallel Data}}},
  url = {https://arxiv.org/abs/1710.04087},
  urldate = {2018-06-07},
  date = {2017-10-11},
  author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
  file = {/home/yigit/Zotero/storage/Q7CD5ZF3/Conneau et al. - 2017 - Word Translation Without Parallel Data.pdf;/home/yigit/Zotero/storage/2EAQQCTX/1710.html}
}

@inproceedings{artetxe_generalizing_2018,
  title = {Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations},
  booktitle = {Proceedings of the {{Thirty}}-{{Second AAAI Conference}} on {{Artificial Intelligence}} ({{AAAI}}-18)},
  date = {2018},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  file = {/home/yigit/Zotero/storage/MCGDKG4G/Artetxe et al. - 2018 - Generalizing and improving bilingual word embeddin.pdf;/home/yigit/Zotero/storage/Z6TZ9Z6B/Artetxe et al. - 2018 - Generalizing and improving bilingual word embeddin.pdf}
}

@article{joulin_improving_2018,
  title = {Improving {{Supervised Bilingual Mapping}} of {{Word Embeddings}}},
  journaltitle = {arXiv preprint arXiv:1804.07745},
  date = {2018},
  author = {Joulin, Armand and Bojanowski, Piotr and Mikolov, Tomas and Grave, Edouard},
  file = {/home/yigit/Zotero/storage/6IKGXQH6/Joulin et al. - 2018 - Improving Supervised Bilingual Mapping of Word Emb.pdf;/home/yigit/Zotero/storage/GC2QFRAE/1804.html}
}

@article{ruder_survey_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.04902},
  primaryClass = {cs},
  title = {A {{Survey Of Cross}}-Lingual {{Word Embedding Models}}},
  url = {http://arxiv.org/abs/1706.04902},
  abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
  urldate = {2018-11-28},
  date = {2017-06-15},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
  file = {/home/yigit/Zotero/storage/87LVYAAX/Ruder et al. - 2017 - A Survey Of Cross-lingual Word Embedding Models.pdf;/home/yigit/Zotero/storage/X2ZAMGLS/1706.html}
}

@article{upadhyay_cross-lingual_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.00425},
  primaryClass = {cs},
  title = {Cross-Lingual {{Models}} of {{Word Embeddings}}: {{An Empirical Comparison}}},
  url = {http://arxiv.org/abs/1604.00425},
  shorttitle = {Cross-Lingual {{Models}} of {{Word Embeddings}}},
  abstract = {Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.},
  urldate = {2018-11-28},
  date = {2016-04-01},
  keywords = {Computer Science - Computation and Language},
  author = {Upadhyay, Shyam and Faruqui, Manaal and Dyer, Chris and Roth, Dan},
  file = {/home/yigit/Zotero/storage/MWZM3DEG/Upadhyay et al. - 2016 - Cross-lingual Models of Word Embeddings An Empiri.pdf;/home/yigit/Zotero/storage/L5IGGNSK/1604.html}
}

@article{faruqui_problems_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.02276},
  primaryClass = {cs},
  title = {Problems {{With Evaluation}} of {{Word Embeddings Using Word Similarity Tasks}}},
  url = {http://arxiv.org/abs/1605.02276},
  abstract = {Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.},
  urldate = {2018-11-28},
  date = {2016-05-08},
  keywords = {Computer Science - Computation and Language},
  author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
  file = {/home/yigit/Zotero/storage/5XLWF3FT/Faruqui et al. - 2016 - Problems With Evaluation of Word Embeddings Using .pdf;/home/yigit/Zotero/storage/H26KZCGL/1605.html}
}

@inproceedings{faruqui_improving_2014,
  location = {{Gothenburg, Sweden}},
  title = {Improving {{Vector Space Word Representations Using Multilingual Correlation}}},
  url = {http://aclweb.org/anthology/E14-1049},
  doi = {10.3115/v1/E14-1049},
  booktitle = {Proceedings of the 14th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2018-11-28},
  date = {2014},
  pages = {462--471},
  author = {Faruqui, Manaal and Dyer, Chris},
  file = {/home/yigit/Zotero/storage/ZHAHMQ6G/Faruqui and Dyer - 2014 - Improving Vector Space Word Representations Using .pdf}
}

@inproceedings{artetxe_learning_2017,
  location = {{Vancouver, Canada}},
  title = {Learning Bilingual Word Embeddings with (Almost) No Bilingual Data},
  url = {http://aclweb.org/anthology/P17-1042},
  doi = {10.18653/v1/P17-1042},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2018-11-28},
  date = {2017},
  pages = {451--462},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  file = {/home/yigit/Zotero/storage/IRJFWXUX/Artetxe et al. - 2017 - Learning bilingual word embeddings with (almost) n.pdf}
}

@article{duong_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.09403},
  primaryClass = {cs},
  title = {Learning {{Crosslingual Word Embeddings}} without {{Bilingual Corpora}}},
  url = {http://arxiv.org/abs/1606.09403},
  abstract = {Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.},
  urldate = {2018-11-28},
  date = {2016-06-30},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  author = {Duong, Long and Kanayama, Hiroshi and Ma, Tengfei and Bird, Steven and Cohn, Trevor},
  file = {/home/yigit/Zotero/storage/XJVS7YU3/Duong et al. - 2016 - Learning Crosslingual Word Embeddings without Bili.pdf;/home/yigit/Zotero/storage/MLBHSSMJ/1606.html}
}

@article{irvine_comprehensive_2017,
  title = {A {{Comprehensive Analysis}} of {{Bilingual Lexicon Induction}}},
  volume = {43},
  issn = {0891-2017},
  url = {https://doi.org/10.1162/COLI_a_00284},
  doi = {10.1162/COLI_a_00284},
  abstract = {Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages: Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese, and Welsh. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. Additionally, we introduce a novel discriminative approach to bilingual lexicon induction. Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence. When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g., using minimum reciprocal rank). We also directly compare our model's performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al. (2008). Our algorithm achieves an accuracy of 42\% versus MCCA's 15\%.},
  number = {2},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  urldate = {2018-11-18},
  date = {2017-03-28},
  pages = {273-310},
  author = {Irvine, Ann and Callison-Burch, Chris},
  file = {/home/yigit/Zotero/storage/9GGUDPBU/Irvine and Callison-Burch - 2017 - A Comprehensive Analysis of Bilingual Lexicon Indu.pdf;/home/yigit/Zotero/storage/DS5MMUAR/COLI_a_00284.html}
}

@article{litschko_unsupervised_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.00879},
  primaryClass = {cs},
  title = {Unsupervised {{Cross}}-{{Lingual Information Retrieval}} Using {{Monolingual Data Only}}},
  url = {http://arxiv.org/abs/1805.00879},
  abstract = {We propose a fully unsupervised framework for ad-hoc cross-lingual information retrieval (CLIR) which requires no bilingual data at all. The framework leverages shared cross-lingual word embedding spaces in which terms, queries, and documents can be represented, irrespective of their actual language. The shared embedding spaces are induced solely on the basis of monolingual corpora in two languages through an iterative process based on adversarial neural networks. Our experiments on the standard CLEF CLIR collections for three language pairs of varying degrees of language similarity (English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed fully unsupervised approach. Our CLIR models with unsupervised cross-lingual embeddings outperform baselines that utilize cross-lingual embeddings induced relying on word-level and document-level alignments. We then demonstrate that further improvements can be achieved by unsupervised ensemble CLIR models. We believe that the proposed framework is the first step towards development of effective CLIR models for language pairs and domains where parallel data are scarce or non-existent.},
  urldate = {2018-11-12},
  date = {2018-05-02},
  keywords = {Computer Science - Computation and Language},
  author = {Litschko, Robert and Glavaš, Goran and Ponzetto, Simone Paolo and Vulić, Ivan},
  file = {/home/yigit/Zotero/storage/SCQUJN8D/Litschko et al. - 2018 - Unsupervised Cross-Lingual Information Retrieval u.pdf;/home/yigit/Zotero/storage/USYHIZXU/1805.html}
}

@inproceedings{vulic_monolingual_2015,
  location = {{New York, NY, USA}},
  title = {Monolingual and {{Cross}}-{{Lingual Information Retrieval Models Based}} on ({{Bilingual}}) {{Word Embeddings}}},
  isbn = {978-1-4503-3621-5},
  url = {http://doi.acm.org/10.1145/2766462.2767752},
  doi = {10.1145/2766462.2767752},
  abstract = {We propose a new unified framework for monolingual (MoIR) and cross-lingual information retrieval (CLIR) which relies on the induction of dense real-valued word vectors known as word embeddings (WE) from comparable data. To this end, we make several important contributions: (1) We present a novel word representation learning model called Bilingual Word Embeddings Skip-Gram (BWESG) which is the first model able to learn bilingual word embeddings solely on the basis of document-aligned comparable data; (2) We demonstrate a simple yet effective approach to building document embeddings from single word embeddings by utilizing models from compositional distributional semantics. BWESG induces a shared cross-lingual embedding vector space in which both words, queries, and documents may be presented as dense real-valued vectors; (3) We build novel ad-hoc MoIR and CLIR models which rely on the induced word and document embeddings and the shared cross-lingual embedding space; (4) Experiments for English and Dutch MoIR, as well as for English-to-Dutch and Dutch-to-English CLIR using benchmarking CLEF 2001-2003 collections and queries demonstrate the utility of our WE-based MoIR and CLIR models. The best results on the CLEF collections are obtained by the combination of the WE-based approach and a unigram language model. We also report on significant improvements in ad-hoc IR tasks of our WE-based framework over the state-of-the-art framework for learning text representations from comparable data based on latent Dirichlet allocation (LDA).},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  series = {{{SIGIR}} '15},
  publisher = {{ACM}},
  urldate = {2018-11-12},
  date = {2015},
  pages = {363--372},
  keywords = {ad-hoc retrieval,comparable data,cross-lingual information retrieval,multilinguality,semantic composition,text representation learning,vector space retrieval models,word embeddings},
  author = {Vulić, Ivan and Moens, Marie-Francine},
  file = {/home/yigit/Zotero/storage/XVULM8SI/Vulić and Moens - 2015 - Monolingual and Cross-Lingual Information Retrieva.pdf}
}

@article{vulic_bilingual_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.07308},
  primaryClass = {cs},
  title = {Bilingual {{Distributed Word Representations}} from {{Document}}-{{Aligned Comparable Data}}},
  url = {http://arxiv.org/abs/1509.07308},
  abstract = {We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.},
  urldate = {2018-11-12},
  date = {2015-09-24},
  keywords = {Computer Science - Computation and Language},
  author = {Vulić, Ivan and Moens, Marie-Francine},
  file = {/home/yigit/Zotero/storage/PXIRYREQ/Vulić and Moens - 2015 - Bilingual Distributed Word Representations from Do.pdf;/home/yigit/Zotero/storage/GP7D3QMI/1509.html}
}

@article{levy_strong_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.05426},
  primaryClass = {cs},
  title = {A {{Strong Baseline}} for {{Learning Cross}}-{{Lingual Word Embeddings}} from {{Sentence Alignments}}},
  url = {http://arxiv.org/abs/1608.05426},
  abstract = {While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.},
  urldate = {2018-11-08},
  date = {2016-08-18},
  keywords = {Computer Science - Computation and Language,cross-lingual word embedding},
  author = {Levy, Omer and Søgaard, Anders and Goldberg, Yoav},
  file = {/home/yigit/Zotero/storage/UJ685QKV/Levy et al. - 2016 - A Strong Baseline for Learning Cross-Lingual Word .pdf;/home/yigit/Zotero/storage/RN39MKCK/1608.html}
}

@article{turney_frequency_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1003.1141},
  title = {From {{Frequency}} to {{Meaning}}: {{Vector Space Models}} of {{Semantics}}},
  volume = {37},
  issn = {1076-9757},
  url = {http://arxiv.org/abs/1003.1141},
  doi = {10.1613/jair.2934},
  shorttitle = {From {{Frequency}} to {{Meaning}}},
  abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
  journaltitle = {Journal of Artificial Intelligence Research},
  urldate = {2018-08-01},
  date = {2010-02-27},
  pages = {141-188},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Information Retrieval,H.3.1,I.2.6,I.2.7},
  author = {Turney, Peter D. and Pantel, Patrick},
  file = {/home/yigit/Zotero/storage/PZH7TLYC/Turney and Pantel - 2010 - From Frequency to Meaning Vector Space Models of .pdf;/home/yigit/Zotero/storage/XWLFQR64/1003.html}
}

@inproceedings{patwardhan_using_2006,
  title = {Using {{WordNet}}-Based Context Vectors to Estimate the Semantic Relatedness of Concepts},
  booktitle = {Proceedings of the {{Workshop}} on {{Making Sense}} of {{Sense}}: {{Bringing Psycholinguistics}} and {{Computational Linguistics Together}}},
  date = {2006},
  author = {Patwardhan, Siddharth and Pedersen, Ted},
  file = {/home/yigit/Zotero/storage/GBQEDN3J/Patwardhan and Pedersen - 2006 - Using WordNet-based context vectors to estimate th.pdf;/home/yigit/Zotero/storage/WNNAHTHT/Patwardhan and Pedersen - 2006 - Using WordNet-based context vectors to estimate th.pdf}
}

@inproceedings{zou_bilingual_2013,
  title = {Bilingual Word Embeddings for Phrase-Based Machine Translation},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  date = {2013},
  pages = {1393--1398},
  author = {Zou, Will Y. and Socher, Richard and Cer, Daniel and Manning, Christopher D.},
  file = {/home/yigit/Zotero/storage/3FCAF7Z6/Zou et al. - 2013 - Bilingual word embeddings for phrase-based machine.pdf;/home/yigit/Zotero/storage/CAJMK2RB/Zou et al. - 2013 - Bilingual word embeddings for phrase-based machine.pdf}
}

@inproceedings{ni_cross_2011,
  location = {{New York, NY, USA}},
  title = {Cross {{Lingual Text Classification}} by {{Mining Multilingual Topics}} from {{Wikipedia}}},
  isbn = {978-1-4503-0493-1},
  url = {http://doi.acm.org/10.1145/1935826.1935887},
  doi = {10.1145/1935826.1935887},
  abstract = {This paper investigates how to effectively do cross lingual text classification by leveraging a large scale and multilingual knowledge base, Wikipedia. Based on the observation that each Wikipedia concept is described by documents of different languages, we adapt existing topic modeling algorithms for mining multilingual topics from this knowledge base. The extracted topics have multiple types of representations, with each type corresponding to one language. In this work, we regard such topics extracted from Wikipedia documents as universal-topics, since each topic corresponds with same semantic information of different languages. Thus new documents of different languages can be represented in a space using a group of universal-topics. We use these universal-topics to do cross lingual text classification. Given the training data labeled for one language, we can train a text classifier to classify the documents of another language by mapping all documents of both languages into the universal-topic space. This approach does not require any additional linguistic resources, like bilingual dictionaries, machine translation tools, or labeling data for the target language. The evaluation results indicate that our topic modeling approach is effective for building cross lingual text classifier.},
  booktitle = {Proceedings of the {{Fourth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  series = {{{WSDM}} '11},
  publisher = {{ACM}},
  date = {2011},
  pages = {375--384},
  keywords = {cross lingual text classification,multilingual,topic modeling,universal-topics,Wikipedia},
  author = {Ni, Xiaochuan and Sun, Jian-Tao and Hu, Jian and Chen, Zheng}
}

@article{cilibrasi_google_2007,
  title = {The {{Google Similarity Distance}}},
  volume = {19},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2007.48},
  abstract = {Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers, the equivalent of "society" is "database," and the equivalent of "use" is "a way to search the database". We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts, we use the World Wide Web (WWW) as the database, and Google as the search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the WWW using Google page counts. The WWW is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87 percent with the expert crafted WordNet categories},
  number = {3},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  date = {2007-03},
  pages = {370-383},
  keywords = {Accuracy comparison with WordNet categories,automatic classification and clustering,automatic meaning discovery using Google,automatic relative semantics,automatic translation,binary classification,Books,classification,data mining,database management system,database management systems,Databases,dissimilarity semantic distance,Earth,Google code,Google distribution via page hit counts,Google search,Google similarity distance,hierarchical classification,hierarchical clustering,Kolmogorov complexity,meaning of words and phrases extracted from the Web,Natural languages,normalized compression distance (ncd),normalized Google distance (ngd),normalized information distance (nid),Painting,parameter-free data mining,search engine,search engines,support vector machine,Support vector machines,universal similarity metric.,Web sites,WordNet database,World Wide Web},
  author = {Cilibrasi, R. L. and Vitanyi, P. M. B.},
  file = {/home/yigit/Zotero/storage/4AFV69H4/4072748.html}
}

@inproceedings{snow_learning_2005,
  title = {Learning Syntactic Patterns for Automatic Hypernym Discovery},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {2005},
  pages = {1297--1304},
  author = {Snow, Rion and Jurafsky, Daniel and Ng, Andrew Y.},
  file = {/home/yigit/Zotero/storage/R8KGW98K/Snow et al. - 2005 - Learning syntactic patterns for automatic hypernym.pdf;/home/yigit/Zotero/storage/SQIEW6ZT/Snow et al. - 2005 - Learning syntactic patterns for automatic hypernym.pdf}
}

@article{li_sentence_2006,
  title = {Sentence Similarity Based on Semantic Nets and Corpus Statistics},
  volume = {18},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2006.130},
  abstract = {Sentence similarity measures play an increasingly important role in text-related research and applications in areas such as text mining, Web page retrieval, and dialogue systems. Existing methods for computing sentence similarity have been adopted from approaches used for long text documents. These methods process sentences in a very high-dimensional space and are consequently inefficient, require human input, and are not adaptable to some application domains. This paper focuses directly on computing the similarity between very short texts of sentence length. It presents an algorithm that takes account of semantic information and word order information implied in the sentences. The semantic similarity of two sentences is calculated using information from a structured lexical database and from corpus statistics. The use of a lexical database enables our method to model human common sense knowledge and the incorporation of corpus statistics allows our method to be adaptable to different domains. The proposed method can be used in a variety of applications that involve text knowledge representation and discovery. Experiments on two sets of selected sentence pairs demonstrate that the proposed method provides a similarity measure that shows a significant correlation to human intuition},
  number = {8},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  date = {2006-08},
  pages = {1138-1150},
  keywords = {Statistics,Humans,data mining,database management systems,Databases,Natural languages,Area measurement,computational linguistics,corpus,corpus statistics,dialogue system,Image retrieval,knowledge representation,Natural language processing,semantic nets,Sentence similarity,sentence similarity computing,structured lexical database,text analysis,text knowledge discovery,text knowledge representation,text mining,Web page retrieval,Web pages,word similarity.},
  author = {Li, Y. and McLean, D. and Bandar, Z. A. and O'Shea, J. D. and Crockett, K.},
  file = {/home/yigit/Zotero/storage/NCJWD84Z/1644735.html}
}

@article{mikolov_exploiting_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.4168},
  primaryClass = {cs},
  title = {Exploiting {{Similarities}} among {{Languages}} for {{Machine Translation}}},
  url = {http://arxiv.org/abs/1309.4168},
  abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
  date = {2013-09-16},
  keywords = {Computer Science - Computation and Language},
  author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
  file = {/home/yigit/Zotero/storage/E4CB2KAG/Mikolov et al. - 2013 - Exploiting Similarities among Languages for Machin.pdf;/home/yigit/Zotero/storage/2DUUT77T/1309.html}
}

@inproceedings{joachims_transductive_1999,
  title = {Transductive Inference for Text Classification Using Support Vector Machines},
  volume = {99},
  booktitle = {{{ICML}}},
  date = {1999},
  pages = {200--209},
  author = {Joachims, Thorsten},
  file = {/home/yigit/Zotero/storage/4R3K5PD4/Joachims - 1999 - Transductive inference for text classification usi.pdf;/home/yigit/Zotero/storage/8X2R4R2I/Joachims - 1999 - Transductive inference for text classification usi.pdf}
}

@inproceedings{mccallum_comparison_1998,
  title = {A Comparison of Event Models for Naive Bayes Text Classification},
  volume = {752},
  booktitle = {{{AAAI}}-98 Workshop on Learning for Text Categorization},
  publisher = {{Citeseer}},
  date = {1998},
  pages = {41--48},
  author = {McCallum, Andrew and Nigam, Kamal}
}

@inproceedings{dara_yoda_2016,
  title = {Yoda System for Wmt16 Shared Task: {{Bilingual}} Document Alignment},
  volume = {2},
  shorttitle = {Yoda System for Wmt16 Shared Task},
  booktitle = {Proceedings of the {{First Conference}} on {{Machine Translation}}: {{Volume}} 2, {{Shared Task Papers}}},
  date = {2016},
  pages = {679--684},
  author = {Dara, Aswarth Abhilash and Lin, Yiu-Chang},
  file = {/home/yigit/Zotero/storage/CTG7FXU4/Dara and Lin - 2016 - Yoda system for wmt16 shared task Bilingual docum.pdf;/home/yigit/Zotero/storage/SN7GA7IG/Dara and Lin - 2016 - Yoda system for wmt16 shared task Bilingual docum.pdf}
}

@inproceedings{lohar_adapt_2016,
  title = {The Adapt Bilingual Document Alignment System at Wmt16},
  volume = {2},
  booktitle = {Proceedings of the {{First Conference}} on {{Machine Translation}}: {{Volume}} 2, {{Shared Task Papers}}},
  date = {2016},
  pages = {717--723},
  author = {Lohar, Pintu and Afli, Haithem and Liu, Chao-Hong and Way, Andy},
  file = {/home/yigit/Zotero/storage/9NJFR8IX/Lohar et al. - 2016 - The adapt bilingual document alignment system at w.pdf;/home/yigit/Zotero/storage/UKUS4E3B/Lohar et al. - 2016 - The adapt bilingual document alignment system at w.pdf}
}

@article{steinberger_jrc-acquis_2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0609058},
  title = {The {{JRC}}-{{Acquis}}: {{A}} Multilingual Aligned Parallel Corpus with 20+ Languages},
  url = {http://arxiv.org/abs/cs/0609058},
  shorttitle = {The {{JRC}}-{{Acquis}}},
  abstract = {We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EUanguages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction).},
  date = {2006-09-12},
  keywords = {Computer Science - Computation and Language,H.3.1,H.3.6},
  author = {Steinberger, Ralf and Pouliquen, Bruno and Widiger, Anna and Ignat, Camelia and Erjavec, Tomaz and Tufis, Dan and Varga, Daniel},
  file = {/home/yigit/Zotero/storage/BA392VCI/Steinberger et al. - 2006 - The JRC-Acquis A multilingual aligned parallel co.pdf;/home/yigit/Zotero/storage/EU8RSF6J/0609058.html}
}

@article{lee_sequential_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.03827},
  primaryClass = {cs, stat},
  title = {Sequential {{Short}}-{{Text Classification}} with {{Recurrent}} and {{Convolutional Neural Networks}}},
  url = {http://arxiv.org/abs/1603.03827},
  abstract = {Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.},
  date = {2016-03-11},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  author = {Lee, Ji Young and Dernoncourt, Franck},
  file = {/home/yigit/Zotero/storage/FBZC96VU/Lee and Dernoncourt - 2016 - Sequential Short-Text Classification with Recurren.pdf;/home/yigit/Zotero/storage/VRNR84NP/1603.html}
}

@inproceedings{wang_short_2013,
  title = {Short {{Text Classification Using Wikipedia Concept Based Document Representation}}},
  doi = {10.1109/ITA.2013.114},
  abstract = {Short text classification is a difficult and challenging task in information retrieval systems since the text data is short, sparse and multidimensional. In this paper, we represent short text with Wikipedia concepts for classification. Short document text is mapped to Wikipedia concepts and the concepts are then used to represent document for text categorization. Traditional methods for classification such as SVM can be used to perform text categorization on the Wikipedia concept document representation. Experimental evaluation on real Google search snippets shows that our approach outperforms the traditional BOW method and gives good performance. Although it's not better than the state-of-the-art classifier (see e.g. Phan et al. WWW '08), our method can be easily implemented with low cost.},
  eventtitle = {2013 {{International Conference}} on {{Information Technology}} and {{Applications}}},
  booktitle = {2013 {{International Conference}} on {{Information Technology}} and {{Applications}}},
  date = {2013-11},
  pages = {471-474},
  keywords = {Wikipedia,Support vector machines,Web sites,text analysis,Document Representation,Electronic publishing,Encyclopedias,Google search snippets,Indexes,information retrieval,information retrieval systems,Internet,multidimensional text data,pattern classification,short document text data mapping,short text classification,sparse text data,SVM,text categorization,wikipedia concept document representation},
  author = {Wang, X. and Chen, R. and Jia, Y. and Zhou, B.},
  file = {/home/yigit/Zotero/storage/QFMWU6EI/6710030.html}
}

@article{le_distributed_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.4053},
  primaryClass = {cs},
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  url = {http://arxiv.org/abs/1405.4053},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  date = {2014-05-16},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Artificial Intelligence},
  author = {Le, Quoc V. and Mikolov, Tomas},
  file = {/home/yigit/Zotero/storage/V4EC6U8K/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf;/home/yigit/Zotero/storage/7236JI6V/1405.html}
}

@inproceedings{lesk_automatic_1986,
  location = {{New York, NY, USA}},
  title = {Automatic {{Sense Disambiguation Using Machine Readable Dictionaries}}: {{How}} to {{Tell}} a {{Pine Cone}} from an {{Ice Cream Cone}}},
  isbn = {978-0-89791-224-2},
  url = {http://doi.acm.org/10.1145/318723.318728},
  doi = {10.1145/318723.318728},
  shorttitle = {Automatic {{Sense Disambiguation Using Machine Readable Dictionaries}}},
  booktitle = {Proceedings of the 5th {{Annual International Conference}} on {{Systems Documentation}}},
  series = {{{SIGDOC}} '86},
  publisher = {{ACM}},
  date = {1986},
  pages = {24--26},
  author = {Lesk, Michael}
}

@inproceedings{khodak_automated_2017,
  title = {Automated {{WordNet Construction Using Word Embeddings}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Sense}}, {{Concept}} and {{Entity Representations}} and Their {{Applications}}},
  date = {2017},
  pages = {12--23},
  keywords = {word embedding wordnet},
  author = {Khodak, Mikhail and Risteski, Andrej and Fellbaum, Christiane and Arora, Sanjeev},
  file = {/home/yigit/Zotero/storage/53HG34BT/Khodak et al. - 2017 - Automated WordNet Construction Using Word Embeddin.pdf;/home/yigit/Zotero/storage/SMACXHXW/Khodak et al. - 2017 - Automated WordNet Construction Using Word Embeddin.pdf}
}

@inproceedings{montazery_automatic_2010,
  title = {Automatic {{Persian}} Wordnet Construction},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Computational Linguistics}}: {{Posters}}},
  publisher = {{Association for Computational Linguistics}},
  date = {2010},
  pages = {846--850},
  author = {Montazery, Mortaza and Faili, Heshaam},
  file = {/home/yigit/Zotero/storage/66HWH744/citation.html}
}

@article{jonker_shortest_1987,
  langid = {english},
  title = {A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems},
  volume = {38},
  issn = {1436-5057},
  url = {https://doi.org/10.1007/BF02278710},
  doi = {10.1007/BF02278710},
  abstract = {We develop a shortest augmenting path algorithm for the linear assignment problem. It contains new initialization routines and a special implementation of Dijkstra's shortest path method. For both dense and sparse problems computational experiments show this algorithm to be uniformly faster than the best algorithms from the literature. A Pascal implementation is presented.},
  number = {4},
  journaltitle = {Computing},
  shortjournal = {Computing},
  urldate = {2019-04-04},
  date = {1987-12-01},
  pages = {325-340},
  keywords = {68 E 10,90 C 08,Linear assignment problem,Pascal implementation,shortest path methods},
  author = {Jonker, R. and Volgenant, A.},
  file = {/home/yigit/Zotero/storage/UT9ZEJAS/Jonker and Volgenant - 1987 - A shortest augmenting path algorithm for dense and.pdf}
}

@article{glavas_how_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.00508},
  primaryClass = {cs},
  title = {How to ({{Properly}}) {{Evaluate Cross}}-{{Lingual Word Embeddings}}: {{On Strong Baselines}}, {{Comparative Analyses}}, and {{Some Misconceptions}}},
  url = {http://arxiv.org/abs/1902.00508},
  shorttitle = {How to ({{Properly}}) {{Evaluate Cross}}-{{Lingual Word Embeddings}}},
  abstract = {Cross-lingual word embeddings (CLEs) enable multilingual modeling of meaning and facilitate cross-lingual transfer of NLP models. Despite their ubiquitous usage in downstream tasks, recent increasingly popular projection-based CLE models are almost exclusively evaluated on a single task only: bilingual lexicon induction (BLI). Even BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we make the first step towards a comprehensive evaluation of cross-lingual word embeddings. We thoroughly evaluate both supervised and unsupervised CLE models on a large number of language pairs in the BLI task and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI can result in deteriorated downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess existing baselines, which still display competitive performance across the board. We hope that our work will catalyze further work on CLE evaluation and model analysis.},
  urldate = {2019-04-04},
  date = {2019-02-01},
  keywords = {Computer Science - Computation and Language},
  author = {Glavas, Goran and Litschko, Robert and Ruder, Sebastian and Vulic, Ivan},
  file = {/home/yigit/Zotero/storage/J8FEKW9D/Glavas et al. - 2019 - How to (Properly) Evaluate Cross-Lingual Word Embe.pdf;/home/yigit/Zotero/storage/Y2K45GPP/1902.html}
}

@article{de_boom_representation_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.00570},
  title = {Representation Learning for Very Short Texts Using Weighted Word Embedding Aggregation},
  volume = {80},
  issn = {01678655},
  url = {http://arxiv.org/abs/1607.00570},
  doi = {10.1016/j.patrec.2016.06.012},
  abstract = {Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box.},
  journaltitle = {Pattern Recognition Letters},
  urldate = {2019-03-26},
  date = {2016-09},
  pages = {150-156},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  author = {De Boom, Cedric and Van Canneyt, Steven and Demeester, Thomas and Dhoedt, Bart},
  file = {/home/yigit/Zotero/storage/D3FIFPRV/De Boom et al. - 2016 - Representation learning for very short texts using.pdf;/home/yigit/Zotero/storage/2VVSAQ2D/1607.html}
}

@article{pedersen_dannet_2009,
  langid = {english},
  title = {{{DanNet}}: The Challenge of Compiling a Wordnet for {{Danish}} by Reusing a Monolingual Dictionary},
  volume = {43},
  issn = {1574-0218},
  url = {https://doi.org/10.1007/s10579-009-9092-1},
  doi = {10.1007/s10579-009-9092-1},
  shorttitle = {{{DanNet}}},
  abstract = {This paper is a contribution to the discussion on compiling computational lexical resources from conventional dictionaries. It describes the theoretical as well as practical problems that are encountered when reusing a conventional dictionary for compiling a lexical-semantic resource in terms of a wordnet. More specifically, it describes the methodological issues of compiling a wordnet for Danish, DanNet, from a monolingual basis, and not—as is often seen—by applying the translational expansion method with Princeton WordNet as the English source. Thus, we apply as our basis a large, corpus-based printed dictionary of modern Danish. Using this approach, we discuss the issues of readjusting inconsistent and/or underspecified hyponymy hierarchies taken from the conventional dictionary, sense distinctions as opposed to the synonym sets of wordnets, generating semantic wordnet relations on the basis of sense definitions, and finally, supplementing missing or implicit information.},
  number = {3},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  urldate = {2019-03-23},
  date = {2009-09-01},
  pages = {269-299},
  keywords = {Dictionary,Hyponymy,Lexical semantics,Nouns,Semantic relations,Verbs,Wordnet},
  author = {Pedersen, Bolette Sandford and Nimb, Sanni and Asmussen, Jørg and Sørensen, Nicolai Hartvig and Trap-Jensen, Lars and Lorentzen, Henrik},
  file = {/home/yigit/Zotero/storage/NTFZN3XS/Pedersen et al. - 2009 - DanNet the challenge of compiling a wordnet for D.pdf}
}

@inproceedings{nothman_stop_2018,
  location = {{Melbourne, Australia}},
  title = {Stop {{Word Lists}} in {{Free Open}}-Source {{Software Packages}}},
  url = {http://www.aclweb.org/anthology/W18-2502},
  abstract = {Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. "hasn't" but not "hadn't") and inclusions ("computer"), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP}}-{{OSS}})},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-02-27},
  date = {2018-07},
  pages = {7--12},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  file = {/home/yigit/Zotero/storage/347Z2YQQ/Nothman et al. - 2018 - Stop Word Lists in Free Open-source Software Packa.pdf}
}

@article{cuturi_sinkhorn_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0895},
  primaryClass = {stat},
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transportation Distances}}},
  url = {http://arxiv.org/abs/1306.0895},
  shorttitle = {Sinkhorn {{Distances}}},
  abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
  urldate = {2019-02-22},
  date = {2013-06-04},
  keywords = {Statistics - Machine Learning},
  author = {Cuturi, Marco},
  file = {/home/yigit/Zotero/storage/XQBJZRY9/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf;/home/yigit/Zotero/storage/BGMAQI53/1306.html}
}

@article{peters_deep_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.05365},
  primaryClass = {cs},
  title = {Deep Contextualized Word Representations},
  url = {http://arxiv.org/abs/1802.05365},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  urldate = {2019-02-20},
  date = {2018-02-14},
  keywords = {Computer Science - Computation and Language},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  file = {/home/yigit/Zotero/storage/R3T6EBPM/Peters et al. - 2018 - Deep contextualized word representations.pdf;/home/yigit/Zotero/storage/FR7VQZ2C/1802.html}
}

@article{radford_language_nodate,
  langid = {english},
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  pages = {24},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  file = {/home/yigit/Zotero/storage/AUCIVHKH/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{radford_improving_nodate,
  langid = {english},
  title = {Improving {{Language Understanding}} by {{Generative Pre}}-{{Training}}},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  pages = {12},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  file = {/home/yigit/Zotero/storage/6QEGU8NC/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{grave_unsupervised_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.11222},
  primaryClass = {cs, stat},
  title = {Unsupervised {{Alignment}} of {{Embeddings}} with {{Wasserstein Procrustes}}},
  url = {http://arxiv.org/abs/1805.11222},
  abstract = {We consider the task of aligning two sets of points in high dimension, which has many applications in natural language processing and computer vision. As an example, it was recently shown that it is possible to infer a bilingual lexicon, without supervised data, by aligning word embeddings trained on monolingual data. These recent advances are based on adversarial training to learn the mapping between the two embeddings. In this paper, we propose to use an alternative formulation, based on the joint estimation of an orthogonal matrix and a permutation matrix. While this problem is not convex, we propose to initialize our optimization algorithm by using a convex relaxation, traditionally considered for the graph isomorphism problem. We propose a stochastic algorithm to minimize our cost function on large scale problems. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. On this task, our method obtains state of the art results, while requiring less computational resources than competing approaches.},
  urldate = {2019-02-14},
  date = {2018-05-28},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Grave, Edouard and Joulin, Armand and Berthet, Quentin},
  file = {/home/yigit/Zotero/storage/7HAHD4B8/Grave et al. - 2018 - Unsupervised Alignment of Embeddings with Wasserst.pdf;/home/yigit/Zotero/storage/R8QKKNF2/1805.html}
}

@inproceedings{zhang_building_2016,
  title = {Building {{Earth Mover}}'s {{Distance}} on {{Bilingual Word Embeddings}} for {{Machine Translation}}},
  url = {http://dl.acm.org/citation.cfm?id=3016100.3016303},
  abstract = {Following their monolingual counterparts, bilingual word embeddings are also on the rise. As a major application task, word translation has been relying on the nearest neighbor to connect embeddings cross-lingually. However, the nearest neighbor strategy suffers from its inherently local nature and fails to cope with variations in realistic bilingual word embeddings. Furthermore, it lacks a mechanism to deal with many-to-many mappings that often show up across languages. We introduce Earth Mover's Distance to this task by providing a natural formulation that translates words in a holistic fashion, addressing the limitations of the nearest neighbor. We further extend the formulation to a new task of identifying parallel sentences, which is useful for statistical machine translation systems, thereby expanding the application realm of bilingual word embeddings. We show encouraging performance on both tasks.},
  booktitle = {Proceedings of the {{Thirtieth AAAI Conference}} on {{Artificial Intelligence}}},
  series = {{{AAAI}}'16},
  publisher = {{AAAI Press}},
  urldate = {2019-02-14},
  date = {2016},
  pages = {2870--2876},
  author = {Zhang, Meng and Liu, Yang and Luan, Huanbo and Sun, Maosong and Izuha, Tatsuya and Hao, Jie},
  venue = {Phoenix, Arizona}
}

@article{de_boom_learning_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.00765},
  title = {Learning {{Semantic Similarity}} for {{Very Short Texts}}},
  url = {http://arxiv.org/abs/1512.00765},
  doi = {10.1109/ICDMW.2015.86},
  abstract = {Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.},
  journaltitle = {2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
  urldate = {2019-04-16},
  date = {2015-11},
  pages = {1229-1234},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  author = {De Boom, Cedric and Van Canneyt, Steven and Bohez, Steven and Demeester, Thomas and Dhoedt, Bart},
  file = {/home/yigit/Zotero/storage/VNNPYN8R/De Boom et al. - 2015 - Learning Semantic Similarity for Very Short Texts.pdf;/home/yigit/Zotero/storage/A64EP4R5/1512.html}
}

@article{levy_improving_2015,
  title = {Improving {{Distributional Similarity}} with {{Lessons Learned}} from {{Word Embeddings}}},
  volume = {3},
  doi = {10.1162/tacl_a_00134},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  date = {2015-12-01},
  pages = {211-225},
  author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  file = {/home/yigit/Zotero/storage/CQGXXRS3/Levy et al. - 2015 - Improving Distributional Similarity with Lessons L.pdf}
}

@inproceedings{banerjee_adapted_2002,
  langid = {english},
  title = {An {{Adapted Lesk Algorithm}} for {{Word Sense Disambiguation Using WordNet}}},
  isbn = {978-3-540-45715-2},
  abstract = {This paper presents an adaptation of Lesk’s dictionarybased word sense disambiguation algorithm. Rather than using a standard dictionary as the source of glosses for our approach, the lexical database WordNet is employed. This provides a rich hierarchy of semantic relations that our algorithm can exploit. This method is evaluated using the English lexical sample data from the Senseval-2 word sense disambiguation exercise, and attains an overall accuracy of 32\%. This represents a significant improvement over the 16\% and 23\% accuracy attained by variations of the Lesk algorithm used as benchmarks during the SENSEVAL-2 comparative exercise among word sense disambiguation systems.},
  booktitle = {Computational {{Linguistics}} and {{Intelligent Text Processing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2002},
  pages = {136-145},
  keywords = {Context Window,Function Word,Relation Pair,Target Word,Word Sense Disambiguation},
  author = {Banerjee, Satanjeev and Pedersen, Ted},
  editor = {Gelbukh, Alexander}
}

@article{uzun_modern_2005,
  title = {Modern Dilbilim Bulguları Işığında {{Türkçe}} Sözlüğe Bir Bakış},
  journaltitle = {Çukurova Üniversitesi Türkoloji Araştırmaları Merkezi},
  date = {2005},
  author = {Uzun, Engin Nadir}
}

@article{ibrahim_usta_turkce_2006,
  title = {Türkçe {{Sözlük Hazırlamada Yöntem Sorunları}}},
  doi = {10.1501/Dtcfder_0000001033},
  journaltitle = {Ankara Üniversitesi Dil ve Tarih-Coğrafya Fakültesi Dergisi},
  shortjournal = {Ankara Üniversitesi Dil ve Tarih-Coğrafya Fakültesi Dergisi},
  date = {2006-01-01},
  pages = {223-242},
  author = {İbrahim USTA, Halil},
  file = {/home/yigit/Zotero/storage/UN8NNCV3/İbrahim USTA - 2006 - Türkçe Sözlük Hazırlamada Yöntem Sorunları.pdf}
}

@article{uzun_1945ten_1999,
  title = {1945'{{TEN}}  {{BU YANA}}  {{TÜRKÇE}}  {{SÖZLÜKLER}}},
  issn = {1300-2864},
  number = {7-8},
  journaltitle = {KEBİKEÇ İnsan Bilimleri İçin Kaynak Araştırmaları Dergisi},
  date = {1999},
  pages = {53 - 57},
  author = {Uzun, Leyla}
}

@online{somers_youre_2014,
  langid = {american},
  title = {You’re Probably Using the Wrong Dictionary},
  url = {http://jsomers.net/blog/dictionary},
  urldate = {2019-05-19},
  date = {2014-05-18},
  author = {Somers, James},
  file = {/home/yigit/Zotero/storage/CPYDBQW9/dictionary.html}
}

@article{resnik_distinguishing_1999,
  langid = {english},
  title = {Distinguishing Systems and Distinguishing Senses: New Evaluation Methods for {{Word Sense Disambiguation}}},
  volume = {5},
  issn = {1469-8110, 1351-3249},
  url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitledistinguishing-systems-and-distinguishing-senses-new-evaluation-methods-for-word-sense-disambiguationdiv/5802B206D98444DCB022479D9E45AE90},
  shorttitle = {Distinguishing Systems and Distinguishing Senses},
  abstract = {Resnik and Yarowsky (1997) made a set of observations about the state-of-the-art in automatic
word sense disambiguation and, motivated by those observations, offered several specific
proposals regarding improved evaluation criteria, common training and testing resources,
and the definition of sense inventories. Subsequent discussion of those proposals resulted
in SENSEVAL, the first evaluation exercise for word sense disambiguation (Kilgarriff and
Palmer 2000). This article is a revised and extended version of our 1997 workshop paper,
reviewing its observations and proposals and discussing them in light of the SENSEVAL exercise.
It also includes a new in-depth empirical study of translingually-based sense inventories
and distance measures, using statistics collected from native-speaker annotations of 222
polysemous contexts across 12 languages. These data show that monolingual sense distinctions
at most levels of granularity can be effectively captured by translations into some set of second
languages, especially as language family distance increases. In addition, the probability that
a given sense pair will tend to lexicalize differently across languages is shown to correlate
with semantic salience and sense granularity; sense hierarchies automatically generated from
such distance matrices yield results remarkably similar to those created by professional
monolingual lexicographers.},
  number = {2},
  journaltitle = {Natural Language Engineering},
  urldate = {2019-05-22},
  date = {1999-06},
  pages = {113-133},
  author = {Resnik, Philip and Yarowsky, David},
  file = {/home/yigit/Zotero/storage/6NRSBW77/Resnik and Yarowsky - 1999 - Distinguishing systems and distinguishing senses .pdf;/home/yigit/Zotero/storage/MG5E7VVY/5802B206D98444DCB022479D9E45AE90.html}
}

@article{sagot_building_2008,
  title = {Building a Free {{French}} Wordnet from Multilingual Resources},
  abstract = {This paper describes automatic construction a freely-available wordnet for French (WOLF) based on Princeton WordNet (PWN) by using various multilingual resources. Polysemous words were dealt with an approach in which a parallel corpus for five languages was word-aligned and the extracted multilingual lexicon was disambiguated with the existing wordnets for these languages. On the other hand, a bilingual approach sufficed to acquire equivalents for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri. The results obtained from each resource were merged and ranked according to the number of resources yielding the same literal. Automatic evaluation of the merged wordnet was performed with the French WordNet (FREWN). Manual evaluation was also carried out on a sample of the generated synsets. Precision shows that the presented approach has proved to be very promising and applications to use the created wordnet are already intended.},
  date = {2008-05-31},
  author = {Sagot, Benoît and Fišer, Darja},
  file = {/home/yigit/Zotero/storage/MBQ5UPWA/Sagot and Fiser - 2008 - Building a free French wordnet from multilingual r.pdf}
}

@collection{sterkenburg_practical_2003,
  langid = {english},
  location = {{Amsterdam}},
  title = {A Practical Guide to Lexicography},
  isbn = {978-90-272-2329-6 978-90-272-2330-2 978-1-58811-380-1 978-1-58811-381-8},
  pagetotal = {459},
  number = {6},
  series = {Terminology and Lexicography Research and Practice},
  publisher = {{Benjamins}},
  date = {2003},
  editor = {van Sterkenburg, Piet},
  file = {/home/yigit/Zotero/storage/ILPVSRRX/Sterkenburg - 2003 - A practical guide to lexicography.pdf},
  note = {OCLC: 249659375}
}

@inproceedings{maslova_neural_2017,
  title = {Neural Network Doc2vec in Automated Sentiment Analysis for Short Informal Texts},
  booktitle = {International {{Conference}} on {{Speech}} and {{Computer}}},
  publisher = {{Springer}},
  date = {2017},
  pages = {546--554},
  author = {Maslova, Natalia and Potapov, Vsevolod},
  file = {/home/yigit/Zotero/storage/SPCQ9GW8/Maslova and Potapov - 2017 - Neural network doc2vec in automated sentiment anal.pdf;/home/yigit/Zotero/storage/A9TZKQ8Y/978-3-319-66429-3_54.html}
}

@inproceedings{bengio_neural_2000,
  title = {A {{Neural Probabilistic Language Model}}},
  volume = {3},
  doi = {10.1162/153244303322533223},
  abstract = {A goal of statistical language modeling is to learn the joint probabilit y function of sequences of words. This is intrinsically difficult because o f the curse of dimensionality: we propose to fight it with its own weap ons. In the proposed approach one learns simultaneously (1) a distributed r ep- resentation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these repr e- sentations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, sh owing on two text corpora that the proposed approach very significantly im- proves on a state-of-the-art trigram model.},
  eventtitle = {Journal of {{Machine Learning Research}}},
  date = {2000-01-01},
  pages = {932-938},
  author = {Bengio, Y and Ducharme, Réjean and Vincent, Pascal},
  file = {/home/yigit/Zotero/storage/QW3T28FK/Bengio et al. - 2000 - A Neural Probabilistic Language Model.pdf}
}

@article{edilson_a._correa_nilc-usp_2017,
  title = {{{NILC}}-{{USP}} at {{SemEval}}-2017 {{Task}} 4: {{A Multi}}-View {{Ensemble}} for {{Twitter Sentiment Analysis}}},
  shorttitle = {{{NILC}}-{{USP}} at {{SemEval}}-2017 {{Task}} 4},
  abstract = {This paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall.},
  date = {2017-04-07},
  author = {Edilson A. Corrêa, Jr and Marinho, Vanessa and Borges dos Santos, Leandro},
  file = {/home/yigit/Zotero/storage/D6TNF56U/Edilson A. Corrêa et al. - 2017 - NILC-USP at SemEval-2017 Task 4 A Multi-view Ense.pdf}
}

@inproceedings{gordeev_unsupervised_2018,
  title = {Unsupervised {{Cross}}-Lingual {{Matching}} of {{Product Classifications}}},
  url = {http://dl.acm.org/citation.cfm?id=3299905.3299967},
  booktitle = {Proceedings of the 23rd {{Conference}} of {{Open Innovations Association FRUCT}}},
  series = {{{FRUCT}}'23},
  publisher = {{FRUCT Oy}},
  date = {2018},
  pages = {62:459-62:464},
  keywords = {word embeddings,cross-lingual embeddings,unsupervised category matching},
  author = {Gordeev, Denis and Rey, Alexey and Shagarov, Dmitry},
  file = {/home/yigit/Zotero/storage/APF99QHB/Gordeev et al. - 2018 - Unsupervised Cross-lingual Matching of Product Cla.pdf},
  venue = {Bologna, Italy},
  articleno = {62},
  numpages = {6},
  acmid = {3299967}
}

@article{miller_wordnet_1995,
  title = {{{WordNet}}: {{A Lexical Database}} for {{English}}},
  volume = {38},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/219717.219748},
  doi = {10.1145/219717.219748},
  shorttitle = {{{WordNet}}},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  number = {11},
  journaltitle = {Commun. ACM},
  urldate = {2019-05-20},
  date = {1995-11},
  pages = {39--41},
  author = {Miller, George A.},
  file = {/home/yigit/Zotero/storage/EAU8RULD/Miller - 1995 - WordNet A Lexical Database for English.pdf}
}

@inproceedings{fiser_slownet_2012,
  title = {{{sloWNet}} 3.0: Development, Extension and Cleaning},
  shorttitle = {{{sloWNet}} 3.0},
  booktitle = {Proceedings of 6th {{International Global Wordnet Conference}} ({{GWC}} 2012)},
  date = {2012},
  pages = {113--117},
  author = {Fišer, Darja and Novak, Jernej and Erjavec, Tomaž},
  file = {/home/yigit/Zotero/storage/K5MC7PA9/Fišer et al. - 2012 - sloWNet 3.0 development, extension and cleaning.pdf}
}

@inproceedings{tufis_romanian_2008,
  title = {Romanian Wordnet: {{Current}} State, New Applications and Prospects},
  shorttitle = {Romanian Wordnet},
  booktitle = {Proceedings of 4th {{Global WordNet Conference}}, {{GWC}}},
  date = {2008},
  pages = {441--452},
  author = {Tufiş, Dan and Ion, Radu and Bozianu, Luigi and Ceauşu, Alexandru and Ştefănescu, Dan},
  file = {/home/yigit/Zotero/storage/CADVMCCW/Tufiş et al. - 2008 - Romanian wordnet Current state, new applications .pdf}
}

@inproceedings{stamou_exploring_2004,
  title = {Exploring {{Balkanet Shared Ontology}} for {{Multilingual Conceptual Indexing}}.},
  booktitle = {{{LREC}}},
  date = {2004},
  author = {Stamou, Sofia and Nenadic, Goran and Christodoulakis, Dimitris},
  file = {/home/yigit/Zotero/storage/B7HHGLHR/Stamou et al. - 2004 - Exploring Balkanet Shared Ontology for Multilingua.pdf}
}

@inproceedings{simov_constructing_2010,
  title = {Constructing of an {{Ontology}}-Based {{Lexicon}} for {{Bulgarian}}.},
  booktitle = {{{LREC}}},
  publisher = {{Citeseer}},
  date = {2010},
  author = {Simov, Kiril Ivanov and Osenova, Petya},
  file = {/home/yigit/Zotero/storage/GUC2U3NG/Simov and Osenova - 2010 - Constructing of an Ontology-based Lexicon for Bulg.pdf}
}

@report{ruci_current_2008,
  title = {On the Current State of {{Albanet}} and Related Applications},
  institution = {{Technical report, University of Vlora.(http://fjalnet. com …}},
  date = {2008},
  author = {Ruci, Ervin}
}

@article{bond_survey_2012,
  title = {A {{Survey}} of {{WordNets}} and Their {{Licenses}}},
  abstract = {This paper surveys currently avail-able wordnets. We measure the ef-fect that license choice has on their us-age, measured by the number of cita-tions. Finally, we discuss methods to make wordnets more generally accessi-ble, starting with a shared online server for freely distributable wordnets.},
  date = {2012-01-01},
  author = {Bond, Francis and Paik, Kyonghee},
  file = {/home/yigit/Zotero/storage/DQL7LC9A/Bond and Paik - 2012 - A Survey of WordNets and their Licenses.pdf}
}

@article{arora_simple_2016,
  title = {A {{Simple}} but {{Tough}}-to-{{Beat Baseline}} for {{Sentence Embeddings}}},
  url = {https://openreview.net/forum?id=SyK00v5xx},
  abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs....},
  urldate = {2019-05-07},
  date = {2016-11-04},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  file = {/home/yigit/Zotero/storage/ZCAPE7A7/Arora et al. - 2016 - A Simple but Tough-to-Beat Baseline for Sentence E.pdf;/home/yigit/Zotero/storage/F2DLKYWI/forum.html}
}

@book{manning_introduction_2009,
  langid = {english},
  location = {{Cambridge}},
  title = {Introduction to Information Retrieval},
  edition = {Reprinted},
  isbn = {978-0-521-86571-5},
  pagetotal = {482},
  publisher = {{Cambridge Univ. Press}},
  date = {2009},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  file = {/home/yigit/Zotero/storage/VL52WU6Y/Manning et al. - 2009 - Introduction to information retrieval.pdf},
  note = {OCLC: 549201180}
}

@article{jawanpuria_learning_2019,
  title = {Learning {{Multilingual Word Embeddings}} in {{Latent Metric Space}}: {{A Geometric Approach}}},
  volume = {7},
  url = {https://doi.org/10.1162/tacl_a_00257},
  doi = {10.1162/tacl_a_00257},
  shorttitle = {Learning {{Multilingual Word Embeddings}} in {{Latent Metric Space}}},
  abstract = {We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks.We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  urldate = {2019-05-02},
  date = {2019-03-01},
  pages = {107-120},
  author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev}
}

@thesis{aldarmaki_cross-lingual_2019,
  langid = {english},
  location = {{United States -- District of Columbia}},
  title = {Cross-{{Lingual Alignment}} of {{Word}} \& {{Sentence Embeddings}}},
  url = {https://search.proquest.com/docview/2206636278/abstract/8A87D7B035414476PQ/1},
  abstract = {One of the notable developments in current natural language processing is the practical efficacy of probabilistic word representations, where words are embedded in high-dimensional continuous vector spaces that are optimized to reflect their distributional relationships. For sequences of words, such as phrases and sentences, distributional representations can be estimated by combining word embeddings using arithmetic operations like vector averaging or by estimating composition parameters from data using various objective functions. The quality of these compositional representations is typically estimated by their performance as features in extrinsic supervised classification benchmarks. Word and compositional embeddings for a single language can be induced without supervision using a large training corpus of raw text. To handle multiple languages and dialects, bilingual dictionaries and parallel corpora are often used for learning cross-lingual embeddings directly or to align pre-trained monolingual embeddings.
In this work, we explore and develop various cross-lingual alignment techniques, compare the performance of the resulting cross-lingual embeddings, and study their characteristics. We pay particular attention to the bilingual data requirements of each approach since lower requirements facilitate wider language expansion. To begin with, we analyze various monolingual general-purpose sentence embedding models to better understand their qualities. By comparing their performance on extrinsic evaluation benchmarks and unsupervised clustering, we infer the characteristics of the most dominant features in their respective vector spaces.
We then look into various cross-lingual alignment frameworks with different degrees of supervision. We begin with unsupervised word alignment, for which we propose an approach for inducing cross-lingual word mappings with no prior bilingual resources. We rely on assumptions about the consistency and structural similarities between the monolingual vector spaces of different languages. Using comparable monolingual news corpora, our approach resulted in highly accurate word mappings for two language pairs: French to English, and Arabic to English. With various refinement heuristics, the performance of the unsupervised alignment methods approached the performance of supervised dictionary mapping.
Finally, we develop and evaluate different alignment approaches based on parallel text. We show that incorporating context in the alignment process often leads to significant improvements in performance. At the word level, we explore the alignment of contextualized word embeddings that are dynamically generated for each sentence. At the sentence level, we develop and investigate three alignment frameworks: joint modeling, representation transfer, and sentence mapping, applied to different sentence embedding models. We experiment with a matrix factorization model based on word-sentence co-occurrence statistics, and two general-purpose neural sentence embedding models. We report the performance of the various cross-lingual models with different sizes of parallel corpora to assess the minimal degree of supervision required by each alignment framework.},
  pagetotal = {113},
  institution = {{The George Washington University}},
  type = {Ph.D.},
  urldate = {2019-04-24},
  date = {2019},
  keywords = {Applied sciences,Cross-lingual embeddings,Language,literature and linguistics,NLP,Sentence embeddings,Unsupervised mapping,Word embeddings,Word translation},
  author = {Aldarmaki, Hanan},
  file = {/home/yigit/Zotero/storage/5S9NLD8M/Aldarmaki - 2019 - Cross-Lingual Alignment of Word & Sentence Embeddi.pdf}
}

@article{singh_context_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.09663},
  primaryClass = {cs, stat},
  title = {Context {{Mover}}'s {{Distance}} \& {{Barycenters}}: {{Optimal}} Transport of Contexts for Building Representations},
  url = {http://arxiv.org/abs/1808.09663},
  shorttitle = {Context {{Mover}}'s {{Distance}} \& {{Barycenters}}},
  abstract = {We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a fixed length vector. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider the problem of representation learning with a perspective from Optimal Transport and take advantage of its numerous tools such as Wasserstein distance and Wasserstein barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance quantitatively as well as qualitatively on tasks such as measuring sentence similarity and word entailment, where we empirically observe significant gains (e.g., 4.1\% relative improvement over Sent2vec and GenSen). The key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. In essence, the framework can be useful for any unsupervised or supervised problem (on text or other modalities); and only requires a co-occurrence structure inherent to many problems. The code, as well as pre-built histograms, are available under https://github.com/context-mover/.},
  urldate = {2019-04-24},
  date = {2018-08-29},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Singh, Sidak Pal and Hug, Andreas and Dieuleveut, Aymeric and Jaggi, Martin},
  file = {/home/yigit/Zotero/storage/47KHRGPC/Singh et al. - 2018 - Context Mover's Distance & Barycenters Optimal tr.pdf;/home/yigit/Zotero/storage/NC3UPLG5/1808.html}
}

@article{conneau_xnli_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.05053},
  primaryClass = {cs},
  title = {{{XNLI}}: {{Evaluating Cross}}-Lingual {{Sentence Representations}}},
  url = {http://arxiv.org/abs/1809.05053},
  shorttitle = {{{XNLI}}},
  abstract = {State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.},
  urldate = {2019-04-24},
  date = {2018-09-13},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R. and Schwenk, Holger and Stoyanov, Veselin},
  file = {/home/yigit/Zotero/storage/JN25KTGH/Conneau et al. - 2018 - XNLI Evaluating Cross-lingual Sentence Representa.pdf;/home/yigit/Zotero/storage/NUZTKPVQ/1809.html}
}

@article{mu_all-but--top_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.01417},
  primaryClass = {cs, stat},
  title = {All-but-the-{{Top}}: {{Simple}} and {{Effective Postprocessing}} for {{Word Representations}}},
  url = {http://arxiv.org/abs/1702.01417},
  shorttitle = {All-but-the-{{Top}}},
  abstract = {Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a \{\textbackslash{}em very simple\}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations \{\textbackslash{}em even stronger\}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and \{ text classification\}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.},
  urldate = {2019-04-24},
  date = {2017-02-05},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  author = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
  file = {/home/yigit/Zotero/storage/CD9TDZZR/Mu et al. - 2017 - All-but-the-Top Simple and Effective Postprocessi.pdf;/home/yigit/Zotero/storage/HILLFWK4/1702.html}
}

@book{klementiev_inducing_2012,
  title = {Inducing {{Crosslingual Distributed Representations}} of {{Words}}},
  abstract = {Distributed representations of words have proven extremely useful in numerous natural language processing tasks. Their appeal is that they can help alleviate data sparsity problems common to supervised learning. Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages. In this work, we induce distributed representations for a pair of languages jointly. We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another. We show that our representations are informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language.},
  date = {2012},
  author = {Klementiev, Alexandre and Titov, Ivan and Bhattarai, Binod},
  file = {/home/yigit/Zotero/storage/UPMFIE54/Klementiev et al. - 2012 - Inducing Crosslingual Distributed Representations .pdf;/home/yigit/Zotero/storage/WTDG675R/summary.html}
}

@inproceedings{xiao_distributed_2014,
  title = {Distributed {{Word Representation Learning}} for {{Cross}}-{{Lingual Dependency Parsing}}},
  doi = {10.3115/v1/W14-1613},
  abstract = {This paper proposes to learn languageindependent word representations to address cross-lingual dependency parsing, which aims to predict the dependency parsing trees for sentences in the target language by training a dependency parser with labeled sentences from a source language. We first combine all sentences from both languages to induce real-valued distributed representation of words under a deep neural network architecture, which is expected to capture semantic similarities of words not only within the same language but also across different languages. We then use the induced interlingual word representation as augmenting features to train a delexicalized dependency parser on labeled sentences in the source language and apply it to the target sentences. To investigate the effectiveness of the proposed technique, extensive experiments are conducted on cross-lingual dependency parsing tasks with nine different languages. The experimental results demonstrate the superior cross-lingual generalizability of the word representation induced by the proposed approach, comparing to alternative comparison methods.},
  date = {2014-01-01},
  pages = {119-129},
  author = {Xiao, Min and Guo, Yuhong},
  file = {/home/yigit/Zotero/storage/EIX39ZF3/Xiao and Guo - 2014 - Distributed Word Representation Learning for Cross.pdf}
}

@book{farreres_towards_2004,
  title = {Towards {{Binding Spanish Senses}} to {{Wordnet Senses}}},
  abstract = {This work tries to enrich the Spanish Wordnet using a Spanish taxonomy  as a knowledge source. The Spanish taxonomy is composed by Spanish senses, while  Wordnet is composed by synsets (English senses). A set of weighted associations  between Spanish words and Wordnet synsets is used for inferring associations between  both taxonomies.},
  date = {2004},
  author = {Farreres, Javier and Gibert, Karina and Rodríguez, Horacio},
  file = {/home/yigit/Zotero/storage/F3VTKBV4/Alignment et al. - 2004 - Towards Binding Spanish Senses to Wordnet Senses.pdf;/home/yigit/Zotero/storage/63QJXR4Q/summary.html}
}

@online{farreres_using_1998,
  langid = {english},
  title = {Using {{WordNet}} for {{Building WordNets}}},
  url = {/paper/Using-WordNet-for-Building-WordNets-Farreres-Rigau/01405726f0dac56b063cb4ee04e766daf3993c23},
  urldate = {2019-05-26},
  date = {1998},
  author = {Farreres, Xavier and Rigau, German and Rodríguez, Horacio},
  file = {/home/yigit/Zotero/storage/3X6GVUW9/Farreres et al. - 1998 - Using WordNet for Building WordNets.pdf;/home/yigit/Zotero/storage/JQKY6K9J/01405726f0dac56b063cb4ee04e766daf3993c23.html}
}

@inproceedings{knight_building_1994,
  title = {Building a Large-Scale Knowledge Base for Machine Translation},
  abstract = {Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PAN-GLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.},
  booktitle = {In {{Proceedings}} of {{AAAI}}},
  date = {1994},
  author = {Knight, Kevin and Luk, Steve K.},
  file = {/home/yigit/Zotero/storage/ZIWINL2B/Knight and Luk - 1994 - Building a large-scale knowledge base for machine .pdf;/home/yigit/Zotero/storage/KNYD8SXY/summary.html}
}

@incollection{peters_cross-linguistic_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {Cross-Linguistic {{Alignment}} of {{Wordnets}} with an {{Inter}}-{{Lingual}}-{{Index}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_7},
  abstract = {This paper discusses the design of the EuroWordNet database, in which semantic databases like WordNetl.5 for several languages are combined via a so-called inter-lingual-index. In this database, language-independent data is shared whilst language-specific properties are maintained. A special interface has been developed to compare the semantic configurations across languages and to track down differences.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {149-179},
  keywords = {aligning wordnets,equivalence relations,multilingual database},
  author = {Peters, Wim and Vossen, Piek and Díez-Orzas, Pedro and Adriaens, Geert},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_7}
}

@incollection{gonzalo_applying_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {Applying {{EuroWordNet}} to {{Cross}}-{{Language Text Retrieval}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_5},
  abstract = {We discuss ways in which EuroWordNet (EWN) can be used in multilingual information retrieval activities, focusing on two approaches to Cross-Language Text Retrieval that use the EWN database as a large-scale multilingual semantic resource. The first approach indexes documents and queries in terms of the EuroWordNet Inter-Lingual-Index, thus turning term weighting and query/document matching into language-independent tasks. The second describes how the information in the EWN database could be integrated with a corpus-based technique, thus allowing retrieval of domain-specific terms that may not be present in our multilingual database. Our objective is to show the potential of EuroWordNet as a promising alternative to existing approaches to Cross-Language Text Retrieval.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {113-135},
  keywords = {cross-language text retrieval,large-scale ontologies,multilingual lexical resources},
  author = {Gonzalo, Julio and Verdejo, Felisa and Peters, Carol and Calzolari, Nicoletta},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_5}
}

@incollection{alonge_linguistic_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {The {{Linguistic Design}} of the {{EuroWordNet Database}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_2},
  abstract = {In this paper the linguistic design of the database under construction within the EuroWordNet project is described. This is mainly structured along the same lines as the Princeton Word-Net, although some changes have been made to the WordNet overall design due to both theoretical and practical reasons. The most important reasons for such changes are the multilinguality of the EuroWordNet database and the fact that it is intended to be used in Language Engineering applications. Thus, i) some relations have been added to those identified in WordNet; ii) some labels have been identified which can be added to the relations in order to make their implications more explicit and precise; iii) some relations, already present in the WordNet design, have been modified in order to specify their role more clearly.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {19-43},
  keywords = {equivalence relations,language-internal relations,lexical-semantic relations,synset},
  author = {Alonge, Antonietta and Calzolari, Nicoletta and Vossen, Piek and Bloksma, Laura and Castellon, Irene and Marti, Maria Antonia and Peters, Wim},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_2}
}

@incollection{rodriguez_top-down_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {The {{Top}}-{{Down Strategy}} for {{Building EuroWordNet}}: {{Vocabulary Coverage}}, {{Base Concepts}} and {{Top Ontology}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_3},
  shorttitle = {The {{Top}}-{{Down Strategy}} for {{Building EuroWordNet}}},
  abstract = {This paper describes two fundamental aspects in the process of building of the EuroWordNet database. In EuroWordNet we have chosen for a flexible design in which local wordnets are built relatively independently as language-specific structures, which are linked to an Inter-Lingual-Index (ILI). To ensure compatibility between the wordnets, a core set of common concepts has been defined that has to be covered by every language. Furthermore, these concepts have been classified via the ILI in terms of a Top Ontology of 63 fundamental semantic distinctions used in various semantic theories and paradigms. This paper first discusses the process leading to the definition of the set of Base Concepts, and the structure and the rationale of the Top Ontology.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {45-80},
  keywords = {Base Concepts,ontology building,Top Ontology},
  author = {Rodríguez, Horacio and Climent, Salvador and Vossen, Piek and Bloksma, Laura and Peters, Wim and Alonge, Antonietta and Bertagna, Francesca and Roventini, Adriana},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_3}
}

@incollection{vossen_compatibility_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {Compatibility in {{Interpretation}} of {{Relations}} in {{EuroWordNet}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_4},
  abstract = {This paper describes how the Euro WordNet project established a maximum level of consensus in the interpretation of relations, without loosing the possibility of encoding language-specific lexicalizations. Problematic cases arise due to the fact that each site re-used different resources and because the core vocabulary of the wordnets show complex properties. Many of these cases are discussed with respect to language internal and equivalence relations. Possible solutions are given in the form of additional criteria.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {81-112},
  keywords = {overlapping relations and lexical gaps,sense differentiation},
  author = {Vossen, Piek and Bloksma, Laura and Alonge, Antonietta and Marinai, Elisabetta and Peters, Carol and Castellon, Irene and Marti, Antonia and Rigau, German},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_4}
}

@article{winston_taxonomy_1987,
  langid = {english},
  title = {A {{Taxonomy}} of {{Part}}-{{Whole Relations}}},
  volume = {11},
  issn = {1551-6709},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1104_2},
  doi = {10.1207/s15516709cog1104_2},
  abstract = {A taxonomy of part-whole or meronymic relations is developed to explain the ordinary English-speaker's use of the term “part of” and its cognates. The resulting classification yields six types of meronymic relations: 1. component-integral object (pedal-bike), 2. member-collection (ship-fleet), 3. portion-mass (slice-pie), 4. stuff-object (steel-car), 5. feature-activity (paying-shopping), and 6. place-area (Everglades-Florida). Meronymic relations ore further distinguished from other inclusion relations, such as spatial inclusion, and class inclusion, and from several other semantic relations: attribution, attachment, and ownership. This taxonomy is then used to explain cases of apparent intransitivity in merological syllogisms, and standard form syllogisms whose premises express different inclusion relations. The data suggest that intransitivities arise due to equivocations between different types of semantic relations. These results are then explained by means of the relation element theory which accounts for the character and behavior of semantic relations in terms of more primitive relational elements. The inferential phenomena observed are then explained by means of a single principle of element matching.},
  number = {4},
  journaltitle = {Cognitive Science},
  urldate = {2019-05-26},
  date = {1987},
  pages = {417-444},
  author = {Winston, Morton E. and Chaffin, Roger and Herrmann, Douglas},
  file = {/home/yigit/Zotero/storage/R5VWVZMF/Winston et al. - 1987 - A Taxonomy of Part-Whole Relations.pdf;/home/yigit/Zotero/storage/ASMZZYGD/s15516709cog1104_2.html}
}

@article{miller_nouns_1990,
  langid = {english},
  title = {Nouns in {{WordNet}}: {{A Lexical Inheritance System}}},
  volume = {3},
  issn = {0950-3846},
  url = {https://academic.oup.com/ijl/article/3/4/245/923281},
  doi = {10.1093/ijl/3.4.245},
  shorttitle = {Nouns in {{WordNet}}},
  abstract = {Abstract.  Definitions of common nouns typically give a superordinate term plus distinguishing features; that information provides the basis for organizing noun},
  number = {4},
  journaltitle = {International Journal of Lexicography},
  shortjournal = {Int J Lexicography},
  urldate = {2019-05-26},
  date = {1990-12-01},
  pages = {245-264},
  author = {Miller, George A.},
  file = {/home/yigit/Zotero/storage/FIYEU93U/923281.html}
}

@article{pianta_multiwordnet_2002,
  title = {{{MultiWordNet}}: {{Developing}} an {{Aligned Multilingual Database}}},
  shorttitle = {{{MultiWordNet}}},
  date = {2002-01-01},
  author = {Pianta, Emanuele and Bentivogli, Luisa and Girardi, C}
}

@inproceedings{fiser_leveraging_2009,
  title = {Leveraging {{Parallel Corpora}} and {{Existing Wordnets}} for {{Automatic Construction}} of the {{Slovene Wordnet}}},
  doi = {10.1007/978-3-642-04235-5_31},
  abstract = {The paper reports on a series of experiments conducted in order to test the feasibility of automatically generating synsets
for Slovene wordnet. The resources used were the multilingual parallel corpus of George Orwell’s Nineteen Eighty-Four and
wordnets for several languages. First, the corpus was word-aligned to obtain multilingual lexicons and then these lexicons
were compared to the wordnets in various languages in order to disambiguate the entries and attach appropriate synset ids
to Slovene entries in the lexicon. Slovene lexicon entries sharing the same attached synset id were then organized into a
synset. The results obtained by the different settings in the experiment are evaluated against a manually created gold standard
and also checked by hand.},
  date = {2009-08-25},
  pages = {359-368},
  author = {Fiser, Darja},
  file = {/home/yigit/Zotero/storage/3DMQJC4L/Fiser - 2009 - Leveraging Parallel Corpora and Existing Wordnets .pdf}
}

@article{vossen_introduction_1998,
  title = {Introduction to {{EuroWordNet}}},
  volume = {32},
  issn = {0010-4817},
  url = {https://www.jstor.org/stable/30200456},
  abstract = {[This paper gives a global introduction to the aims and objectives of the EuroWordNet project, and it provides a general framework for the other papers in this volume. EuroWordNet is an EC project that develops a multilingual database with wordnets in several European languages, structured along the same lines as the Princeton WordNet. Each wordnet represents an autonomous structure of language-specific lexicalizations, which are interconnected via an Inter-Lingual-Index. The wordnets are built at different sites from existing resources, starting from a shared level of basic concepts and extended top-down. The results will be publicly available and will be tested in cross-language information retrieval applications.]},
  number = {2/3},
  journaltitle = {Computers and the Humanities},
  urldate = {2019-05-26},
  date = {1998},
  pages = {73-89},
  author = {Vossen, Piek},
  file = {/home/yigit/Zotero/storage/G9QVFQ7K/Vossen - 1998 - Introduction to EuroWordNet.pdf}
}

@article{linden_finnwordnet_2010,
  title = {{{FinnWordNet}} --- {{WordNet}} Påfinska via Översättning},
  volume = {17},
  journaltitle = {LexicoNordica --- Nordic Journal of Lexicography},
  date = {2010},
  pages = {119-140},
  author = {Lindén, Krister and Carlson., Lauri},
  note = {In Swedish with an English abstract}
}

@book{fellbaum_wordnet_1998,
  title = {{{WordNet}} : An Electronic Lexical Database},
  isbn = {978-0-262-27255-1},
  shorttitle = {{{WordNet}}},
  abstract = {Summary: with a preface by George Miller WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets.The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.Contributors : Reem Al-Halimi, Robert C. Berwick, J. F. M. Burg, Martin Chodorow, Christiane Fellbaum, Joachim Grabowski, Sanda Harabagiu, Marti A. Hearst, Graeme Hirst, Douglas A. Jones, Rick Kazman, Karen T. Kohl, Shari Landes, Claudia Leacock, George A. Miller, Katherine J. Miller, Dan Moldovan, Naoyuki Nomura, Uta Priss, Philip Resnik, David St-Onge, Randee Tengi, Reind P. van de Riet, Ellen Voorhees.},
  series = {Language, Speech, and Communication},
  publisher = {{MIT Press}},
  date = {1998},
  keywords = {Electronic books,English language -- Data processing,Lexicology -- Data processing,Semantics -- Data processing,WordNet},
  author = {Fellbaum, Christiane}
}

@inproceedings{kusner_word_2015,
  title = {From {{Word Embeddings}} to {{Document Distances}}},
  url = {http://dl.acm.org/citation.cfm?id=3045118.3045221},
  abstract = {We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  series = {{{ICML}}'15},
  publisher = {{JMLR.org}},
  urldate = {2019-05-25},
  date = {2015},
  pages = {957--966},
  author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
  venue = {Lille, France}
}

@article{balikas_cross-lingual_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.04437},
  primaryClass = {cs, stat},
  title = {Cross-Lingual {{Document Retrieval}} Using {{Regularized Wasserstein Distance}}},
  url = {http://arxiv.org/abs/1805.04437},
  abstract = {Many information retrieval algorithms rely on the notion of a good distance that allows to efficiently compare objects of different nature. Recently, a new promising metric called Word Mover's Distance was proposed to measure the divergence between text passages. In this paper, we demonstrate that this metric can be extended to incorporate term-weighting schemes and provide more accurate and computationally efficient matching between documents using entropic regularization. We evaluate the benefits of both extensions in the task of cross-lingual document retrieval (CLDR). Our experimental results on eight CLDR problems suggest that the proposed methods achieve remarkable improvements in terms of Mean Reciprocal Rank compared to several baselines.},
  urldate = {2019-05-25},
  date = {2018-05-11},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  author = {Balikas, Georgios and Laclau, Charlotte and Redko, Ievgen and Amini, Massih-Reza},
  file = {/home/yigit/Zotero/storage/YHEQQYC5/Balikas et al. - 2018 - Cross-lingual Document Retrieval using Regularized.pdf;/home/yigit/Zotero/storage/XX9I4BC9/1805.html}
}

@article{mikolov_distributed_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.4546},
  primaryClass = {cs, stat},
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  url = {http://arxiv.org/abs/1310.4546},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  urldate = {2019-05-25},
  date = {2013-10-16},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  file = {/home/yigit/Zotero/storage/CBJYIIJC/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/home/yigit/Zotero/storage/EUSP9GW3/1310.html}
}

@book{kendall_forgotten_2011,
  langid = {english},
  title = {The {{Forgotten Founding Father}}: {{Noah Webster}}'s {{Obsession}} and the {{Creation}} of an {{American Culture}}},
  edition = {1st Edition},
  isbn = {0-399-15699-2},
  abstract = {In 1828 Noah Webster published the groundbreaking American Dictionary of the English Language and secured his niche as an avatar of a distinct American culture. Kendall (The Man Who Made Lists) honors Webster's crucial contributions to early American nationalism, which extended far beyond his primary obsession, the written word. Kendall paints a complex portrait of Webster (1758–1843), a man he claims "housed a host of contradictory identities: revolutionary, reactionary, fighter, peacemaker, intellectual, commonsense philosopher, ladies' man, prig, slick networker and loner." In spite of his flaws, Webster, Kendall argues not wholly successfully, belongs among the ranks of America's notable founders, associating with George Washington and Ben Franklin, among others, to craft an early American identity rooted in national pride and a distinctly American lexicon. Citing frequent references to Webster's nervous afflictions, Kendall ventures the somewhat shaky diagnosis of obsessive-compulsive disorder. The book includes the politics of the "forgotten" founder, for example, noting that Webster "detested Andrew Jackson as the second coming of Jefferson," and a wide range of his activities, including helping found Amherst College. Kendall provides an intriguing look at one of America's earliest men of letters that is sure to appeal to lovers of both words and history. (Apr.)},
  pagetotal = {368},
  publisher = {{G.P. Putnam's Sons}},
  date = {2011-04-14},
  author = {Kendall, Joshua}
}

@incollection{fellbaum_semantic_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {A {{Semantic Network}} of {{English}}: {{The Mother}} of {{All WordNets}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_6},
  shorttitle = {A {{Semantic Network}} of {{English}}},
  abstract = {We give a brief outline of the design and contents of the English lexical database WordNet, which serves as a model for similarly conceived wordnets in several European languages. WordNet is a semantic network, in which the meanings of nouns, verbs, adjectives, and adverbs are represented in terms of their links to other (groups of) words via conceptual-semantic and lexical relations. Each part of speech is treated differently reflecting different semantic properties. We briefly discuss polysemy in WordNet, and focus on the case of meaning extensions in the verb lexicon Finally, we outline the potential uses of WordNet not only for applications in natural language processing, but also for research in stylistic analyses in conjunction with a semantic concordance.1},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-23},
  date = {1998},
  pages = {137-148},
  keywords = {lexicon,Natural Language Processing,semantic network},
  author = {Fellbaum, Christiane},
  editor = {Vossen, Piek},
  file = {/home/yigit/Zotero/storage/4UQ2ITP5/Fellbaum - 1998 - A Semantic Network of English The Mother of All W.pdf},
  doi = {10.1007/978-94-017-1491-4_6}
}

@inproceedings{metzler_similarity_2007,
  langid = {english},
  title = {Similarity {{Measures}} for {{Short Segments}} of {{Text}}},
  isbn = {978-3-540-71496-5},
  abstract = {Measuring the similarity between documents and queries has been extensively studied in information retrieval. However, there are a growing number of tasks that require computing the similarity between two very short segments of text. These tasks include query reformulation, sponsored search, and image retrieval. Standard text similarity measures perform poorly on such tasks because of data sparseness and the lack of context. In this work, we study this problem from an information retrieval perspective, focusing on text representations and similarity measures. We examine a range of similarity measures, including purely lexical measures, stemming, and language modeling-based measures. We formally evaluate and analyze the methods on a query-query similarity task using 363,822 queries from a web search log. Our analysis provides insights into the strengths and weaknesses of each method, including important tradeoffs between effectiveness and efficiency.},
  booktitle = {Advances in {{Information Retrieval}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2007},
  pages = {16-27},
  keywords = {Excellent Match,Expanded Representation,Query Expansion,Short Segment,Similarity Measure},
  author = {Metzler, Donald and Dumais, Susan and Meek, Christopher},
  editor = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni}
}

@article{groves_friend_2015,
  title = {Friend or Foe? {{Google Translate}} in Language for Academic Purposes},
  volume = {37},
  issn = {0889-4906},
  url = {http://www.sciencedirect.com/science/article/pii/S088949061400060X},
  doi = {10.1016/j.esp.2014.09.001},
  shorttitle = {Friend or Foe?},
  abstract = {A recent development in digital technology, machine translation (MT), is improving in its ability to translate with grammatical and lexical accuracy, and is also becoming increasingly available for students of language for academic purposes. Given the acceptance of other digital technology for teaching and learning, it seems likely that machine translation will become a tool students will rely on to complete their assignments in a second language. This would have implications for the community of practice of academic language teaching. In this study students were asked to submit an essay in their first language and this was then translated into English through a web-based translation engine. The resulting English text was analysed for grammatical error. The analysis found that the translation engine was far from able to produce error-free text – however, judging in relation to international testing standards, the level of accuracy is approaching the minimum needed for university admission at many institutions. Thus, this paper sets out to argue, based on the assumption that MT will continue to improve, that this technology will have a profound influence on the teaching of Languages for Academic Purposes, and with imaginative use, will allow this influence to be positive for both the students and their instructors.},
  journaltitle = {English for Specific Purposes},
  shortjournal = {English for Specific Purposes},
  urldate = {2019-06-03},
  date = {2015-01-01},
  pages = {112-121},
  keywords = {Academic literacy,google translate,IT in SLA,Language for academic purposes,Machine translation},
  author = {Groves, Michael and Mundt, Klaus},
  file = {/home/yigit/Zotero/storage/G4ZK6ZL8/Groves and Mundt - 2015 - Friend or foe Google Translate in language for ac.pdf;/home/yigit/Zotero/storage/QVKWSS4F/S088949061400060X.html}
}

@article{luhn_statistical_1957,
  title = {A Statistical Approach to Mechanized Encoding and Searching of Literary Information},
  volume = {1},
  number = {4},
  journaltitle = {IBM Journal of research and development},
  date = {1957},
  pages = {309--317},
  author = {Luhn, Hans Peter},
  file = {/home/yigit/Zotero/storage/TH8IEUQD/Luhn - 1957 - A statistical approach to mechanized encoding and .pdf;/home/yigit/Zotero/storage/TM8CVX8R/5392697.html}
}

@article{bush_as_1945,
  title = {As We May Think},
  volume = {176},
  number = {1},
  journaltitle = {The atlantic monthly},
  date = {1945},
  pages = {101--108},
  author = {Bush, Vannevar},
  file = {/home/yigit/Zotero/storage/YGX7UZTB/Bush - 1945 - As we may think.pdf}
}

@article{singhal_modern_2001,
  title = {Modern Information Retrieval: A Brief Overview},
  volume = {24},
  shorttitle = {Modern Information Retrieval},
  abstract = {For thousands of years people have realized the importance of archiving and finding information. With the advent of computers, it became possible to store large amounts of information; and finding useful information from such collections became a necessity. The field of Information Retrieval (IR) was born in the 1950s out of this necessity. Over the last forty years, the field has matured considerably. Several IR systems are used on an everyday basis by a wide variety of users. This article is a brief overview of the key advances in the field of Information Retrieval, and a description of where the state-of-the-art is at in the field.},
  journaltitle = {Bulletin of the Ieee Computer Society Technical Committee on Data Engineering},
  date = {2001},
  pages = {2001},
  author = {Singhal, Amit},
  file = {/home/yigit/Zotero/storage/Y4P7GIQE/Singhal - 2001 - Modern information retrieval a brief overview.pdf;/home/yigit/Zotero/storage/6TS6UQBR/summary.html}
}

@inproceedings{daume_domain_2011,
  location = {{Stroudsburg, PA, USA}},
  title = {Domain {{Adaptation}} for {{Machine Translation}} by {{Mining Unseen Words}}},
  isbn = {978-1-932432-88-6},
  url = {http://dl.acm.org/citation.cfm?id=2002736.2002819},
  abstract = {We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrase-based translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Short Papers}} - {{Volume}} 2},
  series = {{{HLT}} '11},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-03},
  date = {2011},
  pages = {407--412},
  keywords = {matching},
  author = {Daumé, III, Hal and Jagarlamudi, Jagadeesh},
  file = {/home/yigit/Zotero/storage/UKZ2EZHT/Daumé and Jagarlamudi - 2011 - Domain Adaptation for Machine Translation by Minin.pdf},
  venue = {Portland, Oregon}
}

@inproceedings{jagarlamudi_bilingual_2011,
  location = {{Stroudsburg, PA, USA}},
  title = {From {{Bilingual Dictionaries}} to {{Interlingual Document Representations}}},
  isbn = {978-1-932432-88-6},
  url = {http://dl.acm.org/citation.cfm?id=2002736.2002768},
  abstract = {Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Short Papers}} - {{Volume}} 2},
  series = {{{HLT}} '11},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-03},
  date = {2011},
  pages = {147--152},
  keywords = {matching},
  author = {Jagarlamudi, Jagadeesh and Daumé, III, Hal and Udupa, Raghavendra},
  file = {/home/yigit/Zotero/storage/K648X7IF/Jagarlamudi et al. - 2011 - From Bilingual Dictionaries to Interlingual Docume.pdf},
  venue = {Portland, Oregon}
}

@article{ruder_discriminative_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.09334},
  primaryClass = {cs, stat},
  title = {A {{Discriminative Latent}}-{{Variable Model}} for {{Bilingual Lexicon Induction}}},
  url = {http://arxiv.org/abs/1808.09334},
  abstract = {We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.},
  urldate = {2019-06-03},
  date = {2018-08-28},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Ruder, Sebastian and Cotterell, Ryan and Kementchedjhieva, Yova and Søgaard, Anders},
  file = {/home/yigit/Zotero/storage/WB7FGA2N/Ruder et al. - 2018 - A Discriminative Latent-Variable Model for Bilingu.pdf;/home/yigit/Zotero/storage/S7U5KFHF/1808.html}
}

@inproceedings{saric_takelab_2012,
  location = {{Stroudsburg, PA, USA}},
  title = {{{TakeLab}}: {{Systems}} for {{Measuring Semantic Text Similarity}}},
  url = {http://dl.acm.org/citation.cfm?id=2387636.2387708},
  shorttitle = {{{TakeLab}}},
  abstract = {This paper describes the two systems for determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6. Most of the research on semantic similarity of textual content focuses on large documents. However, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson -- 2nd and 3rd, normalized Pearson -- 1st and 3rd, weighted mean -- 2nd and 5th).},
  booktitle = {Proceedings of the {{First Joint Conference}} on {{Lexical}} and {{Computational Semantics}} - {{Volume}} 1: {{Proceedings}} of the {{Main Conference}} and the {{Shared Task}}, and {{Volume}} 2: {{Proceedings}} of the {{Sixth International Workshop}} on {{Semantic Evaluation}}},
  series = {{{SemEval}} '12},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-02},
  date = {2012},
  pages = {441--448},
  author = {Šarić, Frane and Glavaš, Goran and Karan, Mladen and Šnajder, Jan and Bašić, Bojana Dalbelo},
  file = {/home/yigit/Zotero/storage/I7CWA5G7/Šarić et al. - 2012 - TakeLab Systems for Measuring Semantic Text Simil.pdf},
  venue = {Montréal, Canada}
}

@inproceedings{zhao_ecnu_2015,
  title = {{{ECNU}}: {{Using Traditional Similarity Measurements}} and {{Word Embedding}} for {{Semantic Textual Similarity Estimation}}},
  doi = {10.18653/v1/S15-2021},
  shorttitle = {{{ECNU}}},
  abstract = {This paper reports our submissions to semantic textual similarity task, i.e., task 2 in Semantic Evaluation 2015. We built our systems using various traditional features, such as string-based, corpus-based and syntactic similarity metrics, as well as novel similarity measures based on distributed word representations, which were trained using deep learning paradigms. Since the training and test datasets consist of instances collected from various domains, three different strategies of the usage of training datasets were explored: (1) use all available training datasets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73.},
  booktitle = {{{SemEval}}@{{NAACL}}-{{HLT}}},
  date = {2015},
  keywords = {Computer multitasking,Deep learning,Multi-task learning,Semantic similarity,Supervised learning,Test set,Text corpus,Trustworthy computing,Word embedding},
  author = {Zhao, Jiang and Lan, Man and Tian, Junfeng},
  file = {/home/yigit/Zotero/storage/K6S26JLL/Zhao et al. - 2015 - ECNU Using Traditional Similarity Measurements an.pdf}
}

@inproceedings{morin_hierarchical_2005,
  title = {Hierarchical Probabilistic Neural Network Language Model.},
  volume = {5},
  booktitle = {Aistats},
  publisher = {{Citeseer}},
  date = {2005},
  pages = {246--252},
  author = {Morin, Frederic and Bengio, Yoshua}
}

@article{landauer_solution_1997,
  title = {A Solution to {{Plato}}'s Problem: {{The}} Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.},
  volume = {104},
  shorttitle = {A Solution to {{Plato}}'s Problem},
  number = {2},
  journaltitle = {Psychological review},
  date = {1997},
  pages = {211},
  author = {Landauer, Thomas K. and Dumais, Susan T.},
  file = {/home/yigit/Zotero/storage/9ZQVNR5Z/Landauer and Dumais - 1997 - A solution to Plato's problem The latent semantic.pdf;/home/yigit/Zotero/storage/Y84CS5B6/doiLanding.html}
}

@incollection{mnih_learning_2013,
  title = {Learning Word Embeddings Efficiently with Noise-Contrastive Estimation},
  url = {http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  date = {2013},
  pages = {2265-2273},
  author = {Mnih, Andriy and Kavukcuoglu, Koray},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.}
}

@article{bullinaria_extracting_2007,
  title = {Extracting Semantic Representations from Word Co-Occurrence Statistics: {{A}} Computational Study},
  shorttitle = {Extracting Semantic Representations from Word Co-Occurrence Statistics},
  abstract = {Abstract: In a previous paper we presented a systematic computational study of the extraction of semantic representations from the word-word co-occurrence statistics of large text corpora. The conclusion was that semantic vectors of Pointwise Mutual Information (PMI) values from very small co-occurrence windows, together with a cosine distance measure, consistently resulted in the best representations across a range of psychologically relevant semantic tasks. This paper extends that study by investigating the use of three further factors, namely the application of stop-lists, word stemming, and dimensionality reduction using Singular Value Decomposition (SVD), that have been used to provide improved performance elsewhere. It also introduces an additional semantic task and explores the advantages of using a much larger corpus. This leads to the discovery and analysis of improved SVD based methods for generating semantic representations (that provide new state-of-the-art performance on a standard TOEFL task) and the identification and discussion of problems and misleading results that can arise without a full systematic study.},
  journaltitle = {Behavior Research Methods},
  date = {2007},
  pages = {510--526},
  author = {Bullinaria, John A. and Levy, Joseph P.},
  file = {/home/yigit/Zotero/storage/Z4M6Q46E/Bullinaria and Levy - 2007 - Extracting semantic representations from word co-o.pdf;/home/yigit/Zotero/storage/AZ7FR8YW/summary.html}
}

@inproceedings{baroni_dont_2014,
  title = {Don't Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  url = {https://www.aclweb.org/anthology/P14-1023},
  doi = {10.3115/v1/P14-1023},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  date = {2014-06},
  pages = {238-247},
  author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
  file = {/home/yigit/Zotero/storage/TGTCTUDI/Baroni et al. - 2014 - Don’t count, predict! A systematic comparison of c.pdf},
  venue = {Baltimore, Maryland}
}

@inproceedings{levy_neural_2014,
  location = {{Cambridge, MA, USA}},
  title = {Neural {{Word Embedding As Implicit Matrix Factorization}}},
  url = {http://dl.acm.org/citation.cfm?id=2969033.2969070},
  abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  series = {{{NIPS}}'14},
  publisher = {{MIT Press}},
  urldate = {2019-06-02},
  date = {2014},
  pages = {2177--2185},
  author = {Levy, Omer and Goldberg, Yoav},
  venue = {Montreal, Canada}
}

@inproceedings{pennington_glove_2014,
  title = {Glove: {{Global}} Vectors for Word Representation},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  date = {2014},
  pages = {1532--1543},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  file = {/home/yigit/Zotero/storage/3EVJEGNW/Pennington et al. - 2014 - Glove Global vectors for word representation.pdf}
}

@inproceedings{mikolov_linguistic_2013,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  date = {2013},
  pages = {746--751},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  file = {/home/yigit/Zotero/storage/NNTN7MGF/Mikolov et al. - 2013 - Linguistic regularities in continuous space word r.pdf}
}

@article{goldberg_word2vec_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1402.3722},
  primaryClass = {cs, stat},
  title = {Word2vec {{Explained}}: Deriving {{Mikolov}} et Al.'s Negative-Sampling Word-Embedding Method},
  url = {http://arxiv.org/abs/1402.3722},
  shorttitle = {Word2vec {{Explained}}},
  abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
  urldate = {2019-06-02},
  date = {2014-02-15},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Goldberg, Yoav and Levy, Omer},
  file = {/home/yigit/Zotero/storage/VQLG46UP/Goldberg and Levy - 2014 - word2vec Explained deriving Mikolov et al.'s nega.pdf;/home/yigit/Zotero/storage/E58W5AGI/1402.html}
}

@inproceedings{xu_can_2000,
  title = {Can Artificial Neural Networks Learn Language Models?},
  booktitle = {Sixth {{International Conference}} on {{Spoken Language Processing}}},
  date = {2000},
  author = {Xu, Wei and Rudnicky, Alex},
  file = {/home/yigit/Zotero/storage/CBGN6FED/Xu and Rudnicky - 2000 - Can artificial neural networks learn language mode.pdf}
}

@inproceedings{schutze_dimensions_1992,
  location = {{Los Alamitos, CA, USA}},
  title = {Dimensions of {{Meaning}}},
  isbn = {978-0-8186-2630-2},
  url = {http://dl.acm.org/citation.cfm?id=147877.148132},
  booktitle = {Proceedings of the 1992 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  series = {Supercomputing '92},
  publisher = {{IEEE Computer Society Press}},
  urldate = {2019-05-31},
  date = {1992},
  pages = {787--796},
  author = {Schütze, H.},
  file = {/home/yigit/Zotero/storage/KPTP84TL/Schütze - 1992 - Dimensions of Meaning.pdf},
  venue = {Minneapolis, Minnesota, USA}
}

@book{forsythe_computer_1977,
  title = {Computer Methods for Mathematical Computations},
  volume = {259},
  publisher = {{Prentice-hall Englewood Cliffs, NJ}},
  date = {1977},
  author = {Forsythe, George Elmer and Malcolm, Michael A. and Moler, Cleve B.}
}

@article{church_word_1990,
  title = {Word {{Association Norms}}, {{Mutual Information}}, and {{Lexicography}}},
  volume = {16},
  issn = {0891-2017},
  url = {http://dl.acm.org/citation.cfm?id=89086.89095},
  abstract = {The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.},
  number = {1},
  journaltitle = {Comput. Linguist.},
  urldate = {2019-05-31},
  date = {1990-03},
  pages = {22--29},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  file = {/home/yigit/Zotero/storage/GRGNACB7/Church and Hanks - 1990 - Word Association Norms, Mutual Information, and Le.pdf}
}

@article{firth_synopsis_1957,
  title = {A Synopsis of Linguistic Theory 1930--1955},
  volume = {Special volume of the Phiological Society},
  journaltitle = {Studies in linguistic analysis},
  date = {1957},
  pages = {11},
  author = {Firth, John Ruper}
}

@article{almeida_word_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09069},
  primaryClass = {cs, stat},
  title = {Word {{Embeddings}}: {{A Survey}}},
  url = {http://arxiv.org/abs/1901.09069},
  shorttitle = {Word {{Embeddings}}},
  abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
  urldate = {2019-05-30},
  date = {2019-01-25},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning,I.2.7,A.1},
  author = {Almeida, Felipe and Xexéo, Geraldo},
  file = {/home/yigit/Zotero/storage/55LKUTA5/Almeida and Xexéo - 2019 - Word Embeddings A Survey.pdf;/home/yigit/Zotero/storage/VJA3CTN4/1901.html}
}

@article{dubin_most_2004,
  title = {The {{Most Influential Paper Gerard Salton Never Wrote}}},
  volume = {52},
  issn = {00242594},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=f6h&AN=14619164&lang=tr&site=eds-live&authtype=uid},
  abstract = {Gerard Salton is often credited with developing the vector space model (VSM)for information retrieval (IR). Citations to Salton give the impression that the VSM must have been articulated as an IR model sometime between 1970 and 1975. However, the VSM as it is understood today evolved over a longer time period than is usually acknowledged, and an articulation of the model and its assumptions did not appear in print until several years after those assumptions had been criticized and alternative models proposed. An often cited overview paper titled "A Vector Space Model for Information Retrieval" (alleged to have been published in 1975) does not exist, and citations to it represent a confusion of two 1975 articles, neither of which were overviews of the VSM as a model of information retrieval. Until the late 1970s, Salton did not present vector spaces as models of IR generally but rather as models of specific computations. Citations to the phantom paper reflect an apparently widely held misconception that the operational features and explanatory devices now associated with the VSM must have been introduced at the same time it was first proposed as an IR model.},
  number = {4},
  journaltitle = {Library Trends},
  shortjournal = {Library Trends},
  urldate = {2019-05-30},
  date = {2004-21},
  pages = {748},
  keywords = {GerardVECTOR spacesVECTOR analysisINFORMATION retrievalDOCUMENTATIONINFORMATION science,SALTON},
  author = {Dubin, David},
  file = {/home/yigit/Zotero/storage/QNLKGIEZ/Dubin - 2004 - The Most Influential Paper Gerard Salton Never Wro.pdf}
}

@article{jones_statistical_1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  volume = {28},
  abstract = {Abstract: The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing, in particular, that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure. Exhaustivity and specificity We are familiar with the notions of exhaustivity and specificity: exhaustivity is a property of index descriptions, and specificity one of index terms. They are most clearly illustrated by a simple keyword or descriptor system. In this case the exhaustivity of a document description is the coverage of its various topics given by the terms assigned to it; and the specificity of an individual term is the level of detail at which a given concept is represented.},
  journaltitle = {Journal of Documentation},
  date = {1972},
  pages = {11--21},
  author = {Jones, Karen Spärck},
  file = {/home/yigit/Zotero/storage/SDFA6FUD/Jones - 1972 - A statistical interpretation of term specificity a.pdf;/home/yigit/Zotero/storage/YPQJFBTN/summary.html}
}

@article{arcan_polylingual_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.01411},
  primaryClass = {cs},
  title = {Polylingual {{Wordnet}}},
  url = {http://arxiv.org/abs/1903.01411},
  abstract = {Princeton WordNet is one of the most important resources for natural language processing, but is only available for English. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches. For this reason we leverage existing translations of WordNet in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. We evaluate our approach using 10 translated wordnets for European languages. Our experiment shows a significant improvement over translation without any contextual information. Furthermore, we evaluate how the choice of pivot languages affects performance of multilingual word sense disambiguation.},
  urldate = {2019-05-30},
  date = {2019-03-04},
  keywords = {Computer Science - Computation and Language},
  author = {Arcan, Mihael and McCrae, John and Buitelaar, Paul},
  file = {/home/yigit/Zotero/storage/6VBNCLGA/Arcan et al. - 2019 - Polylingual Wordnet.pdf;/home/yigit/Zotero/storage/P3PTLWLQ/1903.html}
}

@inproceedings{arcan_expanding_2016,
  langid = {american},
  title = {Expanding Wordnets to New Languages with Multilingual Sense Disambiguation},
  url = {https://www.aclweb.org/anthology/papers/C/C16/C16-1010/},
  eventtitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  urldate = {2019-05-30},
  date = {2016-12},
  pages = {97-108},
  author = {Arcan, Mihael and McCrae, John Philip and Buitelaar, Paul},
  file = {/home/yigit/Zotero/storage/RG984P8I/Arcan et al. - 2016 - Expanding wordnets to new languages with multiling.pdf;/home/yigit/Zotero/storage/SVJ87C9L/C16-1010.html}
}

@inproceedings{mikolov2018advances,
  title = {Advances in {{Pre}}-{{Training Distributed Word Representations}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  date = {2018},
  author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  file = {/home/yigit/Zotero/storage/QUAT3A34/Mikolov et al. - 2018 - Advances in Pre-Training Distributed Word Represen.pdf}
}

@article{bengio_neural_2003,
  title = {A Neural Probabilistic Language Model},
  volume = {3},
  issue = {Feb},
  journaltitle = {Journal of machine learning research},
  date = {2003},
  pages = {1137--1155},
  author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
  file = {/home/yigit/Zotero/storage/B3TMW76E/bengio03a.html;/home/yigit/Zotero/storage/R36IZ3R8/plink.html}
}

@article{deerwester_indexing_1990,
  title = {Indexing by Latent Semantic Analysis},
  volume = {41},
  number = {6},
  journaltitle = {Journal of the American society for information science},
  date = {1990},
  pages = {391--407},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  file = {/home/yigit/Zotero/storage/AX7VGKEQ/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf}
}

@article{harris_distributional_1954,
  title = {Distributional {{Structure}}},
  volume = {10},
  issn = {0043-7956},
  url = {https://doi.org/10.1080/00437956.1954.11659520},
  doi = {10.1080/00437956.1954.11659520},
  number = {2-3},
  journaltitle = {WORD},
  urldate = {2019-05-29},
  date = {1954-08-01},
  pages = {146-162},
  author = {Harris, Zellig S.},
  file = {/home/yigit/Zotero/storage/CJCLI3BF/Harris - 1954 - Distributional Structure.pdf;/home/yigit/Zotero/storage/WKMHBIGB/00437956.1954.html}
}

@inproceedings{collobert_unified_2008,
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  isbn = {978-1-60558-205-4},
  url = {http://dl.acm.org/citation.cfm?id=1390156.1390177},
  doi = {10.1145/1390156.1390177},
  shorttitle = {A Unified Architecture for Natural Language Processing},
  eventtitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  publisher = {{ACM}},
  urldate = {2019-05-29},
  date = {2008-05-07},
  pages = {160-167},
  author = {Collobert, Ronan and Weston, Jason},
  file = {/home/yigit/Zotero/storage/HKV2BMY6/Collobert and Weston - 2008 - A unified architecture for natural language proces.pdf;/home/yigit/Zotero/storage/EYLKGHFT/citation.html}
}

@article{lund_producing_1996,
  langid = {english},
  title = {Producing High-Dimensional Semantic Spaces from Lexical Co-Occurrence},
  volume = {28},
  issn = {1532-5970},
  url = {https://doi.org/10.3758/BF03204766},
  doi = {10.3758/BF03204766},
  abstract = {A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).},
  number = {2},
  journaltitle = {Behavior Research Methods, Instruments, \& Computers},
  shortjournal = {Behavior Research Methods, Instruments, \& Computers},
  urldate = {2019-05-29},
  date = {1996-06-01},
  pages = {203-208},
  keywords = {Target Word,Semantic Distance,Semantic Space,Vector Similarity,Word Pair},
  author = {Lund, Kevin and Burgess, Curt},
  file = {/home/yigit/Zotero/storage/AL32VTCC/Lund and Burgess - 1996 - Producing high-dimensional semantic spaces from le.pdf}
}

@inproceedings{bond_linking_2013,
  title = {Linking and {{Extending}} an {{Open Multilingual Wordnet}}},
  volume = {1},
  abstract = {We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open licences, data from Wiktionary and the Unicode Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of languages.},
  eventtitle = {{{ACL}} 2013 - 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}, {{Proceedings}} of the {{Conference}}},
  date = {2013-08-01},
  pages = {1352-1362},
  author = {Bond, Francis and Foster, Ryan}
}

@article{salton_term-weighting_1988,
  title = {Term-Weighting Approaches in Automatic Text Retrieval},
  volume = {24},
  issn = {0306-4573},
  url = {http://www.sciencedirect.com/science/article/pii/0306457388900210},
  doi = {10.1016/0306-4573(88)90021-0},
  abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.},
  number = {5},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  urldate = {2019-05-28},
  date = {1988-01-01},
  pages = {513-523},
  author = {Salton, Gerard and Buckley, Christopher},
  file = {/home/yigit/Zotero/storage/7H7J8CWG/Salton and Buckley - 1988 - Term-weighting approaches in automatic text retrie.pdf;/home/yigit/Zotero/storage/TALCQS38/0306457388900210.html}
}

@inproceedings{p._turian_word_2010,
  title = {Word {{Representations}}: {{A Simple}} and {{General Method}} for {{Semi}}-{{Supervised Learning}}.},
  volume = {2010},
  shorttitle = {Word {{Representations}}},
  abstract = {If we take an existing supervised NLP sys- tem, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih \& Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accu- racy of these baselines. We find further improvements by combining di erent word representations. You can download our word features, for o -the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/},
  eventtitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  date = {2010-01-01},
  pages = {384-394},
  author = {P. Turian, Joseph and Ratinov, Lev-Arie and Bengio, Y},
  file = {/home/yigit/Zotero/storage/9EPRW4KG/P. Turian et al. - 2010 - Word Representations A Simple and General Method .pdf}
}

@book{salton_introduction_1986,
  location = {{New York, NY, USA}},
  title = {Introduction to {{Modern Information Retrieval}}},
  isbn = {978-0-07-054484-0},
  publisher = {{McGraw-Hill, Inc.}},
  date = {1986},
  author = {Salton, Gerard and McGill, Michael J.},
  file = {/home/yigit/Zotero/storage/PN8NTRXH/(Mcgraw Hill Computer Science Series) Gerard Salton - Introduction to Modern Information Retrieval-Mcgraw-Hill College (1983).djvu}
}

@article{camacho-collados_word_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.04032},
  primaryClass = {cs},
  title = {From {{Word}} to {{Sense Embeddings}}: {{A Survey}} on {{Vector Representations}} of {{Meaning}}},
  url = {http://arxiv.org/abs/1805.04032},
  shorttitle = {From {{Word}} to {{Sense Embeddings}}},
  abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
  urldate = {2019-05-28},
  date = {2018-05-10},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  author = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
  file = {/home/yigit/Zotero/storage/KD2S5MMV/Camacho-Collados and Pilehvar - 2018 - From Word to Sense Embeddings A Survey on Vector .pdf;/home/yigit/Zotero/storage/Q843ME6G/1805.html}
}

@article{salton_vector_1975,
  title = {A {{Vector Space Model}} for {{Automatic Indexing}}},
  volume = {18},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/361219.361220},
  doi = {10.1145/361219.361220},
  abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
  number = {11},
  journaltitle = {Commun. ACM},
  urldate = {2019-05-28},
  date = {1975-11},
  pages = {613--620},
  keywords = {automatic indexing,automatic information retrieval,content analysis,document space},
  author = {Salton, G. and Wong, A. and Yang, C. S.},
  file = {/home/yigit/Zotero/storage/95WVZKYH/Salton et al. - 1975 - A Vector Space Model for Automatic Indexing.pdf}
}

@article{artetxe_unsupervised_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.01272},
  primaryClass = {cs},
  title = {Unsupervised {{Statistical Machine Translation}}},
  url = {http://arxiv.org/abs/1809.01272},
  abstract = {While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https://github.com/artetxem/monoses},
  urldate = {2019-05-27},
  date = {2018-09-04},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  file = {/home/yigit/Zotero/storage/XWPQBIAM/Artetxe et al. - 2018 - Unsupervised Statistical Machine Translation.pdf;/home/yigit/Zotero/storage/JBAKJJ3G/1809.html}
}

@thesis{ercan_automated_2006,
  title = {{{AUTOMATED TEXT SUMMARIZATION AND KEYPHRASE EXTRACTION}}},
  date = {2006},
  author = {Ercan, Gonenc}
}

@inproceedings{sasaki_subword-based_2019,
  title = {Subword-Based {{Compact Reconstruction}} of {{Word Embeddings}}},
  url = {https://www.aclweb.org/anthology/N19-1353},
  abstract = {The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings. In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space. The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism. Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks.},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  publisher = {{Association for Computational Linguistics}},
  date = {2019-06},
  pages = {3498-3508},
  author = {Sasaki, Shota and Suzuki, Jun and Inui, Kentaro},
  venue = {Minneapolis, Minnesota}
}

@article{barnard_matching_2003,
  title = {Matching {{Words}} and {{Pictures}}},
  volume = {3},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=944919.944965},
  abstract = {We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.},
  journaltitle = {J. Mach. Learn. Res.},
  urldate = {2019-06-06},
  date = {2003-03},
  pages = {1107--1135},
  keywords = {linear assignment},
  author = {Barnard, Kobus and Duygulu, Pinar and Forsyth, David and de Freitas, Nando and Blei, David M. and Jordan, Michael I.},
  options = {useprefix=true},
  file = {/home/yigit/Zotero/storage/NUKSVYCY/Barnard et al. - 2003 - Matching Words and Pictures.pdf}
}

@inproceedings{klang_pairing_2016,
  title = {Pairing {{Wikipedia Articles Across Languages}}},
  url = {https://www.aclweb.org/anthology/W16-4410},
  abstract = {Wikipedia has become a reference knowledge source for scores of NLP applications. One of its invaluable features lies in its multilingual nature, where articles on a same entity or concept can have from one to more than 200 different versions. The interlinking of language versions in Wikipedia has undergone a major renewal with the advent of Wikidata, a unified scheme to identify entities and their properties using unique numbers. However, as the interlinking is still manually carried out by thousands of editors across the globe, errors may creep in the assignment of entities. In this paper, we describe an optimization technique to match automatically language versions of articles, and hence entities, that is only based on bags of words and anchors. We created a dataset of all the articles on persons we extracted from Wikipedia in six languages: English, French, German, Russian, Spanish, and Swedish. We report a correct match of at least 94.3\% on each pair.},
  booktitle = {Proceedings of the {{Open Knowledge Base}} and {{Question Answering Workshop}} ({{OKBQA}} 2016)},
  publisher = {{The COLING 2016 Organizing Committee}},
  date = {2016-12},
  pages = {72-76},
  keywords = {linear assignment},
  author = {Klang, Marcus and Nugues, Pierre},
  venue = {Osaka, Japan}
}

@article{grave_learning_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06893},
  primaryClass = {cs},
  title = {Learning {{Word Vectors}} for 157 {{Languages}}},
  url = {http://arxiv.org/abs/1802.06893},
  abstract = {Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.},
  urldate = {2019-06-06},
  date = {2018-02-19},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  file = {/home/yigit/Zotero/storage/AWI86TU2/Grave et al. - 2018 - Learning Word Vectors for 157 Languages.pdf;/home/yigit/Zotero/storage/LRZY2TP7/1802.html}
}

@inproceedings{neale_survey_2018,
  langid = {english},
  title = {A {{Survey}} on {{Automatically}}-{{Constructed WordNets}} and Their {{Evaluation}}: {{Lexical}} and {{Word Embedding}}-Based {{Approaches}}},
  isbn = {979-10-95546-00-9},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  publisher = {{European Language Resources Association (ELRA)}},
  date = {2018-05-07/2018-05-12},
  author = {Neale, Steven},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  file = {/home/yigit/Zotero/storage/WJHLSRYB/Neale - 2018 - A Survey on Automatically-Constructed WordNets and.pdf},
  venue = {Miyazaki, Japan}
}

@book{osgood_measurement_1957,
  langid = {english},
  title = {The {{Measurement}} of {{Meaning}}},
  isbn = {978-0-252-74539-3},
  abstract = {In this pioneering study, the authors deal with the nature and theory of meaning and present a new, objective method for its measurement which they call the semantic differential. This instrument is not a specific test, but rather a general technique of measurement that can be adapted to a wide variety of problems in such areas as clinical psychology, social psychology, linguistics, mass communications,  esthetics, and political science. The core of the book is the authors' description,  application, and evaluation of this important tool and its far-reaching implications for empirical research.},
  pagetotal = {358},
  publisher = {{University of Illinois Press}},
  date = {1957},
  keywords = {Psychology / General},
  author = {Osgood, Charles Egerton and Suci, George J. and Tannenbaum, Percy H.},
  eprinttype = {googlebooks}
}

@inproceedings{speer_conceptnet_2017,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972},
  eventtitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  date = {2017},
  pages = {4444-4451},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine}
}

@inproceedings{ruiz-casado_automatic_2005,
  title = {Automatic Assignment of Wikipedia Encyclopedic Entries to Wordnet Synsets},
  booktitle = {International {{Atlantic Web Intelligence Conference}}},
  publisher = {{Springer}},
  date = {2005},
  pages = {380--386},
  keywords = {wordnet},
  author = {Ruiz-Casado, Maria and Alfonseca, Enrique and Castells, Pablo},
  file = {/home/yigit/Zotero/storage/ZLJ6I2TE/Ruiz-Casado et al. - 2005 - Automatic assignment of wikipedia encyclopedic ent.pdf;/home/yigit/Zotero/storage/7UC5P56M/11495772_59.html}
}

@inproceedings{navigli_babelnet_2010,
  location = {{Stroudsburg, PA, USA}},
  title = {{{BabelNet}}: {{Building}} a {{Very Large Multilingual Semantic Network}}},
  url = {http://dl.acm.org/citation.cfm?id=1858681.1858704},
  shorttitle = {{{BabelNet}}},
  abstract = {In this paper we present BabelNet -- a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource.},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  series = {{{ACL}} '10},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-05},
  date = {2010},
  pages = {216--225},
  author = {Navigli, Roberto and Ponzetto, Simone Paolo},
  file = {/home/yigit/Zotero/storage/YEIZWDU9/Navigli and Ponzetto - 2010 - BabelNet Building a Very Large Multilingual Seman.pdf},
  venue = {Uppsala, Sweden}
}

@article{robertson_understanding_2004,
  title = {Understanding Inverse Document Frequency: On Theoretical Arguments for {{IDF}}},
  volume = {60},
  issn = {0022-0418},
  url = {https://www.emeraldinsight.com/doi/full/10.1108/00220410410560582},
  doi = {10.1108/00220410410560582},
  shorttitle = {Understanding Inverse Document Frequency},
  number = {5},
  journaltitle = {Journal of Documentation},
  shortjournal = {Journal of Documentation},
  urldate = {2019-06-05},
  date = {2004-10-01},
  pages = {503-520},
  keywords = {Information theory,Modelling,Probabilistic analysis,Text retrieval,tfidf},
  author = {Robertson, Stephen},
  file = {/home/yigit/Zotero/storage/8MNSFFR9/Robertson - 2004 - Understanding inverse document frequency on theor.pdf;/home/yigit/Zotero/storage/N42TCN3A/00220410410560582.html}
}

@article{kiros_skip-thought_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.06726},
  primaryClass = {cs},
  title = {Skip-{{Thought Vectors}}},
  url = {http://arxiv.org/abs/1506.06726},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  urldate = {2019-06-05},
  date = {2015-06-22},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,sentence embedding},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  file = {/home/yigit/Zotero/storage/KSQ254IF/Kiros et al. - 2015 - Skip-Thought Vectors.pdf;/home/yigit/Zotero/storage/3AMAJL5Q/1506.html}
}

@article{wieting_towards_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.08198},
  primaryClass = {cs},
  title = {Towards {{Universal Paraphrastic Sentence Embeddings}}},
  url = {http://arxiv.org/abs/1511.08198},
  abstract = {We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community with the hope that they can serve as the new baseline for further work on universal sentence embeddings.},
  urldate = {2019-06-04},
  date = {2015-11-25},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,sentence embedding},
  author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  file = {/home/yigit/Zotero/storage/ZQPG5DQG/Wieting et al. - 2015 - Towards Universal Paraphrastic Sentence Embeddings.pdf;/home/yigit/Zotero/storage/3KBUFXKV/1511.html}
}

@incollection{Craswell2009,
  location = {{Boston, MA}},
  title = {Mean {{Reciprocal Rank}}},
  isbn = {978-0-387-39940-9},
  url = {https://doi.org/10.1007/978-0-387-39940-9_488},
  booktitle = {Encyclopedia of {{Database Systems}}},
  publisher = {{Springer US}},
  date = {2009},
  pages = {1703-1703},
  author = {Craswell, Nick},
  editor = {LIU, LING and ÖZSU, M. TAMER},
  doi = {10.1007/978-0-387-39940-9_488}
}

@article{salton_state_1992,
  title = {The State of Retrieval System Evaluation},
  volume = {28},
  number = {4},
  journaltitle = {Information processing \& management},
  date = {1992},
  pages = {441--449},
  author = {Salton, Gerard},
  file = {/home/yigit/Zotero/storage/PBGZQGC5/Salton - 1992 - The state of retrieval system evaluation.ps;/home/yigit/Zotero/storage/Y2SWIUTP/030645739290002H.html}
}

@inproceedings{voorhees_trec-8_1999,
  title = {The {{TREC}}-8 {{Question Answering Track Report}}},
  abstract = {The TREC-8 Question Answering track was the first large-scale evaluation of domain-independent  question answering systems. This paper summarizes the results of the track by giving a brief overview of  the different approaches taken to solve the problem. The most accurate systems found a correct response  for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for finding  answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing  was necessary for more direct responses (50 bytes).  The TREC-8 Question Answering track was an initial effort to bring the benefits of large-scale evaluation to bear on a question answering (QA) task. The goal in the QA task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than find the a...},
  booktitle = {In {{Proceedings}} of {{TREC}}-8},
  date = {1999},
  pages = {77--82},
  author = {Voorhees, Ellen M.},
  file = {/home/yigit/Zotero/storage/J9YHZSKR/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf;/home/yigit/Zotero/storage/YVH3JDDD/summary.html}
}

@article{arora_linear_2018,
  langid = {english},
  title = {Linear {{Algebraic Structure}} of {{Word Senses}}, with {{Applications}} to {{Polysemy}}},
  volume = {6},
  issn = {2307-387X},
  url = {https://transacl.org/ojs/index.php/tacl/article/view/1346},
  abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 "discourse atoms" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.},
  number = {0},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  urldate = {2019-06-07},
  date = {2018-07-20},
  pages = {483-495},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  file = {/home/yigit/Zotero/storage/T8QRFSS5/Arora et al. - 2018 - Linear Algebraic Structure of Word Senses, with Ap.pdf;/home/yigit/Zotero/storage/8IDJQSIM/1346.html}
}

@inproceedings{tarouti_enhancing_2016,
  langid = {american},
  title = {Enhancing {{Automatic Wordnet Construction Using Word Embeddings}}},
  url = {https://aclweb.org/anthology/papers/W/W16/W16-1204/},
  doi = {10.18653/v1/W16-1204},
  eventtitle = {Proceedings of the {{Workshop}} on {{Multilingual}} and {{Cross}}-Lingual {{Methods}} in {{NLP}}},
  urldate = {2019-06-07},
  date = {2016-06},
  pages = {30-34},
  keywords = {word embedding wordnet},
  author = {Tarouti, Feras Al and Kalita, Jugal},
  file = {/home/yigit/Zotero/storage/Z294YXE6/Tarouti and Kalita - 2016 - Enhancing Automatic Wordnet Construction Using Wor.pdf;/home/yigit/Zotero/storage/HCMEEVIJ/W16-1204.html}
}

@inproceedings{lam_automatically_2014,
  langid = {american},
  title = {Automatically Constructing {{Wordnet Synsets}}},
  url = {https://aclweb.org/anthology/papers/P/P14/P14-2018/},
  doi = {10.3115/v1/P14-2018},
  eventtitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  urldate = {2019-06-07},
  date = {2014-06},
  pages = {106-111},
  author = {Lam, Khang Nhut and Tarouti, Feras Al and Kalita, Jugal},
  file = {/home/yigit/Zotero/storage/GA26QK7Z/Lam et al. - 2014 - Automatically constructing Wordnet Synsets.pdf;/home/yigit/Zotero/storage/MC9PNIGC/P14-2018.html}
}

@inproceedings{sand_wordnet_2017,
  langid = {american},
  title = {Wordnet Extension via Word Embeddings: {{Experiments}} on the {{Norwegian Wordnet}}},
  url = {https://aclweb.org/anthology/papers/W/W17/W17-0242/},
  shorttitle = {Wordnet Extension via Word Embeddings},
  eventtitle = {Proceedings of the 21st {{Nordic Conference}} on {{Computational Linguistics}}},
  urldate = {2019-06-07},
  date = {2017-05},
  pages = {298-302},
  author = {Sand, Heidi and Velldal, Erik and Øvrelid, Lilja},
  file = {/home/yigit/Zotero/storage/VKCFGBPF/Sand et al. - 2017 - Wordnet extension via word embeddings Experiments.pdf;/home/yigit/Zotero/storage/DHW3JSAR/W17-0242.html}
}

@inproceedings{bhattacharyya_indowordnet_2010,
  title = {Indowordnet},
  abstract = {India is a multilingual country where machine translation and cross lingual search are highly relevant problems. These problems require large resources- like wordnets and lexicons- of high quality and coverage. Wordnets are lexical structures composed of synsets and semantic relations. Synsets are sets of synonyms. They are linked by semantic relations like hypernymy (is-a), meronymy (part-of), troponymy (manner-of) etc. IndoWordnet is a linked structure of wordnets of major Indian languages from Indo-Aryan, Dravidian and Sino-Tibetan families. These wordnets have been created by following the expansion approach from Hindi wordnet which was made available free for research in 2006. Since then a number of Indian languages have been creating their wordnets. In this paper we discuss the methodology, coverage, important considerations and multifarious benefits of IndoWordnet. Case studies are provided for Marathi, Sanskrit, Bodo and Telugu, to bring out the basic methodology of and challenges involved in the expansion approach. The guidelines the lexicographers follow for wordnet construction are enumerated. The difference between IndoWordnet and EuroWordnet also is discussed. 1.},
  booktitle = {In {{Proc}}. of {{LREC}}-10},
  date = {2010},
  author = {Bhattacharyya, Pushpak},
  file = {/home/yigit/Zotero/storage/3BQA3CWX/Bhattacharyya - 2010 - Indowordnet.pdf;/home/yigit/Zotero/storage/33JYNZND/summary.html}
}

@article{ercan_using_2007,
  title = {Using Lexical Chains for Keyword Extraction},
  volume = {43},
  number = {6},
  journaltitle = {Information Processing \& Management},
  date = {2007},
  pages = {1705-1714},
  author = {Ercan, Gonenc and Cicekli, Ilyas},
  publisher = {{Elsevier}}
}

@inproceedings{diab_feasibility_2004,
  title = {The Feasibility of Bootstrapping an Arabic Wordnet Leveraging Parallel Corpora and an English Wordnet},
  booktitle = {Proceedings of the {{Arabic Language Technologies}} and {{Resources}}, {{NEMLAR}}, {{Cairo}}},
  date = {2004},
  author = {Diab, Mona}
}

@article{kitamura_cultural_2009,
  title = {Cultural Untranslatability},
  volume = {13},
  number = {3},
  journaltitle = {Translation Journal},
  date = {2009},
  author = {Kitamura, Kanji}
}

@inproceedings{boyd-graber_adding_2006,
  title = {Adding Dense, Weighted Connections to {{WordNet}}},
  booktitle = {Proceedings of the Third International {{WordNet}} Conference},
  publisher = {{Citeseer}},
  date = {2006},
  pages = {29--36},
  author = {Boyd-Graber, Jordan and Fellbaum, Christiane and Osherson, Daniel and Schapire, Robert},
  file = {/home/yigit/Zotero/storage/487G7YYU/Boyd-Graber et al. - 2006 - Adding dense, weighted connections to WordNet.pdf}
}

@article{vossen_eurowordnet_2004,
  langid = {english},
  title = {{{EUROWORDNET}}: {{A MULTILINGUAL DATABASE OF AUTONOMOUS AND LANGUAGE}}-{{SPECIFIC WORDNETS CONNECTED VIA AN INTER}}-{{LINGUALINDEX}}},
  volume = {17},
  issn = {0950-3846},
  url = {https://academic.oup.com/ijl/article/17/2/161/969685},
  doi = {10.1093/ijl/17.2.161},
  shorttitle = {{{EUROWORDNET}}},
  abstract = {Abstract.  This paper describes the multilingual design of the EuroWordNet database. The EuroWordNet database stores wordnets as autonomous language-specific st},
  number = {2},
  journaltitle = {International Journal of Lexicography},
  shortjournal = {Int J Lexicography},
  urldate = {2019-06-08},
  date = {2004-06-01},
  pages = {161-173},
  author = {Vossen, Piek},
  file = {/home/yigit/Zotero/storage/BUE3MJ95/Vossen - 2004 - EUROWORDNET A MULTILINGUAL DATABASE OF AUTONOMOUS.pdf;/home/yigit/Zotero/storage/PYXSZ8PM/969685.html}
}


