
@article{mikolov_exploiting_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.4168},
  primaryClass = {cs},
  title = {Exploiting {{Similarities}} among {{Languages}} for {{Machine Translation}}},
  url = {http://arxiv.org/abs/1309.4168},
  abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
  date = {2013-09-16},
  keywords = {Computer Science - Computation and Language},
  author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/E4CB2KAG/Mikolov et al. - 2013 - Exploiting Similarities among Languages for Machin.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/2DUUT77T/1309.html}
}

@article{le_distributed_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.4053},
  primaryClass = {cs},
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  url = {http://arxiv.org/abs/1405.4053},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  date = {2014-05-16},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence,Computer Science - Learning},
  author = {Le, Quoc V. and Mikolov, Tomas},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/V4EC6U8K/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/7236JI6V/1405.html}
}

@inproceedings{lesk_automatic_1986,
  location = {{New York, NY, USA}},
  title = {Automatic {{Sense Disambiguation Using Machine Readable Dictionaries}}: {{How}} to {{Tell}} a {{Pine Cone}} from an {{Ice Cream Cone}}},
  isbn = {978-0-89791-224-2},
  url = {http://doi.acm.org/10.1145/318723.318728},
  doi = {10.1145/318723.318728},
  shorttitle = {Automatic {{Sense Disambiguation Using Machine Readable Dictionaries}}},
  booktitle = {Proceedings of the 5th {{Annual International Conference}} on {{Systems Documentation}}},
  series = {{{SIGDOC}} '86},
  publisher = {{ACM}},
  date = {1986},
  pages = {24--26},
  author = {Lesk, Michael}
}

@inproceedings{khodak_automated_2017,
  title = {Automated {{WordNet Construction Using Word Embeddings}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Sense}}, {{Concept}} and {{Entity Representations}} and Their {{Applications}}},
  date = {2017},
  pages = {12--23},
  keywords = {word embedding wordnet},
  author = {Khodak, Mikhail and Risteski, Andrej and Fellbaum, Christiane and Arora, Sanjeev},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/53HG34BT/Khodak et al. - 2017 - Automated WordNet Construction Using Word Embeddin.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SMACXHXW/Khodak et al. - 2017 - Automated WordNet Construction Using Word Embeddin.pdf}
}

@article{mikolov_efficient_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  url = {http://arxiv.org/abs/1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  date = {2013-01-16},
  keywords = {Computer Science - Computation and Language},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/U2RB95RH/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/RM3ZK25B/1301.html}
}

@article{bojanowski_enriching_2016,
  langid = {english},
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  url = {https://arxiv.org/abs/1607.04606},
  urldate = {2018-06-07},
  date = {2016-07-15},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/E95E7P5P/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/7QUSK2HT/1607.html}
}

@inproceedings{artetxe_generalizing_2018,
  title = {Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations},
  booktitle = {Proceedings of the {{Thirty}}-{{Second AAAI Conference}} on {{Artificial Intelligence}} ({{AAAI}}-18)},
  date = {2018},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/MCGDKG4G/Artetxe et al. - 2018 - Generalizing and improving bilingual word embeddin.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Z6TZ9Z6B/Artetxe et al. - 2018 - Generalizing and improving bilingual word embeddin.pdf}
}

@article{turney_frequency_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1003.1141},
  title = {From {{Frequency}} to {{Meaning}}: {{Vector Space Models}} of {{Semantics}}},
  volume = {37},
  issn = {1076-9757},
  url = {http://arxiv.org/abs/1003.1141},
  doi = {10.1613/jair.2934},
  shorttitle = {From {{Frequency}} to {{Meaning}}},
  abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
  journaltitle = {Journal of Artificial Intelligence Research},
  urldate = {2018-08-01},
  date = {2010-02-27},
  pages = {141-188},
  keywords = {Computer Science - Computation and Language,H.3.1,Computer Science - Information Retrieval,Computer Science - Machine Learning,I.2.6,I.2.7},
  author = {Turney, Peter D. and Pantel, Patrick},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/PZH7TLYC/Turney and Pantel - 2010 - From Frequency to Meaning Vector Space Models of .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/XWLFQR64/1003.html}
}

@article{levy_strong_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.05426},
  primaryClass = {cs},
  title = {A {{Strong Baseline}} for {{Learning Cross}}-{{Lingual Word Embeddings}} from {{Sentence Alignments}}},
  url = {http://arxiv.org/abs/1608.05426},
  abstract = {While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.},
  urldate = {2018-11-08},
  date = {2016-08-18},
  keywords = {Computer Science - Computation and Language,cross-lingual word embedding},
  author = {Levy, Omer and Søgaard, Anders and Goldberg, Yoav},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/UJ685QKV/Levy et al. - 2016 - A Strong Baseline for Learning Cross-Lingual Word .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/RN39MKCK/1608.html}
}

@article{vulic_bilingual_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.07308},
  primaryClass = {cs},
  title = {Bilingual {{Distributed Word Representations}} from {{Document}}-{{Aligned Comparable Data}}},
  url = {http://arxiv.org/abs/1509.07308},
  abstract = {We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.},
  urldate = {2018-11-12},
  date = {2015-09-24},
  keywords = {Computer Science - Computation and Language},
  author = {Vulić, Ivan and Moens, Marie-Francine},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/PXIRYREQ/Vulić and Moens - 2015 - Bilingual Distributed Word Representations from Do.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/GP7D3QMI/1509.html}
}

@inproceedings{vulic_monolingual_2015,
  location = {{New York, NY, USA}},
  title = {Monolingual and {{Cross}}-{{Lingual Information Retrieval Models Based}} on ({{Bilingual}}) {{Word Embeddings}}},
  isbn = {978-1-4503-3621-5},
  url = {http://doi.acm.org/10.1145/2766462.2767752},
  doi = {10.1145/2766462.2767752},
  abstract = {We propose a new unified framework for monolingual (MoIR) and cross-lingual information retrieval (CLIR) which relies on the induction of dense real-valued word vectors known as word embeddings (WE) from comparable data. To this end, we make several important contributions: (1) We present a novel word representation learning model called Bilingual Word Embeddings Skip-Gram (BWESG) which is the first model able to learn bilingual word embeddings solely on the basis of document-aligned comparable data; (2) We demonstrate a simple yet effective approach to building document embeddings from single word embeddings by utilizing models from compositional distributional semantics. BWESG induces a shared cross-lingual embedding vector space in which both words, queries, and documents may be presented as dense real-valued vectors; (3) We build novel ad-hoc MoIR and CLIR models which rely on the induced word and document embeddings and the shared cross-lingual embedding space; (4) Experiments for English and Dutch MoIR, as well as for English-to-Dutch and Dutch-to-English CLIR using benchmarking CLEF 2001-2003 collections and queries demonstrate the utility of our WE-based MoIR and CLIR models. The best results on the CLEF collections are obtained by the combination of the WE-based approach and a unigram language model. We also report on significant improvements in ad-hoc IR tasks of our WE-based framework over the state-of-the-art framework for learning text representations from comparable data based on latent Dirichlet allocation (LDA).},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  series = {{{SIGIR}} '15},
  publisher = {{ACM}},
  urldate = {2018-11-12},
  date = {2015},
  pages = {363--372},
  keywords = {ad-hoc retrieval,comparable data,cross-lingual information retrieval,multilinguality,semantic composition,text representation learning,vector space retrieval models,word embeddings},
  author = {Vulić, Ivan and Moens, Marie-Francine},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/XVULM8SI/Vulić and Moens - 2015 - Monolingual and Cross-Lingual Information Retrieva.pdf}
}

@article{litschko_unsupervised_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.00879},
  primaryClass = {cs},
  title = {Unsupervised {{Cross}}-{{Lingual Information Retrieval}} Using {{Monolingual Data Only}}},
  url = {http://arxiv.org/abs/1805.00879},
  abstract = {We propose a fully unsupervised framework for ad-hoc cross-lingual information retrieval (CLIR) which requires no bilingual data at all. The framework leverages shared cross-lingual word embedding spaces in which terms, queries, and documents can be represented, irrespective of their actual language. The shared embedding spaces are induced solely on the basis of monolingual corpora in two languages through an iterative process based on adversarial neural networks. Our experiments on the standard CLEF CLIR collections for three language pairs of varying degrees of language similarity (English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed fully unsupervised approach. Our CLIR models with unsupervised cross-lingual embeddings outperform baselines that utilize cross-lingual embeddings induced relying on word-level and document-level alignments. We then demonstrate that further improvements can be achieved by unsupervised ensemble CLIR models. We believe that the proposed framework is the first step towards development of effective CLIR models for language pairs and domains where parallel data are scarce or non-existent.},
  urldate = {2018-11-12},
  date = {2018-05-02},
  keywords = {Computer Science - Computation and Language},
  author = {Litschko, Robert and Glavaš, Goran and Ponzetto, Simone Paolo and Vulić, Ivan},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SCQUJN8D/Litschko et al. - 2018 - Unsupervised Cross-Lingual Information Retrieval u.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/USYHIZXU/1805.html}
}

@article{irvine_comprehensive_2017,
  title = {A {{Comprehensive Analysis}} of {{Bilingual Lexicon Induction}}},
  volume = {43},
  issn = {0891-2017},
  url = {https://doi.org/10.1162/COLI_a_00284},
  doi = {10.1162/COLI_a_00284},
  abstract = {Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages: Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese, and Welsh. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. Additionally, we introduce a novel discriminative approach to bilingual lexicon induction. Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence. When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g., using minimum reciprocal rank). We also directly compare our model's performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al. (2008). Our algorithm achieves an accuracy of 42\% versus MCCA's 15\%.},
  number = {2},
  journaltitle = {Computational Linguistics},
  urldate = {2018-11-18},
  date = {2017-03-28},
  pages = {273-310},
  author = {Irvine, Ann and Callison-Burch, Chris},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/9GGUDPBU/Irvine and Callison-Burch - 2017 - A Comprehensive Analysis of Bilingual Lexicon Indu.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/DS5MMUAR/COLI_a_00284.html}
}

@article{duong_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.09403},
  primaryClass = {cs},
  title = {Learning {{Crosslingual Word Embeddings}} without {{Bilingual Corpora}}},
  url = {http://arxiv.org/abs/1606.09403},
  abstract = {Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.},
  urldate = {2018-11-28},
  date = {2016-06-30},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  author = {Duong, Long and Kanayama, Hiroshi and Ma, Tengfei and Bird, Steven and Cohn, Trevor},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/XJVS7YU3/Duong et al. - 2016 - Learning Crosslingual Word Embeddings without Bili.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/MLBHSSMJ/1606.html}
}

@inproceedings{artetxe_learning_2017,
  location = {{Vancouver, Canada}},
  title = {Learning Bilingual Word Embeddings with (Almost) No Bilingual Data},
  url = {http://aclweb.org/anthology/P17-1042},
  doi = {10.18653/v1/P17-1042},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2018-11-28},
  date = {2017},
  pages = {451--462},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/IRJFWXUX/Artetxe et al. - 2017 - Learning bilingual word embeddings with (almost) n.pdf}
}

@inproceedings{faruqui_improving_2014,
  location = {{Gothenburg, Sweden}},
  title = {Improving {{Vector Space Word Representations Using Multilingual Correlation}}},
  url = {http://aclweb.org/anthology/E14-1049},
  doi = {10.3115/v1/E14-1049},
  booktitle = {Proceedings of the 14th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2018-11-28},
  date = {2014},
  pages = {462--471},
  author = {Faruqui, Manaal and Dyer, Chris},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ZHAHMQ6G/Faruqui and Dyer - 2014 - Improving Vector Space Word Representations Using .pdf}
}

@article{faruqui_problems_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.02276},
  primaryClass = {cs},
  title = {Problems {{With Evaluation}} of {{Word Embeddings Using Word Similarity Tasks}}},
  url = {http://arxiv.org/abs/1605.02276},
  abstract = {Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.},
  urldate = {2018-11-28},
  date = {2016-05-08},
  keywords = {Computer Science - Computation and Language},
  author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/5XLWF3FT/Faruqui et al. - 2016 - Problems With Evaluation of Word Embeddings Using .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/H26KZCGL/1605.html}
}

@article{upadhyay_cross-lingual_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.00425},
  primaryClass = {cs},
  title = {Cross-Lingual {{Models}} of {{Word Embeddings}}: {{An Empirical Comparison}}},
  url = {http://arxiv.org/abs/1604.00425},
  shorttitle = {Cross-Lingual {{Models}} of {{Word Embeddings}}},
  abstract = {Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.},
  urldate = {2018-11-28},
  date = {2016-04-01},
  keywords = {Computer Science - Computation and Language},
  author = {Upadhyay, Shyam and Faruqui, Manaal and Dyer, Chris and Roth, Dan},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/MWZM3DEG/Upadhyay et al. - 2016 - Cross-lingual Models of Word Embeddings An Empiri.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/L5IGGNSK/1604.html}
}

@article{ruder_survey_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.04902},
  primaryClass = {cs},
  title = {A {{Survey Of Cross}}-Lingual {{Word Embedding Models}}},
  url = {http://arxiv.org/abs/1706.04902},
  abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
  urldate = {2018-11-28},
  date = {2017-06-15},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/87LVYAAX/Ruder et al. - 2017 - A Survey Of Cross-lingual Word Embedding Models.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/X2ZAMGLS/1706.html}
}

@inproceedings{zhang_building_2016,
  title = {Building {{Earth Mover}}'s {{Distance}} on {{Bilingual Word Embeddings}} for {{Machine Translation}}},
  url = {http://dl.acm.org/citation.cfm?id=3016100.3016303},
  abstract = {Following their monolingual counterparts, bilingual word embeddings are also on the rise. As a major application task, word translation has been relying on the nearest neighbor to connect embeddings cross-lingually. However, the nearest neighbor strategy suffers from its inherently local nature and fails to cope with variations in realistic bilingual word embeddings. Furthermore, it lacks a mechanism to deal with many-to-many mappings that often show up across languages. We introduce Earth Mover's Distance to this task by providing a natural formulation that translates words in a holistic fashion, addressing the limitations of the nearest neighbor. We further extend the formulation to a new task of identifying parallel sentences, which is useful for statistical machine translation systems, thereby expanding the application realm of bilingual word embeddings. We show encouraging performance on both tasks.},
  booktitle = {Proceedings of the {{Thirtieth AAAI Conference}} on {{Artificial Intelligence}}},
  series = {{{AAAI}}'16},
  publisher = {{AAAI Press}},
  urldate = {2019-02-14},
  date = {2016},
  pages = {2870--2876},
  author = {Zhang, Meng and Liu, Yang and Luan, Huanbo and Sun, Maosong and Izuha, Tatsuya and Hao, Jie},
  venue = {Phoenix, Arizona}
}

@article{grave_unsupervised_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.11222},
  primaryClass = {cs, stat},
  title = {Unsupervised {{Alignment}} of {{Embeddings}} with {{Wasserstein Procrustes}}},
  url = {http://arxiv.org/abs/1805.11222},
  abstract = {We consider the task of aligning two sets of points in high dimension, which has many applications in natural language processing and computer vision. As an example, it was recently shown that it is possible to infer a bilingual lexicon, without supervised data, by aligning word embeddings trained on monolingual data. These recent advances are based on adversarial training to learn the mapping between the two embeddings. In this paper, we propose to use an alternative formulation, based on the joint estimation of an orthogonal matrix and a permutation matrix. While this problem is not convex, we propose to initialize our optimization algorithm by using a convex relaxation, traditionally considered for the graph isomorphism problem. We propose a stochastic algorithm to minimize our cost function on large scale problems. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. On this task, our method obtains state of the art results, while requiring less computational resources than competing approaches.},
  urldate = {2019-02-14},
  date = {2018-05-28},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Grave, Edouard and Joulin, Armand and Berthet, Quentin},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/7HAHD4B8/Grave et al. - 2018 - Unsupervised Alignment of Embeddings with Wasserst.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/R8QKKNF2/1805.html}
}

@article{radford_improving_nodate,
  langid = {english},
  title = {Improving {{Language Understanding}} by {{Generative Pre}}-{{Training}}},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  pages = {12},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/6QEGU8NC/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{radford_language_nodate,
  langid = {english},
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  pages = {24},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AUCIVHKH/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{peters_deep_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.05365},
  primaryClass = {cs},
  title = {Deep Contextualized Word Representations},
  url = {http://arxiv.org/abs/1802.05365},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  urldate = {2019-02-20},
  date = {2018-02-14},
  keywords = {Computer Science - Computation and Language},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/R3T6EBPM/Peters et al. - 2018 - Deep contextualized word representations.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/FR7VQZ2C/1802.html}
}

@article{cuturi_sinkhorn_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0895},
  primaryClass = {stat},
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transportation Distances}}},
  url = {http://arxiv.org/abs/1306.0895},
  shorttitle = {Sinkhorn {{Distances}}},
  abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
  urldate = {2019-02-22},
  date = {2013-06-04},
  keywords = {Statistics - Machine Learning},
  author = {Cuturi, Marco},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/XQBJZRY9/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/BGMAQI53/1306.html}
}

@inproceedings{nothman_stop_2018,
  location = {{Melbourne, Australia}},
  title = {Stop {{Word Lists}} in {{Free Open}}-Source {{Software Packages}}},
  url = {http://www.aclweb.org/anthology/W18-2502},
  abstract = {Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. "hasn't" but not "hadn't") and inclusions ("computer"), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP}}-{{OSS}})},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-02-27},
  date = {2018-07},
  pages = {7--12},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/347Z2YQQ/Nothman et al. - 2018 - Stop Word Lists in Free Open-source Software Packa.pdf}
}

@article{pedersen_dannet_2009,
  langid = {english},
  title = {{{DanNet}}: The Challenge of Compiling a Wordnet for {{Danish}} by Reusing a Monolingual Dictionary},
  volume = {43},
  issn = {1574-0218},
  url = {https://doi.org/10.1007/s10579-009-9092-1},
  doi = {10.1007/s10579-009-9092-1},
  shorttitle = {{{DanNet}}},
  abstract = {This paper is a contribution to the discussion on compiling computational lexical resources from conventional dictionaries. It describes the theoretical as well as practical problems that are encountered when reusing a conventional dictionary for compiling a lexical-semantic resource in terms of a wordnet. More specifically, it describes the methodological issues of compiling a wordnet for Danish, DanNet, from a monolingual basis, and not—as is often seen—by applying the translational expansion method with Princeton WordNet as the English source. Thus, we apply as our basis a large, corpus-based printed dictionary of modern Danish. Using this approach, we discuss the issues of readjusting inconsistent and/or underspecified hyponymy hierarchies taken from the conventional dictionary, sense distinctions as opposed to the synonym sets of wordnets, generating semantic wordnet relations on the basis of sense definitions, and finally, supplementing missing or implicit information.},
  number = {3},
  journaltitle = {Lang Resources \& Evaluation},
  urldate = {2019-03-23},
  date = {2009-09-01},
  pages = {269-299},
  keywords = {Dictionary,Hyponymy,Lexical semantics,Nouns,Semantic relations,Verbs,Wordnet},
  author = {Pedersen, Bolette Sandford and Nimb, Sanni and Asmussen, Jørg and Sørensen, Nicolai Hartvig and Trap-Jensen, Lars and Lorentzen, Henrik},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NTFZN3XS/Pedersen et al. - 2009 - DanNet the challenge of compiling a wordnet for D.pdf}
}

@article{de_boom_representation_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.00570},
  title = {Representation Learning for Very Short Texts Using Weighted Word Embedding Aggregation},
  volume = {80},
  issn = {01678655},
  url = {http://arxiv.org/abs/1607.00570},
  doi = {10.1016/j.patrec.2016.06.012},
  abstract = {Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box.},
  journaltitle = {Pattern Recognition Letters},
  urldate = {2019-03-26},
  date = {2016-09},
  pages = {150-156},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  author = {De Boom, Cedric and Van Canneyt, Steven and Demeester, Thomas and Dhoedt, Bart},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/D3FIFPRV/De Boom et al. - 2016 - Representation learning for very short texts using.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/2VVSAQ2D/1607.html}
}

@article{glavas_how_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.00508},
  primaryClass = {cs},
  title = {How to ({{Properly}}) {{Evaluate Cross}}-{{Lingual Word Embeddings}}: {{On Strong Baselines}}, {{Comparative Analyses}}, and {{Some Misconceptions}}},
  url = {http://arxiv.org/abs/1902.00508},
  shorttitle = {How to ({{Properly}}) {{Evaluate Cross}}-{{Lingual Word Embeddings}}},
  abstract = {Cross-lingual word embeddings (CLEs) enable multilingual modeling of meaning and facilitate cross-lingual transfer of NLP models. Despite their ubiquitous usage in downstream tasks, recent increasingly popular projection-based CLE models are almost exclusively evaluated on a single task only: bilingual lexicon induction (BLI). Even BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we make the first step towards a comprehensive evaluation of cross-lingual word embeddings. We thoroughly evaluate both supervised and unsupervised CLE models on a large number of language pairs in the BLI task and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI can result in deteriorated downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess existing baselines, which still display competitive performance across the board. We hope that our work will catalyze further work on CLE evaluation and model analysis.},
  urldate = {2019-04-04},
  date = {2019-02-01},
  keywords = {Computer Science - Computation and Language},
  author = {Glavas, Goran and Litschko, Robert and Ruder, Sebastian and Vulic, Ivan},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/J8FEKW9D/Glavas et al. - 2019 - How to (Properly) Evaluate Cross-Lingual Word Embe.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Y2K45GPP/1902.html}
}

@article{jonker_shortest_1987,
  langid = {english},
  title = {A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems},
  volume = {38},
  issn = {1436-5057},
  url = {https://doi.org/10.1007/BF02278710},
  doi = {10.1007/BF02278710},
  abstract = {We develop a shortest augmenting path algorithm for the linear assignment problem. It contains new initialization routines and a special implementation of Dijkstra's shortest path method. For both dense and sparse problems computational experiments show this algorithm to be uniformly faster than the best algorithms from the literature. A Pascal implementation is presented.},
  number = {4},
  journaltitle = {Computing},
  urldate = {2019-04-04},
  date = {1987-12-01},
  pages = {325-340},
  keywords = {68 E 10,90 C 08,Linear assignment problem,Pascal implementation,shortest path methods},
  author = {Jonker, R. and Volgenant, A.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/UT9ZEJAS/Jonker and Volgenant - 1987 - A shortest augmenting path algorithm for dense and.pdf}
}

@article{de_boom_learning_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.00765},
  title = {Learning {{Semantic Similarity}} for {{Very Short Texts}}},
  url = {http://arxiv.org/abs/1512.00765},
  doi = {10.1109/ICDMW.2015.86},
  abstract = {Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.},
  journaltitle = {2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
  urldate = {2019-04-16},
  date = {2015-11},
  pages = {1229-1234},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  author = {De Boom, Cedric and Van Canneyt, Steven and Bohez, Steven and Demeester, Thomas and Dhoedt, Bart},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/VNNPYN8R/De Boom et al. - 2015 - Learning Semantic Similarity for Very Short Texts.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/A64EP4R5/1512.html}
}

@article{levy_improving_2015,
  title = {Improving {{Distributional Similarity}} with {{Lessons Learned}} from {{Word Embeddings}}},
  volume = {3},
  doi = {10.1162/tacl_a_00134},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  date = {2015-12-01},
  pages = {211-225},
  author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CQGXXRS3/Levy et al. - 2015 - Improving Distributional Similarity with Lessons L.pdf}
}

@article{mu_all-but--top_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.01417},
  primaryClass = {cs, stat},
  title = {All-but-the-{{Top}}: {{Simple}} and {{Effective Postprocessing}} for {{Word Representations}}},
  url = {http://arxiv.org/abs/1702.01417},
  shorttitle = {All-but-the-{{Top}}},
  abstract = {Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a \{\textbackslash{}em very simple\}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations \{\textbackslash{}em even stronger\}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and \{ text classification\}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.},
  urldate = {2019-04-24},
  date = {2017-02-05},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  author = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CD9TDZZR/Mu et al. - 2017 - All-but-the-Top Simple and Effective Postprocessi.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/HILLFWK4/1702.html}
}

@article{conneau_xnli_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.05053},
  primaryClass = {cs},
  title = {{{XNLI}}: {{Evaluating Cross}}-Lingual {{Sentence Representations}}},
  url = {http://arxiv.org/abs/1809.05053},
  shorttitle = {{{XNLI}}},
  abstract = {State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.},
  urldate = {2019-04-24},
  date = {2018-09-13},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R. and Schwenk, Holger and Stoyanov, Veselin},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/JN25KTGH/Conneau et al. - 2018 - XNLI Evaluating Cross-lingual Sentence Representa.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NUZTKPVQ/1809.html}
}

@article{singh_context_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.09663},
  primaryClass = {cs, stat},
  title = {Context {{Mover}}'s {{Distance}} \& {{Barycenters}}: {{Optimal}} Transport of Contexts for Building Representations},
  url = {http://arxiv.org/abs/1808.09663},
  shorttitle = {Context {{Mover}}'s {{Distance}} \& {{Barycenters}}},
  abstract = {We present a framework for building unsupervised representations of entities and their compositions, where each entity is viewed as a probability distribution rather than a fixed length vector. In particular, this distribution is supported over the contexts which co-occur with the entity and are embedded in a suitable low-dimensional space. This enables us to consider the problem of representation learning with a perspective from Optimal Transport and take advantage of its numerous tools such as Wasserstein distance and Wasserstein barycenters. We elaborate how the method can be applied for obtaining unsupervised representations of text and illustrate the performance quantitatively as well as qualitatively on tasks such as measuring sentence similarity and word entailment, where we empirically observe significant gains (e.g., 4.1\% relative improvement over Sent2vec and GenSen). The key benefits of the proposed approach include: (a) capturing uncertainty and polysemy via modeling the entities as distributions, (b) utilizing the underlying geometry of the particular task (with the ground cost), (c) simultaneously providing interpretability with the notion of optimal transport between contexts and (d) easy applicability on top of existing point embedding methods. In essence, the framework can be useful for any unsupervised or supervised problem (on text or other modalities); and only requires a co-occurrence structure inherent to many problems. The code, as well as pre-built histograms, are available under https://github.com/context-mover/.},
  urldate = {2019-04-24},
  date = {2018-08-29},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Singh, Sidak Pal and Hug, Andreas and Dieuleveut, Aymeric and Jaggi, Martin},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/47KHRGPC/Singh et al. - 2018 - Context Mover's Distance & Barycenters Optimal tr.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NC3UPLG5/1808.html}
}

@thesis{aldarmaki_cross-lingual_2019,
  langid = {english},
  location = {{United States -- District of Columbia}},
  title = {Cross-{{Lingual Alignment}} of {{Word}} \& {{Sentence Embeddings}}},
  url = {https://search.proquest.com/docview/2206636278/abstract/8A87D7B035414476PQ/1},
  abstract = {One of the notable developments in current natural language processing is the practical efficacy of probabilistic word representations, where words are embedded in high-dimensional continuous vector spaces that are optimized to reflect their distributional relationships. For sequences of words, such as phrases and sentences, distributional representations can be estimated by combining word embeddings using arithmetic operations like vector averaging or by estimating composition parameters from data using various objective functions. The quality of these compositional representations is typically estimated by their performance as features in extrinsic supervised classification benchmarks. Word and compositional embeddings for a single language can be induced without supervision using a large training corpus of raw text. To handle multiple languages and dialects, bilingual dictionaries and parallel corpora are often used for learning cross-lingual embeddings directly or to align pre-trained monolingual embeddings.
In this work, we explore and develop various cross-lingual alignment techniques, compare the performance of the resulting cross-lingual embeddings, and study their characteristics. We pay particular attention to the bilingual data requirements of each approach since lower requirements facilitate wider language expansion. To begin with, we analyze various monolingual general-purpose sentence embedding models to better understand their qualities. By comparing their performance on extrinsic evaluation benchmarks and unsupervised clustering, we infer the characteristics of the most dominant features in their respective vector spaces.
We then look into various cross-lingual alignment frameworks with different degrees of supervision. We begin with unsupervised word alignment, for which we propose an approach for inducing cross-lingual word mappings with no prior bilingual resources. We rely on assumptions about the consistency and structural similarities between the monolingual vector spaces of different languages. Using comparable monolingual news corpora, our approach resulted in highly accurate word mappings for two language pairs: French to English, and Arabic to English. With various refinement heuristics, the performance of the unsupervised alignment methods approached the performance of supervised dictionary mapping.
Finally, we develop and evaluate different alignment approaches based on parallel text. We show that incorporating context in the alignment process often leads to significant improvements in performance. At the word level, we explore the alignment of contextualized word embeddings that are dynamically generated for each sentence. At the sentence level, we develop and investigate three alignment frameworks: joint modeling, representation transfer, and sentence mapping, applied to different sentence embedding models. We experiment with a matrix factorization model based on word-sentence co-occurrence statistics, and two general-purpose neural sentence embedding models. We report the performance of the various cross-lingual models with different sizes of parallel corpora to assess the minimal degree of supervision required by each alignment framework.},
  pagetotal = {113},
  institution = {{The George Washington University}},
  type = {Ph.D.},
  urldate = {2019-04-24},
  date = {2019},
  keywords = {Applied sciences,Cross-lingual embeddings,Language,literature and linguistics,NLP,Sentence embeddings,Unsupervised mapping,Word embeddings,Word translation},
  author = {Aldarmaki, Hanan},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/5S9NLD8M/Aldarmaki - 2019 - Cross-Lingual Alignment of Word & Sentence Embeddi.pdf}
}

@article{jawanpuria_learning_2019,
  title = {Learning {{Multilingual Word Embeddings}} in {{Latent Metric Space}}: {{A Geometric Approach}}},
  volume = {7},
  url = {https://doi.org/10.1162/tacl_a_00257},
  doi = {10.1162/tacl_a_00257},
  shorttitle = {Learning {{Multilingual Word Embeddings}} in {{Latent Metric Space}}},
  abstract = {We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks.We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  urldate = {2019-05-02},
  date = {2019-03-01},
  pages = {107-120},
  author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev}
}

@book{manning_introduction_2009,
  langid = {english},
  location = {{Cambridge}},
  title = {Introduction to Information Retrieval},
  edition = {Reprinted},
  isbn = {978-0-521-86571-5},
  pagetotal = {482},
  publisher = {{Cambridge Univ. Press}},
  date = {2009},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/VL52WU6Y/Manning et al. - 2009 - Introduction to information retrieval.pdf},
  note = {OCLC: 549201180}
}

@article{arora_simple_2016,
  title = {A {{Simple}} but {{Tough}}-to-{{Beat Baseline}} for {{Sentence Embeddings}}},
  url = {https://openreview.net/forum?id=SyK00v5xx},
  abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs....},
  urldate = {2019-05-07},
  date = {2016-11-04},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ZCAPE7A7/Arora et al. - 2016 - A Simple but Tough-to-Beat Baseline for Sentence E.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/F2DLKYWI/forum.html}
}

@online{somers_youre_2014,
  langid = {american},
  title = {You’re Probably Using the Wrong Dictionary},
  url = {http://jsomers.net/blog/dictionary},
  urldate = {2019-05-19},
  date = {2014-05-18},
  author = {Somers, James},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CPYDBQW9/dictionary.html}
}

@article{bond_survey_2012,
  title = {A {{Survey}} of {{WordNets}} and Their {{Licenses}}},
  abstract = {This paper surveys currently avail-able wordnets. We measure the ef-fect that license choice has on their us-age, measured by the number of cita-tions. Finally, we discuss methods to make wordnets more generally accessi-ble, starting with a shared online server for freely distributable wordnets.},
  date = {2012-01-01},
  author = {Bond, Francis and Paik, Kyonghee},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/DQL7LC9A/Bond and Paik - 2012 - A Survey of WordNets and their Licenses.pdf}
}

@report{ruci_current_2008,
  title = {On the Current State of {{Albanet}} and Related Applications},
  institution = {{Technical report, University of Vlora.(http://fjalnet. com …}},
  date = {2008},
  author = {Ruci, Ervin}
}

@inproceedings{simov_constructing_2010,
  title = {Constructing of an {{Ontology}}-Based {{Lexicon}} for {{Bulgarian}}.},
  booktitle = {{{LREC}}},
  publisher = {{Citeseer}},
  date = {2010},
  author = {Simov, Kiril Ivanov and Osenova, Petya},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/GUC2U3NG/Simov and Osenova - 2010 - Constructing of an Ontology-based Lexicon for Bulg.pdf}
}

@inproceedings{stamou_exploring_2004,
  title = {Exploring {{Balkanet Shared Ontology}} for {{Multilingual Conceptual Indexing}}.},
  booktitle = {{{LREC}}},
  date = {2004},
  author = {Stamou, Sofia and Nenadic, Goran and Christodoulakis, Dimitris},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/B7HHGLHR/Stamou et al. - 2004 - Exploring Balkanet Shared Ontology for Multilingua.pdf}
}

@inproceedings{tufis_romanian_2008,
  title = {Romanian Wordnet: {{Current}} State, New Applications and Prospects},
  shorttitle = {Romanian Wordnet},
  booktitle = {Proceedings of 4th {{Global WordNet Conference}}, {{GWC}}},
  date = {2008},
  pages = {441--452},
  author = {Tufiş, Dan and Ion, Radu and Bozianu, Luigi and Ceauşu, Alexandru and Ştefănescu, Dan},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CADVMCCW/Tufiş et al. - 2008 - Romanian wordnet Current state, new applications .pdf}
}

@inproceedings{fiser_slownet_2012,
  title = {{{sloWNet}} 3.0: Development, Extension and Cleaning},
  shorttitle = {{{sloWNet}} 3.0},
  booktitle = {Proceedings of 6th {{International Global Wordnet Conference}} ({{GWC}} 2012)},
  date = {2012},
  pages = {113--117},
  author = {Fišer, Darja and Novak, Jernej and Erjavec, Tomaž},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/K5MC7PA9/Fišer et al. - 2012 - sloWNet 3.0 development, extension and cleaning.pdf}
}

@article{miller_wordnet_1995,
  title = {{{WordNet}}: {{A Lexical Database}} for {{English}}},
  volume = {38},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/219717.219748},
  doi = {10.1145/219717.219748},
  shorttitle = {{{WordNet}}},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  number = {11},
  journaltitle = {Commun. ACM},
  urldate = {2019-05-20},
  date = {1995-11},
  pages = {39--41},
  author = {Miller, George A.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/EAU8RULD/Miller - 1995 - WordNet A Lexical Database for English.pdf}
}

@inproceedings{gordeev_unsupervised_2018,
  title = {Unsupervised {{Cross}}-Lingual {{Matching}} of {{Product Classifications}}},
  url = {http://dl.acm.org/citation.cfm?id=3299905.3299967},
  booktitle = {Proceedings of the 23rd {{Conference}} of {{Open Innovations Association FRUCT}}},
  series = {{{FRUCT}}'23},
  publisher = {{FRUCT Oy}},
  date = {2018},
  pages = {62:459-62:464},
  keywords = {word embeddings,cross-lingual embeddings,unsupervised category matching},
  author = {Gordeev, Denis and Rey, Alexey and Shagarov, Dmitry},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/APF99QHB/Gordeev et al. - 2018 - Unsupervised Cross-lingual Matching of Product Cla.pdf},
  venue = {Bologna, Italy},
  articleno = {62},
  numpages = {6},
  acmid = {3299967}
}

@article{edilson_a._correa_nilc-usp_2017,
  title = {{{NILC}}-{{USP}} at {{SemEval}}-2017 {{Task}} 4: {{A Multi}}-View {{Ensemble}} for {{Twitter Sentiment Analysis}}},
  shorttitle = {{{NILC}}-{{USP}} at {{SemEval}}-2017 {{Task}} 4},
  abstract = {This paper describes our multi-view ensemble approach to SemEval-2017 Task 4 on Sentiment Analysis in Twitter, specifically, the Message Polarity Classification subtask for English (subtask A). Our system is a voting ensemble, where each base classifier is trained in a different feature space. The first space is a bag-of-words model and has a Linear SVM as base classifier. The second and third spaces are two different strategies of combining word embeddings to represent sentences and use a Linear SVM and a Logistic Regressor as base classifiers. The proposed system was ranked 18th out of 38 systems considering F1 score and 20th considering recall.},
  date = {2017-04-07},
  author = {Edilson A. Corrêa, Jr and Marinho, Vanessa and Borges dos Santos, Leandro},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/D6TNF56U/Edilson A. Corrêa et al. - 2017 - NILC-USP at SemEval-2017 Task 4 A Multi-view Ense.pdf}
}

@inproceedings{bengio_neural_2000,
  title = {A {{Neural Probabilistic Language Model}}},
  volume = {3},
  doi = {10.1162/153244303322533223},
  abstract = {A goal of statistical language modeling is to learn the joint probabilit y function of sequences of words. This is intrinsically difficult because o f the curse of dimensionality: we propose to fight it with its own weap ons. In the proposed approach one learns simultaneously (1) a distributed r ep- resentation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these repr e- sentations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, sh owing on two text corpora that the proposed approach very significantly im- proves on a state-of-the-art trigram model.},
  eventtitle = {Journal of {{Machine Learning Research}}},
  date = {2000-01-01},
  pages = {932-938},
  author = {Bengio, Y and Ducharme, Réjean and Vincent, Pascal},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/QW3T28FK/Bengio et al. - 2000 - A Neural Probabilistic Language Model.pdf}
}

@inproceedings{maslova_neural_2017,
  title = {Neural Network Doc2vec in Automated Sentiment Analysis for Short Informal Texts},
  booktitle = {International {{Conference}} on {{Speech}} and {{Computer}}},
  publisher = {{Springer}},
  date = {2017},
  pages = {546--554},
  author = {Maslova, Natalia and Potapov, Vsevolod},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SPCQ9GW8/Maslova and Potapov - 2017 - Neural network doc2vec in automated sentiment anal.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/A9TZKQ8Y/978-3-319-66429-3_54.html}
}

@collection{sterkenburg_practical_2003,
  langid = {english},
  location = {{Amsterdam}},
  title = {A Practical Guide to Lexicography},
  isbn = {978-90-272-2329-6 978-90-272-2330-2 978-1-58811-380-1 978-1-58811-381-8},
  pagetotal = {459},
  number = {6},
  series = {Terminology and Lexicography Research and Practice},
  publisher = {{Benjamins}},
  date = {2003},
  editor = {van Sterkenburg, Piet},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ILPVSRRX/Sterkenburg - 2003 - A practical guide to lexicography.pdf},
  note = {OCLC: 249659375}
}

@article{sagot_building_2008,
  title = {Building a Free {{French}} Wordnet from Multilingual Resources},
  abstract = {This paper describes automatic construction a freely-available wordnet for French (WOLF) based on Princeton WordNet (PWN) by using various multilingual resources. Polysemous words were dealt with an approach in which a parallel corpus for five languages was word-aligned and the extracted multilingual lexicon was disambiguated with the existing wordnets for these languages. On the other hand, a bilingual approach sufficed to acquire equivalents for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri. The results obtained from each resource were merged and ranked according to the number of resources yielding the same literal. Automatic evaluation of the merged wordnet was performed with the French WordNet (FREWN). Manual evaluation was also carried out on a sample of the generated synsets. Precision shows that the presented approach has proved to be very promising and applications to use the created wordnet are already intended.},
  date = {2008-05-31},
  author = {Sagot, Benoît and Fišer, Darja},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/MBQ5UPWA/Sagot and Fiser - 2008 - Building a free French wordnet from multilingual r.pdf}
}

@article{resnik_distinguishing_1999,
  langid = {english},
  title = {Distinguishing Systems and Distinguishing Senses: New Evaluation Methods for {{Word Sense Disambiguation}}},
  volume = {5},
  issn = {1469-8110, 1351-3249},
  url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitledistinguishing-systems-and-distinguishing-senses-new-evaluation-methods-for-word-sense-disambiguationdiv/5802B206D98444DCB022479D9E45AE90},
  shorttitle = {Distinguishing Systems and Distinguishing Senses},
  abstract = {Resnik and Yarowsky (1997) made a set of observations about the state-of-the-art in automatic
word sense disambiguation and, motivated by those observations, offered several specific
proposals regarding improved evaluation criteria, common training and testing resources,
and the definition of sense inventories. Subsequent discussion of those proposals resulted
in SENSEVAL, the first evaluation exercise for word sense disambiguation (Kilgarriff and
Palmer 2000). This article is a revised and extended version of our 1997 workshop paper,
reviewing its observations and proposals and discussing them in light of the SENSEVAL exercise.
It also includes a new in-depth empirical study of translingually-based sense inventories
and distance measures, using statistics collected from native-speaker annotations of 222
polysemous contexts across 12 languages. These data show that monolingual sense distinctions
at most levels of granularity can be effectively captured by translations into some set of second
languages, especially as language family distance increases. In addition, the probability that
a given sense pair will tend to lexicalize differently across languages is shown to correlate
with semantic salience and sense granularity; sense hierarchies automatically generated from
such distance matrices yield results remarkably similar to those created by professional
monolingual lexicographers.},
  number = {2},
  journaltitle = {Natural Language Engineering},
  urldate = {2019-05-22},
  date = {1999-06},
  pages = {113-133},
  author = {Resnik, Philip and Yarowsky, David},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/6NRSBW77/Resnik and Yarowsky - 1999 - Distinguishing systems and distinguishing senses .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/MG5E7VVY/5802B206D98444DCB022479D9E45AE90.html}
}

@article{uzun_1945ten_1999,
  title = {1945'{{TEN}}  {{BU YANA}}  {{TÜRKÇE}}  {{SÖZLÜKLER}}},
  issn = {1300-2864},
  number = {7-8},
  journaltitle = {KEBİKEÇ İnsan Bilimleri İçin Kaynak Araştırmaları Dergisi},
  date = {1999},
  pages = {53 - 57},
  author = {Uzun, Leyla}
}

@article{ibrahim_usta_turkce_2006,
  title = {Türkçe {{Sözlük Hazırlamada Yöntem Sorunları}}},
  doi = {10.1501/Dtcfder_0000001033},
  journaltitle = {Ankara Üniversitesi Dil ve Tarih-Coğrafya Fakültesi Dergisi},
  date = {2006-01-01},
  pages = {223-242},
  author = {İbrahim USTA, Halil},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/UN8NNCV3/İbrahim USTA - 2006 - Türkçe Sözlük Hazırlamada Yöntem Sorunları.pdf}
}

@article{uzun_modern_2005,
  title = {Modern Dilbilim Bulguları Işığında {{Türkçe}} Sözlüğe Bir Bakış},
  journaltitle = {Çukurova Üniversitesi Türkoloji Araştırmaları Merkezi},
  date = {2005},
  author = {Uzun, Engin Nadir}
}

@inproceedings{banerjee_adapted_2002,
  langid = {english},
  title = {An {{Adapted Lesk Algorithm}} for {{Word Sense Disambiguation Using WordNet}}},
  isbn = {978-3-540-45715-2},
  abstract = {This paper presents an adaptation of Lesk’s dictionarybased word sense disambiguation algorithm. Rather than using a standard dictionary as the source of glosses for our approach, the lexical database WordNet is employed. This provides a rich hierarchy of semantic relations that our algorithm can exploit. This method is evaluated using the English lexical sample data from the Senseval-2 word sense disambiguation exercise, and attains an overall accuracy of 32\%. This represents a significant improvement over the 16\% and 23\% accuracy attained by variations of the Lesk algorithm used as benchmarks during the SENSEVAL-2 comparative exercise among word sense disambiguation systems.},
  booktitle = {Computational {{Linguistics}} and {{Intelligent Text Processing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2002},
  pages = {136-145},
  keywords = {Context Window,Function Word,Relation Pair,Target Word,Word Sense Disambiguation},
  author = {Banerjee, Satanjeev and Pedersen, Ted},
  editor = {Gelbukh, Alexander}
}

@inproceedings{metzler_similarity_2007,
  langid = {english},
  title = {Similarity {{Measures}} for {{Short Segments}} of {{Text}}},
  isbn = {978-3-540-71496-5},
  abstract = {Measuring the similarity between documents and queries has been extensively studied in information retrieval. However, there are a growing number of tasks that require computing the similarity between two very short segments of text. These tasks include query reformulation, sponsored search, and image retrieval. Standard text similarity measures perform poorly on such tasks because of data sparseness and the lack of context. In this work, we study this problem from an information retrieval perspective, focusing on text representations and similarity measures. We examine a range of similarity measures, including purely lexical measures, stemming, and language modeling-based measures. We formally evaluate and analyze the methods on a query-query similarity task using 363,822 queries from a web search log. Our analysis provides insights into the strengths and weaknesses of each method, including important tradeoffs between effectiveness and efficiency.},
  booktitle = {Advances in {{Information Retrieval}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2007},
  pages = {16-27},
  keywords = {Excellent Match,Expanded Representation,Query Expansion,Short Segment,Similarity Measure},
  author = {Metzler, Donald and Dumais, Susan and Meek, Christopher},
  editor = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni}
}

@incollection{fellbaum_semantic_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {A {{Semantic Network}} of {{English}}: {{The Mother}} of {{All WordNets}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_6},
  shorttitle = {A {{Semantic Network}} of {{English}}},
  abstract = {We give a brief outline of the design and contents of the English lexical database WordNet, which serves as a model for similarly conceived wordnets in several European languages. WordNet is a semantic network, in which the meanings of nouns, verbs, adjectives, and adverbs are represented in terms of their links to other (groups of) words via conceptual-semantic and lexical relations. Each part of speech is treated differently reflecting different semantic properties. We briefly discuss polysemy in WordNet, and focus on the case of meaning extensions in the verb lexicon Finally, we outline the potential uses of WordNet not only for applications in natural language processing, but also for research in stylistic analyses in conjunction with a semantic concordance.1},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-23},
  date = {1998},
  pages = {137-148},
  keywords = {lexicon,Natural Language Processing,semantic network},
  author = {Fellbaum, Christiane},
  editor = {Vossen, Piek},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/4UQ2ITP5/Fellbaum - 1998 - A Semantic Network of English The Mother of All W.pdf},
  doi = {10.1007/978-94-017-1491-4_6}
}

@book{kendall_forgotten_2011,
  langid = {english},
  title = {The {{Forgotten Founding Father}}: {{Noah Webster}}'s {{Obsession}} and the {{Creation}} of an {{American Culture}}},
  edition = {1st Edition},
  isbn = {0-399-15699-2},
  abstract = {In 1828 Noah Webster published the groundbreaking American Dictionary of the English Language and secured his niche as an avatar of a distinct American culture. Kendall (The Man Who Made Lists) honors Webster's crucial contributions to early American nationalism, which extended far beyond his primary obsession, the written word. Kendall paints a complex portrait of Webster (1758–1843), a man he claims "housed a host of contradictory identities: revolutionary, reactionary, fighter, peacemaker, intellectual, commonsense philosopher, ladies' man, prig, slick networker and loner." In spite of his flaws, Webster, Kendall argues not wholly successfully, belongs among the ranks of America's notable founders, associating with George Washington and Ben Franklin, among others, to craft an early American identity rooted in national pride and a distinctly American lexicon. Citing frequent references to Webster's nervous afflictions, Kendall ventures the somewhat shaky diagnosis of obsessive-compulsive disorder. The book includes the politics of the "forgotten" founder, for example, noting that Webster "detested Andrew Jackson as the second coming of Jefferson," and a wide range of his activities, including helping found Amherst College. Kendall provides an intriguing look at one of America's earliest men of letters that is sure to appeal to lovers of both words and history. (Apr.)},
  pagetotal = {368},
  publisher = {{G.P. Putnam's Sons}},
  date = {2011-04-14},
  author = {Kendall, Joshua}
}

@article{mikolov_distributed_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.4546},
  primaryClass = {cs, stat},
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  url = {http://arxiv.org/abs/1310.4546},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  urldate = {2019-05-25},
  date = {2013-10-16},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CBJYIIJC/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/EUSP9GW3/1310.html}
}

@article{balikas_cross-lingual_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.04437},
  primaryClass = {cs, stat},
  title = {Cross-Lingual {{Document Retrieval}} Using {{Regularized Wasserstein Distance}}},
  url = {http://arxiv.org/abs/1805.04437},
  abstract = {Many information retrieval algorithms rely on the notion of a good distance that allows to efficiently compare objects of different nature. Recently, a new promising metric called Word Mover's Distance was proposed to measure the divergence between text passages. In this paper, we demonstrate that this metric can be extended to incorporate term-weighting schemes and provide more accurate and computationally efficient matching between documents using entropic regularization. We evaluate the benefits of both extensions in the task of cross-lingual document retrieval (CLDR). Our experimental results on eight CLDR problems suggest that the proposed methods achieve remarkable improvements in terms of Mean Reciprocal Rank compared to several baselines.},
  urldate = {2019-05-25},
  date = {2018-05-11},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  author = {Balikas, Georgios and Laclau, Charlotte and Redko, Ievgen and Amini, Massih-Reza},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YHEQQYC5/Balikas et al. - 2018 - Cross-lingual Document Retrieval using Regularized.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/XX9I4BC9/1805.html}
}

@inproceedings{kusner_word_2015,
  title = {From {{Word Embeddings}} to {{Document Distances}}},
  url = {http://dl.acm.org/citation.cfm?id=3045118.3045221},
  abstract = {We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  series = {{{ICML}}'15},
  publisher = {{JMLR.org}},
  urldate = {2019-05-25},
  date = {2015},
  pages = {957--966},
  author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
  venue = {Lille, France}
}

@book{fellbaum_wordnet_1998,
  title = {{{WordNet}} : An Electronic Lexical Database},
  isbn = {978-0-262-27255-1},
  shorttitle = {{{WordNet}}},
  abstract = {Summary: with a preface by George Miller WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets.The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.Contributors : Reem Al-Halimi, Robert C. Berwick, J. F. M. Burg, Martin Chodorow, Christiane Fellbaum, Joachim Grabowski, Sanda Harabagiu, Marti A. Hearst, Graeme Hirst, Douglas A. Jones, Rick Kazman, Karen T. Kohl, Shari Landes, Claudia Leacock, George A. Miller, Katherine J. Miller, Dan Moldovan, Naoyuki Nomura, Uta Priss, Philip Resnik, David St-Onge, Randee Tengi, Reind P. van de Riet, Ellen Voorhees.},
  series = {Language, Speech, and Communication},
  publisher = {{MIT Press}},
  date = {1998},
  keywords = {Electronic books,English language -- Data processing,Lexicology -- Data processing,Semantics -- Data processing,WordNet},
  author = {Fellbaum, Christiane}
}

@article{linden_finnwordnet_2010,
  title = {{{FinnWordNet}} --- {{WordNet}} Påfinska via Översättning},
  volume = {17},
  journaltitle = {LexicoNordica --- Nordic Journal of Lexicography},
  date = {2010},
  pages = {119-140},
  author = {Lindén, Krister and Carlson., Lauri},
  note = {In Swedish with an English abstract}
}

@article{vossen_introduction_1998,
  title = {Introduction to {{EuroWordNet}}},
  volume = {32},
  issn = {0010-4817},
  url = {https://www.jstor.org/stable/30200456},
  abstract = {[This paper gives a global introduction to the aims and objectives of the EuroWordNet project, and it provides a general framework for the other papers in this volume. EuroWordNet is an EC project that develops a multilingual database with wordnets in several European languages, structured along the same lines as the Princeton WordNet. Each wordnet represents an autonomous structure of language-specific lexicalizations, which are interconnected via an Inter-Lingual-Index. The wordnets are built at different sites from existing resources, starting from a shared level of basic concepts and extended top-down. The results will be publicly available and will be tested in cross-language information retrieval applications.]},
  number = {2/3},
  journaltitle = {Computers and the Humanities},
  urldate = {2019-05-26},
  date = {1998},
  pages = {73-89},
  author = {Vossen, Piek},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/G9QVFQ7K/Vossen - 1998 - Introduction to EuroWordNet.pdf}
}

@inproceedings{fiser_leveraging_2009,
  title = {Leveraging {{Parallel Corpora}} and {{Existing Wordnets}} for {{Automatic Construction}} of the {{Slovene Wordnet}}},
  doi = {10.1007/978-3-642-04235-5_31},
  abstract = {The paper reports on a series of experiments conducted in order to test the feasibility of automatically generating synsets
for Slovene wordnet. The resources used were the multilingual parallel corpus of George Orwell’s Nineteen Eighty-Four and
wordnets for several languages. First, the corpus was word-aligned to obtain multilingual lexicons and then these lexicons
were compared to the wordnets in various languages in order to disambiguate the entries and attach appropriate synset ids
to Slovene entries in the lexicon. Slovene lexicon entries sharing the same attached synset id were then organized into a
synset. The results obtained by the different settings in the experiment are evaluated against a manually created gold standard
and also checked by hand.},
  date = {2009-08-25},
  pages = {359-368},
  author = {Fiser, Darja},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3DMQJC4L/Fiser - 2009 - Leveraging Parallel Corpora and Existing Wordnets .pdf}
}

@article{pianta_multiwordnet_2002,
  title = {{{MultiWordNet}}: {{Developing}} an {{Aligned Multilingual Database}}},
  shorttitle = {{{MultiWordNet}}},
  date = {2002-01-01},
  author = {Pianta, Emanuele and Bentivogli, Luisa and Girardi, C}
}

@article{miller_nouns_1990,
  langid = {english},
  title = {Nouns in {{WordNet}}: {{A Lexical Inheritance System}}},
  volume = {3},
  issn = {0950-3846},
  url = {https://academic.oup.com/ijl/article/3/4/245/923281},
  doi = {10.1093/ijl/3.4.245},
  shorttitle = {Nouns in {{WordNet}}},
  abstract = {Abstract.  Definitions of common nouns typically give a superordinate term plus distinguishing features; that information provides the basis for organizing noun},
  number = {4},
  journaltitle = {Int J Lexicography},
  urldate = {2019-05-26},
  date = {1990-12-01},
  pages = {245-264},
  author = {Miller, George A.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/FIYEU93U/923281.html}
}

@article{winston_taxonomy_1987,
  langid = {english},
  title = {A {{Taxonomy}} of {{Part}}-{{Whole Relations}}},
  volume = {11},
  issn = {1551-6709},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1104_2},
  doi = {10.1207/s15516709cog1104_2},
  abstract = {A taxonomy of part-whole or meronymic relations is developed to explain the ordinary English-speaker's use of the term “part of” and its cognates. The resulting classification yields six types of meronymic relations: 1. component-integral object (pedal-bike), 2. member-collection (ship-fleet), 3. portion-mass (slice-pie), 4. stuff-object (steel-car), 5. feature-activity (paying-shopping), and 6. place-area (Everglades-Florida). Meronymic relations ore further distinguished from other inclusion relations, such as spatial inclusion, and class inclusion, and from several other semantic relations: attribution, attachment, and ownership. This taxonomy is then used to explain cases of apparent intransitivity in merological syllogisms, and standard form syllogisms whose premises express different inclusion relations. The data suggest that intransitivities arise due to equivocations between different types of semantic relations. These results are then explained by means of the relation element theory which accounts for the character and behavior of semantic relations in terms of more primitive relational elements. The inferential phenomena observed are then explained by means of a single principle of element matching.},
  number = {4},
  journaltitle = {Cognitive Science},
  urldate = {2019-05-26},
  date = {1987},
  pages = {417-444},
  author = {Winston, Morton E. and Chaffin, Roger and Herrmann, Douglas},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/R5VWVZMF/Winston et al. - 1987 - A Taxonomy of Part-Whole Relations.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ASMZZYGD/s15516709cog1104_2.html}
}

@incollection{rodriguez_top-down_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {The {{Top}}-{{Down Strategy}} for {{Building EuroWordNet}}: {{Vocabulary Coverage}}, {{Base Concepts}} and {{Top Ontology}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_3},
  shorttitle = {The {{Top}}-{{Down Strategy}} for {{Building EuroWordNet}}},
  abstract = {This paper describes two fundamental aspects in the process of building of the EuroWordNet database. In EuroWordNet we have chosen for a flexible design in which local wordnets are built relatively independently as language-specific structures, which are linked to an Inter-Lingual-Index (ILI). To ensure compatibility between the wordnets, a core set of common concepts has been defined that has to be covered by every language. Furthermore, these concepts have been classified via the ILI in terms of a Top Ontology of 63 fundamental semantic distinctions used in various semantic theories and paradigms. This paper first discusses the process leading to the definition of the set of Base Concepts, and the structure and the rationale of the Top Ontology.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {45-80},
  keywords = {Base Concepts,ontology building,Top Ontology},
  author = {Rodríguez, Horacio and Climent, Salvador and Vossen, Piek and Bloksma, Laura and Peters, Wim and Alonge, Antonietta and Bertagna, Francesca and Roventini, Adriana},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_3}
}

@incollection{alonge_linguistic_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {The {{Linguistic Design}} of the {{EuroWordNet Database}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_2},
  abstract = {In this paper the linguistic design of the database under construction within the EuroWordNet project is described. This is mainly structured along the same lines as the Princeton Word-Net, although some changes have been made to the WordNet overall design due to both theoretical and practical reasons. The most important reasons for such changes are the multilinguality of the EuroWordNet database and the fact that it is intended to be used in Language Engineering applications. Thus, i) some relations have been added to those identified in WordNet; ii) some labels have been identified which can be added to the relations in order to make their implications more explicit and precise; iii) some relations, already present in the WordNet design, have been modified in order to specify their role more clearly.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {19-43},
  keywords = {equivalence relations,language-internal relations,lexical-semantic relations,synset},
  author = {Alonge, Antonietta and Calzolari, Nicoletta and Vossen, Piek and Bloksma, Laura and Castellon, Irene and Marti, Maria Antonia and Peters, Wim},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_2}
}

@incollection{gonzalo_applying_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {Applying {{EuroWordNet}} to {{Cross}}-{{Language Text Retrieval}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_5},
  abstract = {We discuss ways in which EuroWordNet (EWN) can be used in multilingual information retrieval activities, focusing on two approaches to Cross-Language Text Retrieval that use the EWN database as a large-scale multilingual semantic resource. The first approach indexes documents and queries in terms of the EuroWordNet Inter-Lingual-Index, thus turning term weighting and query/document matching into language-independent tasks. The second describes how the information in the EWN database could be integrated with a corpus-based technique, thus allowing retrieval of domain-specific terms that may not be present in our multilingual database. Our objective is to show the potential of EuroWordNet as a promising alternative to existing approaches to Cross-Language Text Retrieval.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {113-135},
  keywords = {cross-language text retrieval,large-scale ontologies,multilingual lexical resources},
  author = {Gonzalo, Julio and Verdejo, Felisa and Peters, Carol and Calzolari, Nicoletta},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_5}
}

@incollection{peters_cross-linguistic_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {Cross-Linguistic {{Alignment}} of {{Wordnets}} with an {{Inter}}-{{Lingual}}-{{Index}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_7},
  abstract = {This paper discusses the design of the EuroWordNet database, in which semantic databases like WordNetl.5 for several languages are combined via a so-called inter-lingual-index. In this database, language-independent data is shared whilst language-specific properties are maintained. A special interface has been developed to compare the semantic configurations across languages and to track down differences.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {149-179},
  keywords = {aligning wordnets,equivalence relations,multilingual database},
  author = {Peters, Wim and Vossen, Piek and Díez-Orzas, Pedro and Adriaens, Geert},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_7}
}

@incollection{vossen_compatibility_1998,
  langid = {english},
  location = {{Dordrecht}},
  title = {Compatibility in {{Interpretation}} of {{Relations}} in {{EuroWordNet}}},
  isbn = {978-94-017-1491-4},
  url = {https://doi.org/10.1007/978-94-017-1491-4_4},
  abstract = {This paper describes how the Euro WordNet project established a maximum level of consensus in the interpretation of relations, without loosing the possibility of encoding language-specific lexicalizations. Problematic cases arise due to the fact that each site re-used different resources and because the core vocabulary of the wordnets show complex properties. Many of these cases are discussed with respect to language internal and equivalence relations. Possible solutions are given in the form of additional criteria.},
  booktitle = {{{EuroWordNet}}: {{A}} Multilingual Database with Lexical Semantic Networks},
  publisher = {{Springer Netherlands}},
  urldate = {2019-05-26},
  date = {1998},
  pages = {81-112},
  keywords = {overlapping relations and lexical gaps,sense differentiation},
  author = {Vossen, Piek and Bloksma, Laura and Alonge, Antonietta and Marinai, Elisabetta and Peters, Carol and Castellon, Irene and Marti, Antonia and Rigau, German},
  editor = {Vossen, Piek},
  doi = {10.1007/978-94-017-1491-4_4}
}

@inproceedings{knight_building_1994,
  title = {Building a Large-Scale Knowledge Base for Machine Translation},
  abstract = {Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PAN-GLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.},
  booktitle = {In {{Proceedings}} of {{AAAI}}},
  date = {1994},
  author = {Knight, Kevin and Luk, Steve K.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ZIWINL2B/Knight and Luk - 1994 - Building a large-scale knowledge base for machine .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/KNYD8SXY/summary.html}
}

@online{farreres_using_1998,
  langid = {english},
  title = {Using {{WordNet}} for {{Building WordNets}}},
  url = {/paper/Using-WordNet-for-Building-WordNets-Farreres-Rigau/01405726f0dac56b063cb4ee04e766daf3993c23},
  urldate = {2019-05-26},
  date = {1998},
  author = {Farreres, Xavier and Rigau, German and Rodríguez, Horacio},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3X6GVUW9/Farreres et al. - 1998 - Using WordNet for Building WordNets.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/JQKY6K9J/01405726f0dac56b063cb4ee04e766daf3993c23.html}
}

@book{farreres_towards_2004,
  title = {Towards {{Binding Spanish Senses}} to {{Wordnet Senses}}},
  abstract = {This work tries to enrich the Spanish Wordnet using a Spanish taxonomy  as a knowledge source. The Spanish taxonomy is composed by Spanish senses, while  Wordnet is composed by synsets (English senses). A set of weighted associations  between Spanish words and Wordnet synsets is used for inferring associations between  both taxonomies.},
  date = {2004},
  author = {Farreres, Javier and Gibert, Karina and Rodríguez, Horacio},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/F3VTKBV4/Alignment et al. - 2004 - Towards Binding Spanish Senses to Wordnet Senses.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/63QJXR4Q/summary.html}
}

@inproceedings{xiao_distributed_2014,
  title = {Distributed {{Word Representation Learning}} for {{Cross}}-{{Lingual Dependency Parsing}}},
  doi = {10.3115/v1/W14-1613},
  abstract = {This paper proposes to learn languageindependent word representations to address cross-lingual dependency parsing, which aims to predict the dependency parsing trees for sentences in the target language by training a dependency parser with labeled sentences from a source language. We first combine all sentences from both languages to induce real-valued distributed representation of words under a deep neural network architecture, which is expected to capture semantic similarities of words not only within the same language but also across different languages. We then use the induced interlingual word representation as augmenting features to train a delexicalized dependency parser on labeled sentences in the source language and apply it to the target sentences. To investigate the effectiveness of the proposed technique, extensive experiments are conducted on cross-lingual dependency parsing tasks with nine different languages. The experimental results demonstrate the superior cross-lingual generalizability of the word representation induced by the proposed approach, comparing to alternative comparison methods.},
  date = {2014-01-01},
  pages = {119-129},
  author = {Xiao, Min and Guo, Yuhong},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/EIX39ZF3/Xiao and Guo - 2014 - Distributed Word Representation Learning for Cross.pdf}
}

@book{klementiev_inducing_2012,
  title = {Inducing {{Crosslingual Distributed Representations}} of {{Words}}},
  abstract = {Distributed representations of words have proven extremely useful in numerous natural language processing tasks. Their appeal is that they can help alleviate data sparsity problems common to supervised learning. Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages. In this work, we induce distributed representations for a pair of languages jointly. We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another. We show that our representations are informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language.},
  date = {2012},
  author = {Klementiev, Alexandre and Titov, Ivan and Bhattarai, Binod},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/UPMFIE54/Klementiev et al. - 2012 - Inducing Crosslingual Distributed Representations .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/WTDG675R/summary.html}
}

@thesis{ercan_automated_2006,
  title = {{{AUTOMATED TEXT SUMMARIZATION AND KEYPHRASE EXTRACTION}}},
  date = {2006},
  author = {Ercan, Gonenc}
}

@article{artetxe_unsupervised_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.01272},
  primaryClass = {cs},
  title = {Unsupervised {{Statistical Machine Translation}}},
  url = {http://arxiv.org/abs/1809.01272},
  abstract = {While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https://github.com/artetxem/monoses},
  urldate = {2019-05-27},
  date = {2018-09-04},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/XWPQBIAM/Artetxe et al. - 2018 - Unsupervised Statistical Machine Translation.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/JBAKJJ3G/1809.html}
}

@article{salton_vector_1975,
  title = {A {{Vector Space Model}} for {{Automatic Indexing}}},
  volume = {18},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/361219.361220},
  doi = {10.1145/361219.361220},
  abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
  number = {11},
  journaltitle = {Commun. ACM},
  urldate = {2019-05-28},
  date = {1975-11},
  pages = {613--620},
  keywords = {automatic indexing,automatic information retrieval,content analysis,document space},
  author = {Salton, G. and Wong, A. and Yang, C. S.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/95WVZKYH/Salton et al. - 1975 - A Vector Space Model for Automatic Indexing.pdf}
}

@article{camacho-collados_word_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.04032},
  primaryClass = {cs},
  title = {From {{Word}} to {{Sense Embeddings}}: {{A Survey}} on {{Vector Representations}} of {{Meaning}}},
  url = {http://arxiv.org/abs/1805.04032},
  shorttitle = {From {{Word}} to {{Sense Embeddings}}},
  abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
  urldate = {2019-05-28},
  date = {2018-05-10},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/KD2S5MMV/Camacho-Collados and Pilehvar - 2018 - From Word to Sense Embeddings A Survey on Vector .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Q843ME6G/1805.html}
}

@book{salton_introduction_1986,
  location = {{New York, NY, USA}},
  title = {Introduction to {{Modern Information Retrieval}}},
  isbn = {978-0-07-054484-0},
  publisher = {{McGraw-Hill, Inc.}},
  date = {1986},
  author = {Salton, Gerard and McGill, Michael J.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/PN8NTRXH/(Mcgraw Hill Computer Science Series) Gerard Salton - Introduction to Modern Information Retrieval-Mcgraw-Hill College (1983).djvu}
}

@inproceedings{p._turian_word_2010,
  title = {Word {{Representations}}: {{A Simple}} and {{General Method}} for {{Semi}}-{{Supervised Learning}}.},
  volume = {2010},
  shorttitle = {Word {{Representations}}},
  abstract = {If we take an existing supervised NLP sys- tem, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih \& Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accu- racy of these baselines. We find further improvements by combining di erent word representations. You can download our word features, for o -the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/},
  eventtitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  date = {2010-01-01},
  pages = {384-394},
  author = {P. Turian, Joseph and Ratinov, Lev-Arie and Bengio, Y},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/9EPRW4KG/P. Turian et al. - 2010 - Word Representations A Simple and General Method .pdf}
}

@article{salton_term-weighting_1988,
  title = {Term-Weighting Approaches in Automatic Text Retrieval},
  volume = {24},
  issn = {0306-4573},
  url = {http://www.sciencedirect.com/science/article/pii/0306457388900210},
  doi = {10.1016/0306-4573(88)90021-0},
  abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.},
  number = {5},
  journaltitle = {Information Processing \& Management},
  urldate = {2019-05-28},
  date = {1988-01-01},
  pages = {513-523},
  author = {Salton, Gerard and Buckley, Christopher},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/7H7J8CWG/Salton and Buckley - 1988 - Term-weighting approaches in automatic text retrie.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/TALCQS38/0306457388900210.html}
}

@inproceedings{bond_linking_2013,
  title = {Linking and {{Extending}} an {{Open Multilingual Wordnet}}},
  volume = {1},
  abstract = {We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open licences, data from Wiktionary and the Unicode Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of languages.},
  eventtitle = {{{ACL}} 2013 - 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}, {{Proceedings}} of the {{Conference}}},
  date = {2013-08-01},
  pages = {1352-1362},
  author = {Bond, Francis and Foster, Ryan}
}

@inproceedings{collobert_unified_2008,
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  isbn = {978-1-60558-205-4},
  url = {http://dl.acm.org/citation.cfm?id=1390156.1390177},
  doi = {10.1145/1390156.1390177},
  shorttitle = {A Unified Architecture for Natural Language Processing},
  eventtitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  publisher = {{ACM}},
  urldate = {2019-05-29},
  date = {2008-05-07},
  pages = {160-167},
  author = {Collobert, Ronan and Weston, Jason},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/HKV2BMY6/Collobert and Weston - 2008 - A unified architecture for natural language proces.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/EYLKGHFT/citation.html}
}

@article{lund_producing_1996,
  langid = {english},
  title = {Producing High-Dimensional Semantic Spaces from Lexical Co-Occurrence},
  volume = {28},
  issn = {1532-5970},
  url = {https://doi.org/10.3758/BF03204766},
  doi = {10.3758/BF03204766},
  abstract = {A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).},
  number = {2},
  journaltitle = {Behavior Research Methods, Instruments, \& Computers},
  urldate = {2019-05-29},
  date = {1996-06-01},
  pages = {203-208},
  keywords = {Target Word,Semantic Distance,Semantic Space,Vector Similarity,Word Pair},
  author = {Lund, Kevin and Burgess, Curt},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AL32VTCC/Lund and Burgess - 1996 - Producing high-dimensional semantic spaces from le.pdf}
}

@article{harris_distributional_1954,
  title = {Distributional {{Structure}}},
  volume = {10},
  issn = {0043-7956},
  url = {https://doi.org/10.1080/00437956.1954.11659520},
  doi = {10.1080/00437956.1954.11659520},
  number = {2-3},
  journaltitle = {WORD},
  urldate = {2019-05-29},
  date = {1954-08-01},
  pages = {146-162},
  author = {Harris, Zellig S.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CJCLI3BF/Harris - 1954 - Distributional Structure.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/WKMHBIGB/00437956.1954.html}
}

@article{deerwester_indexing_1990,
  title = {Indexing by Latent Semantic Analysis},
  volume = {41},
  number = {6},
  journaltitle = {Journal of the American society for information science},
  date = {1990},
  pages = {391--407},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AX7VGKEQ/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf}
}

@article{bengio_neural_2003,
  title = {A Neural Probabilistic Language Model},
  volume = {3},
  issue = {Feb},
  journaltitle = {Journal of machine learning research},
  date = {2003},
  pages = {1137--1155},
  author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/B3TMW76E/bengio03a.html;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/R36IZ3R8/plink.html}
}

@inproceedings{mikolov2018advances,
  title = {Advances in {{Pre}}-{{Training Distributed Word Representations}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  date = {2018},
  author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/QUAT3A34/Mikolov et al. - 2018 - Advances in Pre-Training Distributed Word Represen.pdf}
}

@inproceedings{arcan_expanding_2016,
  langid = {american},
  title = {Expanding Wordnets to New Languages with Multilingual Sense Disambiguation},
  url = {https://www.aclweb.org/anthology/papers/C/C16/C16-1010/},
  eventtitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  urldate = {2019-05-30},
  date = {2016-12},
  pages = {97-108},
  author = {Arcan, Mihael and McCrae, John Philip and Buitelaar, Paul},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/RG984P8I/Arcan et al. - 2016 - Expanding wordnets to new languages with multiling.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SVJ87C9L/C16-1010.html}
}

@article{arcan_polylingual_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.01411},
  primaryClass = {cs},
  title = {Polylingual {{Wordnet}}},
  url = {http://arxiv.org/abs/1903.01411},
  abstract = {Princeton WordNet is one of the most important resources for natural language processing, but is only available for English. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches. For this reason we leverage existing translations of WordNet in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. We evaluate our approach using 10 translated wordnets for European languages. Our experiment shows a significant improvement over translation without any contextual information. Furthermore, we evaluate how the choice of pivot languages affects performance of multilingual word sense disambiguation.},
  urldate = {2019-05-30},
  date = {2019-03-04},
  keywords = {Computer Science - Computation and Language},
  author = {Arcan, Mihael and McCrae, John and Buitelaar, Paul},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/6VBNCLGA/Arcan et al. - 2019 - Polylingual Wordnet.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/P3PTLWLQ/1903.html}
}

@article{jones_statistical_1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  volume = {28},
  abstract = {Abstract: The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing, in particular, that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure. Exhaustivity and specificity We are familiar with the notions of exhaustivity and specificity: exhaustivity is a property of index descriptions, and specificity one of index terms. They are most clearly illustrated by a simple keyword or descriptor system. In this case the exhaustivity of a document description is the coverage of its various topics given by the terms assigned to it; and the specificity of an individual term is the level of detail at which a given concept is represented.},
  journaltitle = {Journal of Documentation},
  date = {1972},
  pages = {11--21},
  author = {Jones, Karen Spärck},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SDFA6FUD/Jones - 1972 - A statistical interpretation of term specificity a.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YPQJFBTN/summary.html}
}

@article{dubin_most_2004,
  title = {The {{Most Influential Paper Gerard Salton Never Wrote}}},
  volume = {52},
  issn = {00242594},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=f6h&AN=14619164&lang=tr&site=eds-live&authtype=uid},
  abstract = {Gerard Salton is often credited with developing the vector space model (VSM)for information retrieval (IR). Citations to Salton give the impression that the VSM must have been articulated as an IR model sometime between 1970 and 1975. However, the VSM as it is understood today evolved over a longer time period than is usually acknowledged, and an articulation of the model and its assumptions did not appear in print until several years after those assumptions had been criticized and alternative models proposed. An often cited overview paper titled "A Vector Space Model for Information Retrieval" (alleged to have been published in 1975) does not exist, and citations to it represent a confusion of two 1975 articles, neither of which were overviews of the VSM as a model of information retrieval. Until the late 1970s, Salton did not present vector spaces as models of IR generally but rather as models of specific computations. Citations to the phantom paper reflect an apparently widely held misconception that the operational features and explanatory devices now associated with the VSM must have been introduced at the same time it was first proposed as an IR model.},
  number = {4},
  journaltitle = {Library Trends},
  urldate = {2019-05-30},
  date = {2004-21},
  pages = {748},
  keywords = {GerardVECTOR spacesVECTOR analysisINFORMATION retrievalDOCUMENTATIONINFORMATION science,SALTON},
  author = {Dubin, David},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/QNLKGIEZ/Dubin - 2004 - The Most Influential Paper Gerard Salton Never Wro.pdf}
}

@article{almeida_word_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09069},
  primaryClass = {cs, stat},
  title = {Word {{Embeddings}}: {{A Survey}}},
  url = {http://arxiv.org/abs/1901.09069},
  shorttitle = {Word {{Embeddings}}},
  abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
  urldate = {2019-05-30},
  date = {2019-01-25},
  keywords = {A.1,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7,Statistics - Machine Learning},
  author = {Almeida, Felipe and Xexéo, Geraldo},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/55LKUTA5/Almeida and Xexéo - 2019 - Word Embeddings A Survey.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/VJA3CTN4/1901.html}
}

@article{firth_synopsis_1957,
  title = {A Synopsis of Linguistic Theory 1930--1955},
  volume = {Special volume of the Phiological Society},
  journaltitle = {Studies in linguistic analysis},
  date = {1957},
  pages = {11},
  author = {Firth, John Ruper}
}

@article{church_word_1990,
  title = {Word {{Association Norms}}, {{Mutual Information}}, and {{Lexicography}}},
  volume = {16},
  issn = {0891-2017},
  url = {http://dl.acm.org/citation.cfm?id=89086.89095},
  abstract = {The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.},
  number = {1},
  journaltitle = {Comput. Linguist.},
  urldate = {2019-05-31},
  date = {1990-03},
  pages = {22--29},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/GRGNACB7/Church and Hanks - 1990 - Word Association Norms, Mutual Information, and Le.pdf}
}

@book{forsythe_computer_1977,
  title = {Computer Methods for Mathematical Computations},
  volume = {259},
  publisher = {{Prentice-hall Englewood Cliffs, NJ}},
  date = {1977},
  author = {Forsythe, George Elmer and Malcolm, Michael A. and Moler, Cleve B.}
}

@inproceedings{schutze_dimensions_1992,
  location = {{Los Alamitos, CA, USA}},
  title = {Dimensions of {{Meaning}}},
  isbn = {978-0-8186-2630-2},
  url = {http://dl.acm.org/citation.cfm?id=147877.148132},
  booktitle = {Proceedings of the 1992 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  series = {Supercomputing '92},
  publisher = {{IEEE Computer Society Press}},
  urldate = {2019-05-31},
  date = {1992},
  pages = {787--796},
  author = {Schütze, H.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/KPTP84TL/Schütze - 1992 - Dimensions of Meaning.pdf},
  venue = {Minneapolis, Minnesota, USA}
}

@inproceedings{xu_can_2000,
  title = {Can Artificial Neural Networks Learn Language Models?},
  booktitle = {Sixth {{International Conference}} on {{Spoken Language Processing}}},
  date = {2000},
  author = {Xu, Wei and Rudnicky, Alex},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CBGN6FED/Xu and Rudnicky - 2000 - Can artificial neural networks learn language mode.pdf}
}

@article{goldberg_word2vec_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1402.3722},
  primaryClass = {cs, stat},
  title = {Word2vec {{Explained}}: Deriving {{Mikolov}} et Al.'s Negative-Sampling Word-Embedding Method},
  url = {http://arxiv.org/abs/1402.3722},
  shorttitle = {Word2vec {{Explained}}},
  abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
  urldate = {2019-06-02},
  date = {2014-02-15},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Goldberg, Yoav and Levy, Omer},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/VQLG46UP/Goldberg and Levy - 2014 - word2vec Explained deriving Mikolov et al.'s nega.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/E58W5AGI/1402.html}
}

@inproceedings{mikolov_linguistic_2013,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  date = {2013},
  pages = {746--751},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NNTN7MGF/Mikolov et al. - 2013 - Linguistic regularities in continuous space word r.pdf}
}

@inproceedings{pennington_glove_2014,
  title = {Glove: {{Global}} Vectors for Word Representation},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  date = {2014},
  pages = {1532--1543},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3EVJEGNW/Pennington et al. - 2014 - Glove Global vectors for word representation.pdf}
}

@inproceedings{levy_neural_2014,
  location = {{Cambridge, MA, USA}},
  title = {Neural {{Word Embedding As Implicit Matrix Factorization}}},
  url = {http://dl.acm.org/citation.cfm?id=2969033.2969070},
  abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  series = {{{NIPS}}'14},
  publisher = {{MIT Press}},
  urldate = {2019-06-02},
  date = {2014},
  pages = {2177--2185},
  author = {Levy, Omer and Goldberg, Yoav},
  venue = {Montreal, Canada}
}

@inproceedings{baroni_dont_2014,
  title = {Don't Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  url = {https://www.aclweb.org/anthology/P14-1023},
  doi = {10.3115/v1/P14-1023},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  date = {2014-06},
  pages = {238-247},
  author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/TGTCTUDI/Baroni et al. - 2014 - Don’t count, predict! A systematic comparison of c.pdf},
  venue = {Baltimore, Maryland}
}

@article{bullinaria_extracting_2007,
  title = {Extracting Semantic Representations from Word Co-Occurrence Statistics: {{A}} Computational Study},
  shorttitle = {Extracting Semantic Representations from Word Co-Occurrence Statistics},
  abstract = {Abstract: In a previous paper we presented a systematic computational study of the extraction of semantic representations from the word-word co-occurrence statistics of large text corpora. The conclusion was that semantic vectors of Pointwise Mutual Information (PMI) values from very small co-occurrence windows, together with a cosine distance measure, consistently resulted in the best representations across a range of psychologically relevant semantic tasks. This paper extends that study by investigating the use of three further factors, namely the application of stop-lists, word stemming, and dimensionality reduction using Singular Value Decomposition (SVD), that have been used to provide improved performance elsewhere. It also introduces an additional semantic task and explores the advantages of using a much larger corpus. This leads to the discovery and analysis of improved SVD based methods for generating semantic representations (that provide new state-of-the-art performance on a standard TOEFL task) and the identification and discussion of problems and misleading results that can arise without a full systematic study.},
  journaltitle = {Behavior Research Methods},
  date = {2007},
  pages = {510--526},
  author = {Bullinaria, John A. and Levy, Joseph P.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Z4M6Q46E/Bullinaria and Levy - 2007 - Extracting semantic representations from word co-o.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AZ7FR8YW/summary.html}
}

@incollection{mnih_learning_2013,
  title = {Learning Word Embeddings Efficiently with Noise-Contrastive Estimation},
  url = {http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  date = {2013},
  pages = {2265-2273},
  author = {Mnih, Andriy and Kavukcuoglu, Koray},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.}
}

@article{landauer_solution_1997,
  title = {A Solution to {{Plato}}'s Problem: {{The}} Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.},
  volume = {104},
  shorttitle = {A Solution to {{Plato}}'s Problem},
  number = {2},
  journaltitle = {Psychological review},
  date = {1997},
  pages = {211},
  author = {Landauer, Thomas K. and Dumais, Susan T.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/9ZQVNR5Z/Landauer and Dumais - 1997 - A solution to Plato's problem The latent semantic.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Y84CS5B6/doiLanding.html}
}

@inproceedings{morin_hierarchical_2005,
  title = {Hierarchical Probabilistic Neural Network Language Model.},
  volume = {5},
  booktitle = {Aistats},
  publisher = {{Citeseer}},
  date = {2005},
  pages = {246--252},
  author = {Morin, Frederic and Bengio, Yoshua}
}

@inproceedings{zhao_ecnu_2015,
  title = {{{ECNU}}: {{Using Traditional Similarity Measurements}} and {{Word Embedding}} for {{Semantic Textual Similarity Estimation}}},
  doi = {10.18653/v1/S15-2021},
  shorttitle = {{{ECNU}}},
  abstract = {This paper reports our submissions to semantic textual similarity task, i.e., task 2 in Semantic Evaluation 2015. We built our systems using various traditional features, such as string-based, corpus-based and syntactic similarity metrics, as well as novel similarity measures based on distributed word representations, which were trained using deep learning paradigms. Since the training and test datasets consist of instances collected from various domains, three different strategies of the usage of training datasets were explored: (1) use all available training datasets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73.},
  booktitle = {{{SemEval}}@{{NAACL}}-{{HLT}}},
  date = {2015},
  keywords = {Computer multitasking,Deep learning,Multi-task learning,Semantic similarity,Supervised learning,Test set,Text corpus,Trustworthy computing,Word embedding},
  author = {Zhao, Jiang and Lan, Man and Tian, Junfeng},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/K6S26JLL/Zhao et al. - 2015 - ECNU Using Traditional Similarity Measurements an.pdf}
}

@inproceedings{saric_takelab_2012,
  location = {{Stroudsburg, PA, USA}},
  title = {{{TakeLab}}: {{Systems}} for {{Measuring Semantic Text Similarity}}},
  url = {http://dl.acm.org/citation.cfm?id=2387636.2387708},
  shorttitle = {{{TakeLab}}},
  abstract = {This paper describes the two systems for determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6. Most of the research on semantic similarity of textual content focuses on large documents. However, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson -- 2nd and 3rd, normalized Pearson -- 1st and 3rd, weighted mean -- 2nd and 5th).},
  booktitle = {Proceedings of the {{First Joint Conference}} on {{Lexical}} and {{Computational Semantics}} - {{Volume}} 1: {{Proceedings}} of the {{Main Conference}} and the {{Shared Task}}, and {{Volume}} 2: {{Proceedings}} of the {{Sixth International Workshop}} on {{Semantic Evaluation}}},
  series = {{{SemEval}} '12},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-02},
  date = {2012},
  pages = {441--448},
  author = {Šarić, Frane and Glavaš, Goran and Karan, Mladen and Šnajder, Jan and Bašić, Bojana Dalbelo},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/I7CWA5G7/Šarić et al. - 2012 - TakeLab Systems for Measuring Semantic Text Simil.pdf},
  venue = {Montréal, Canada}
}

@article{ruder_discriminative_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.09334},
  primaryClass = {cs, stat},
  title = {A {{Discriminative Latent}}-{{Variable Model}} for {{Bilingual Lexicon Induction}}},
  url = {http://arxiv.org/abs/1808.09334},
  abstract = {We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.},
  urldate = {2019-06-03},
  date = {2018-08-28},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Ruder, Sebastian and Cotterell, Ryan and Kementchedjhieva, Yova and Søgaard, Anders},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/WB7FGA2N/Ruder et al. - 2018 - A Discriminative Latent-Variable Model for Bilingu.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/S7U5KFHF/1808.html}
}

@inproceedings{jagarlamudi_bilingual_2011,
  location = {{Stroudsburg, PA, USA}},
  title = {From {{Bilingual Dictionaries}} to {{Interlingual Document Representations}}},
  isbn = {978-1-932432-88-6},
  url = {http://dl.acm.org/citation.cfm?id=2002736.2002768},
  abstract = {Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Short Papers}} - {{Volume}} 2},
  series = {{{HLT}} '11},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-03},
  date = {2011},
  pages = {147--152},
  keywords = {matching},
  author = {Jagarlamudi, Jagadeesh and Daumé, III, Hal and Udupa, Raghavendra},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/K648X7IF/Jagarlamudi et al. - 2011 - From Bilingual Dictionaries to Interlingual Docume.pdf},
  venue = {Portland, Oregon}
}

@inproceedings{daume_domain_2011,
  location = {{Stroudsburg, PA, USA}},
  title = {Domain {{Adaptation}} for {{Machine Translation}} by {{Mining Unseen Words}}},
  isbn = {978-1-932432-88-6},
  url = {http://dl.acm.org/citation.cfm?id=2002736.2002819},
  abstract = {We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrase-based translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Short Papers}} - {{Volume}} 2},
  series = {{{HLT}} '11},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-03},
  date = {2011},
  pages = {407--412},
  keywords = {matching},
  author = {Daumé, III, Hal and Jagarlamudi, Jagadeesh},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/UKZ2EZHT/Daumé and Jagarlamudi - 2011 - Domain Adaptation for Machine Translation by Minin.pdf},
  venue = {Portland, Oregon}
}

@article{singhal_modern_2001,
  title = {Modern Information Retrieval: A Brief Overview},
  volume = {24},
  shorttitle = {Modern Information Retrieval},
  abstract = {For thousands of years people have realized the importance of archiving and finding information. With the advent of computers, it became possible to store large amounts of information; and finding useful information from such collections became a necessity. The field of Information Retrieval (IR) was born in the 1950s out of this necessity. Over the last forty years, the field has matured considerably. Several IR systems are used on an everyday basis by a wide variety of users. This article is a brief overview of the key advances in the field of Information Retrieval, and a description of where the state-of-the-art is at in the field.},
  journaltitle = {Bulletin of the Ieee Computer Society Technical Committee on Data Engineering},
  date = {2001},
  pages = {2001},
  author = {Singhal, Amit},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Y4P7GIQE/Singhal - 2001 - Modern information retrieval a brief overview.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/6TS6UQBR/summary.html}
}

@article{bush_as_1945,
  title = {As We May Think},
  volume = {176},
  number = {1},
  journaltitle = {The atlantic monthly},
  date = {1945},
  pages = {101--108},
  author = {Bush, Vannevar},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YGX7UZTB/Bush - 1945 - As we may think.pdf}
}

@article{luhn_statistical_1957,
  title = {A Statistical Approach to Mechanized Encoding and Searching of Literary Information},
  volume = {1},
  number = {4},
  journaltitle = {IBM Journal of research and development},
  date = {1957},
  pages = {309--317},
  author = {Luhn, Hans Peter},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/TH8IEUQD/Luhn - 1957 - A statistical approach to mechanized encoding and .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/TM8CVX8R/5392697.html}
}

@article{groves_friend_2015,
  title = {Friend or Foe? {{Google Translate}} in Language for Academic Purposes},
  volume = {37},
  issn = {0889-4906},
  url = {http://www.sciencedirect.com/science/article/pii/S088949061400060X},
  doi = {10.1016/j.esp.2014.09.001},
  shorttitle = {Friend or Foe?},
  abstract = {A recent development in digital technology, machine translation (MT), is improving in its ability to translate with grammatical and lexical accuracy, and is also becoming increasingly available for students of language for academic purposes. Given the acceptance of other digital technology for teaching and learning, it seems likely that machine translation will become a tool students will rely on to complete their assignments in a second language. This would have implications for the community of practice of academic language teaching. In this study students were asked to submit an essay in their first language and this was then translated into English through a web-based translation engine. The resulting English text was analysed for grammatical error. The analysis found that the translation engine was far from able to produce error-free text – however, judging in relation to international testing standards, the level of accuracy is approaching the minimum needed for university admission at many institutions. Thus, this paper sets out to argue, based on the assumption that MT will continue to improve, that this technology will have a profound influence on the teaching of Languages for Academic Purposes, and with imaginative use, will allow this influence to be positive for both the students and their instructors.},
  journaltitle = {English for Specific Purposes},
  urldate = {2019-06-03},
  date = {2015-01-01},
  pages = {112-121},
  keywords = {Academic literacy,IT in SLA,Language for academic purposes,Machine translation,google translate},
  author = {Groves, Michael and Mundt, Klaus},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/G4ZK6ZL8/Groves and Mundt - 2015 - Friend or foe Google Translate in language for ac.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/QVKWSS4F/S088949061400060X.html}
}

@inproceedings{voorhees_trec-8_1999,
  title = {The {{TREC}}-8 {{Question Answering Track Report}}},
  abstract = {The TREC-8 Question Answering track was the first large-scale evaluation of domain-independent  question answering systems. This paper summarizes the results of the track by giving a brief overview of  the different approaches taken to solve the problem. The most accurate systems found a correct response  for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for finding  answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing  was necessary for more direct responses (50 bytes).  The TREC-8 Question Answering track was an initial effort to bring the benefits of large-scale evaluation to bear on a question answering (QA) task. The goal in the QA task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than find the a...},
  booktitle = {In {{Proceedings}} of {{TREC}}-8},
  date = {1999},
  pages = {77--82},
  author = {Voorhees, Ellen M.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/J9YHZSKR/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YVH3JDDD/summary.html}
}

@article{salton_state_1992,
  title = {The State of Retrieval System Evaluation},
  volume = {28},
  number = {4},
  journaltitle = {Information processing \& management},
  date = {1992},
  pages = {441--449},
  author = {Salton, Gerard},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/PBGZQGC5/Salton - 1992 - The state of retrieval system evaluation.ps;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/Y2SWIUTP/030645739290002H.html}
}

@incollection{Craswell2009,
  location = {{Boston, MA}},
  title = {Mean {{Reciprocal Rank}}},
  isbn = {978-0-387-39940-9},
  url = {https://doi.org/10.1007/978-0-387-39940-9_488},
  booktitle = {Encyclopedia of {{Database Systems}}},
  publisher = {{Springer US}},
  date = {2009},
  pages = {1703-1703},
  author = {Craswell, Nick},
  editor = {LIU, LING and ÖZSU, M. TAMER},
  doi = {10.1007/978-0-387-39940-9_488}
}

@article{wieting_towards_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.08198},
  primaryClass = {cs},
  title = {Towards {{Universal Paraphrastic Sentence Embeddings}}},
  url = {http://arxiv.org/abs/1511.08198},
  abstract = {We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community with the hope that they can serve as the new baseline for further work on universal sentence embeddings.},
  urldate = {2019-06-04},
  date = {2015-11-25},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,sentence embedding},
  author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ZQPG5DQG/Wieting et al. - 2015 - Towards Universal Paraphrastic Sentence Embeddings.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3KBUFXKV/1511.html}
}

@article{kiros_skip-thought_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.06726},
  primaryClass = {cs},
  title = {Skip-{{Thought Vectors}}},
  url = {http://arxiv.org/abs/1506.06726},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  urldate = {2019-06-05},
  date = {2015-06-22},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,sentence embedding},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/KSQ254IF/Kiros et al. - 2015 - Skip-Thought Vectors.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3AMAJL5Q/1506.html}
}

@article{robertson_understanding_2004,
  title = {Understanding Inverse Document Frequency: On Theoretical Arguments for {{IDF}}},
  volume = {60},
  issn = {0022-0418},
  url = {https://www.emeraldinsight.com/doi/full/10.1108/00220410410560582},
  doi = {10.1108/00220410410560582},
  shorttitle = {Understanding Inverse Document Frequency},
  number = {5},
  journaltitle = {Journal of Documentation},
  urldate = {2019-06-05},
  date = {2004-10-01},
  pages = {503-520},
  keywords = {Information theory,Modelling,Probabilistic analysis,Text retrieval,tfidf},
  author = {Robertson, Stephen},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/8MNSFFR9/Robertson - 2004 - Understanding inverse document frequency on theor.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/N42TCN3A/00220410410560582.html}
}

@inproceedings{navigli_babelnet_2010,
  location = {{Stroudsburg, PA, USA}},
  title = {{{BabelNet}}: {{Building}} a {{Very Large Multilingual Semantic Network}}},
  url = {http://dl.acm.org/citation.cfm?id=1858681.1858704},
  shorttitle = {{{BabelNet}}},
  abstract = {In this paper we present BabelNet -- a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource.},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  series = {{{ACL}} '10},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-06-05},
  date = {2010},
  pages = {216--225},
  author = {Navigli, Roberto and Ponzetto, Simone Paolo},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YEIZWDU9/Navigli and Ponzetto - 2010 - BabelNet Building a Very Large Multilingual Seman.pdf},
  venue = {Uppsala, Sweden}
}

@inproceedings{ruiz-casado_automatic_2005,
  title = {Automatic Assignment of Wikipedia Encyclopedic Entries to Wordnet Synsets},
  booktitle = {International {{Atlantic Web Intelligence Conference}}},
  publisher = {{Springer}},
  date = {2005},
  pages = {380--386},
  keywords = {wordnet},
  author = {Ruiz-Casado, Maria and Alfonseca, Enrique and Castells, Pablo},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ZLJ6I2TE/Ruiz-Casado et al. - 2005 - Automatic assignment of wikipedia encyclopedic ent.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/7UC5P56M/11495772_59.html}
}

@inproceedings{speer_conceptnet_2017,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972},
  eventtitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  date = {2017},
  pages = {4444-4451},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine}
}

@book{osgood_measurement_1957,
  langid = {english},
  title = {The {{Measurement}} of {{Meaning}}},
  isbn = {978-0-252-74539-3},
  abstract = {In this pioneering study, the authors deal with the nature and theory of meaning and present a new, objective method for its measurement which they call the semantic differential. This instrument is not a specific test, but rather a general technique of measurement that can be adapted to a wide variety of problems in such areas as clinical psychology, social psychology, linguistics, mass communications,  esthetics, and political science. The core of the book is the authors' description,  application, and evaluation of this important tool and its far-reaching implications for empirical research.},
  pagetotal = {358},
  publisher = {{University of Illinois Press}},
  date = {1957},
  keywords = {Psychology / General},
  author = {Osgood, Charles Egerton and Suci, George J. and Tannenbaum, Percy H.},
  eprinttype = {googlebooks}
}

@inproceedings{neale_survey_2018,
  langid = {english},
  title = {A {{Survey}} on {{Automatically}}-{{Constructed WordNets}} and Their {{Evaluation}}: {{Lexical}} and {{Word Embedding}}-Based {{Approaches}}},
  isbn = {979-10-95546-00-9},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  publisher = {{European Language Resources Association (ELRA)}},
  date = {2018-05-07/2018-05-12},
  author = {Neale, Steven},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/WJHLSRYB/Neale - 2018 - A Survey on Automatically-Constructed WordNets and.pdf},
  venue = {Miyazaki, Japan}
}

@article{grave_learning_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06893},
  primaryClass = {cs},
  title = {Learning {{Word Vectors}} for 157 {{Languages}}},
  url = {http://arxiv.org/abs/1802.06893},
  abstract = {Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.},
  urldate = {2019-06-06},
  date = {2018-02-19},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AWI86TU2/Grave et al. - 2018 - Learning Word Vectors for 157 Languages.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/LRZY2TP7/1802.html}
}

@inproceedings{klang_pairing_2016,
  title = {Pairing {{Wikipedia Articles Across Languages}}},
  url = {https://www.aclweb.org/anthology/W16-4410},
  abstract = {Wikipedia has become a reference knowledge source for scores of NLP applications. One of its invaluable features lies in its multilingual nature, where articles on a same entity or concept can have from one to more than 200 different versions. The interlinking of language versions in Wikipedia has undergone a major renewal with the advent of Wikidata, a unified scheme to identify entities and their properties using unique numbers. However, as the interlinking is still manually carried out by thousands of editors across the globe, errors may creep in the assignment of entities. In this paper, we describe an optimization technique to match automatically language versions of articles, and hence entities, that is only based on bags of words and anchors. We created a dataset of all the articles on persons we extracted from Wikipedia in six languages: English, French, German, Russian, Spanish, and Swedish. We report a correct match of at least 94.3\% on each pair.},
  booktitle = {Proceedings of the {{Open Knowledge Base}} and {{Question Answering Workshop}} ({{OKBQA}} 2016)},
  publisher = {{The COLING 2016 Organizing Committee}},
  date = {2016-12},
  pages = {72-76},
  keywords = {linear assignment},
  author = {Klang, Marcus and Nugues, Pierre},
  venue = {Osaka, Japan}
}

@article{barnard_matching_2003,
  title = {Matching {{Words}} and {{Pictures}}},
  volume = {3},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=944919.944965},
  abstract = {We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.},
  journaltitle = {J. Mach. Learn. Res.},
  urldate = {2019-06-06},
  date = {2003-03},
  pages = {1107--1135},
  keywords = {linear assignment},
  author = {Barnard, Kobus and Duygulu, Pinar and Forsyth, David and de Freitas, Nando and Blei, David M. and Jordan, Michael I.},
  options = {useprefix=true},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NUKSVYCY/Barnard et al. - 2003 - Matching Words and Pictures.pdf}
}

@inproceedings{sasaki_subword-based_2019,
  title = {Subword-Based {{Compact Reconstruction}} of {{Word Embeddings}}},
  url = {https://www.aclweb.org/anthology/N19-1353},
  abstract = {The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings. In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space. The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism. Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks.},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  publisher = {{Association for Computational Linguistics}},
  date = {2019-06},
  pages = {3498-3508},
  author = {Sasaki, Shota and Suzuki, Jun and Inui, Kentaro},
  venue = {Minneapolis, Minnesota}
}

@inproceedings{bhattacharyya_indowordnet_2010,
  title = {Indowordnet},
  abstract = {India is a multilingual country where machine translation and cross lingual search are highly relevant problems. These problems require large resources- like wordnets and lexicons- of high quality and coverage. Wordnets are lexical structures composed of synsets and semantic relations. Synsets are sets of synonyms. They are linked by semantic relations like hypernymy (is-a), meronymy (part-of), troponymy (manner-of) etc. IndoWordnet is a linked structure of wordnets of major Indian languages from Indo-Aryan, Dravidian and Sino-Tibetan families. These wordnets have been created by following the expansion approach from Hindi wordnet which was made available free for research in 2006. Since then a number of Indian languages have been creating their wordnets. In this paper we discuss the methodology, coverage, important considerations and multifarious benefits of IndoWordnet. Case studies are provided for Marathi, Sanskrit, Bodo and Telugu, to bring out the basic methodology of and challenges involved in the expansion approach. The guidelines the lexicographers follow for wordnet construction are enumerated. The difference between IndoWordnet and EuroWordnet also is discussed. 1.},
  booktitle = {In {{Proc}}. of {{LREC}}-10},
  date = {2010},
  author = {Bhattacharyya, Pushpak},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3BQA3CWX/Bhattacharyya - 2010 - Indowordnet.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/33JYNZND/summary.html}
}

@inproceedings{sand_wordnet_2017,
  langid = {american},
  title = {Wordnet Extension via Word Embeddings: {{Experiments}} on the {{Norwegian Wordnet}}},
  url = {https://aclweb.org/anthology/papers/W/W17/W17-0242/},
  shorttitle = {Wordnet Extension via Word Embeddings},
  eventtitle = {Proceedings of the 21st {{Nordic Conference}} on {{Computational Linguistics}}},
  urldate = {2019-06-07},
  date = {2017-05},
  pages = {298-302},
  author = {Sand, Heidi and Velldal, Erik and Øvrelid, Lilja},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/VKCFGBPF/Sand et al. - 2017 - Wordnet extension via word embeddings Experiments.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/DHW3JSAR/W17-0242.html}
}

@inproceedings{lam_automatically_2014,
  langid = {american},
  title = {Automatically Constructing {{Wordnet Synsets}}},
  url = {https://aclweb.org/anthology/papers/P/P14/P14-2018/},
  doi = {10.3115/v1/P14-2018},
  eventtitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  urldate = {2019-06-07},
  date = {2014-06},
  pages = {106-111},
  author = {Lam, Khang Nhut and Tarouti, Feras Al and Kalita, Jugal},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/GA26QK7Z/Lam et al. - 2014 - Automatically constructing Wordnet Synsets.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/MC9PNIGC/P14-2018.html}
}

@article{arora_linear_2018,
  langid = {english},
  title = {Linear {{Algebraic Structure}} of {{Word Senses}}, with {{Applications}} to {{Polysemy}}},
  volume = {6},
  issn = {2307-387X},
  url = {https://transacl.org/ojs/index.php/tacl/article/view/1346},
  abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 "discourse atoms" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.},
  number = {0},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  urldate = {2019-06-07},
  date = {2018-07-20},
  pages = {483-495},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/T8QRFSS5/Arora et al. - 2018 - Linear Algebraic Structure of Word Senses, with Ap.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/8IDJQSIM/1346.html}
}

@article{vossen_eurowordnet_2004,
  langid = {english},
  title = {{{EUROWORDNET}}: {{A MULTILINGUAL DATABASE OF AUTONOMOUS AND LANGUAGE}}-{{SPECIFIC WORDNETS CONNECTED VIA AN INTER}}-{{LINGUALINDEX}}},
  volume = {17},
  issn = {0950-3846},
  url = {https://academic.oup.com/ijl/article/17/2/161/969685},
  doi = {10.1093/ijl/17.2.161},
  shorttitle = {{{EUROWORDNET}}},
  abstract = {Abstract.  This paper describes the multilingual design of the EuroWordNet database. The EuroWordNet database stores wordnets as autonomous language-specific st},
  number = {2},
  journaltitle = {Int J Lexicography},
  urldate = {2019-06-08},
  date = {2004-06-01},
  pages = {161-173},
  author = {Vossen, Piek},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/BUE3MJ95/Vossen - 2004 - EUROWORDNET A MULTILINGUAL DATABASE OF AUTONOMOUS.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/PYXSZ8PM/969685.html}
}

@inproceedings{boyd-graber_adding_2006,
  title = {Adding Dense, Weighted Connections to {{WordNet}}},
  booktitle = {Proceedings of the Third International {{WordNet}} Conference},
  publisher = {{Citeseer}},
  date = {2006},
  pages = {29--36},
  author = {Boyd-Graber, Jordan and Fellbaum, Christiane and Osherson, Daniel and Schapire, Robert},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/487G7YYU/Boyd-Graber et al. - 2006 - Adding dense, weighted connections to WordNet.pdf}
}

@article{kitamura_cultural_2009,
  title = {Cultural Untranslatability},
  volume = {13},
  number = {3},
  journaltitle = {Translation Journal},
  date = {2009},
  author = {Kitamura, Kanji}
}

@inproceedings{diab_feasibility_2004,
  title = {The Feasibility of Bootstrapping an Arabic Wordnet Leveraging Parallel Corpora and an English Wordnet},
  booktitle = {Proceedings of the {{Arabic Language Technologies}} and {{Resources}}, {{NEMLAR}}, {{Cairo}}},
  date = {2004},
  author = {Diab, Mona}
}

@article{ercan_using_2007,
  title = {Using Lexical Chains for Keyword Extraction},
  volume = {43},
  number = {6},
  journaltitle = {Information Processing \& Management},
  date = {2007},
  pages = {1705-1714},
  author = {Ercan, Gonenc and Cicekli, Ilyas},
  publisher = {{Elsevier}}
}

@inproceedings{mueller_siamese_2016,
  title = {Siamese {{Recurrent Architectures}} for {{Learning Sentence Similarity}}},
  abstract = {We present a siamese adaptation of the Long Short-Term Memory (LSTM) network for labeled data comprised of pairs of variable-length sequences. Our model is applied to assess semantic similarity between sentences, where we exceed state of the art, outperforming carefully handcrafted features and recently proposed neural network systems of greater complexity. For these applications, we provide wordembedding vectors supplemented with synonymic information to the LSTMs, which use a fixed size vector to encode the underlying meaning expressed in a sentence (irrespective of the particular wording/syntax). By restricting subsequent operations to rely on a simple Manhattan metric, we compel the sentence representations learned by our model to form a highly structured space whose geometry reflects complex semantic relationships. Our results are the latest in a line of findings that showcase LSTMs as powerful language models capable of tasks requiring intricate understanding.},
  booktitle = {{{AAAI}}},
  date = {2016},
  keywords = {Artificial neural network,ENCODE,Language model,Long short-term memory,Semantic similarity,Taxicab geometry},
  author = {Mueller, Jonas and Thyagarajan, Aditya},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/BK9RKISF/Mueller and Thyagarajan - 2016 - Siamese Recurrent Architectures for Learning Sente.pdf}
}

@article{gers_learning_2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  volume = {12},
  doi = {10.1162/089976600300015015},
  shorttitle = {Learning to {{Forget}}},
  abstract = {Long short-term memory (LSTM; Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
  journaltitle = {Neural Computation},
  date = {2000},
  pages = {2451-2471},
  keywords = {Algorithm,Artificial neural network,Benchmark (computing),contents - HtmlLinkType,Decision problem,Languages,Linguistics,Long short-term memory,LSTM,Machine learning,Neural Network Simulation,Recurrent neural network,Relevance,Timer},
  author = {Gers, Felix A. and Schmidhuber, Jürgen and Cummins, Fred A.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NMSNKXSX/Gers et al. - 2000 - Learning to Forget Continual Prediction with LSTM.pdf}
}

@article{gers_learning_2003,
  title = {Learning {{Precise Timing}} with {{Lstm Recurrent Networks}}},
  volume = {3},
  issn = {1532-4435},
  url = {https://doi.org/10.1162/153244303768966139},
  doi = {10.1162/153244303768966139},
  abstract = {The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.},
  journaltitle = {J. Mach. Learn. Res.},
  urldate = {2019-06-09},
  date = {2003-03},
  pages = {115--143},
  keywords = {long short-term memory,LSTM,recurrent neural networks,timing},
  author = {Gers, Felix A. and Schraudolph, Nicol N. and Schmidhuber, Jürgen},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SVDFMXRR/Gers et al. - 2003 - Learning Precise Timing with Lstm Recurrent Networ.pdf}
}

@incollection{graves_long_2012,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Long {{Short}}-{{Term Memory}}},
  isbn = {978-3-642-24797-2},
  url = {https://doi.org/10.1007/978-3-642-24797-2_4},
  abstract = {As discussed in the previous chapter, an important benefit of recurrent neural networks is their ability to use contextual information when mapping between input and output sequences. Unfortunately, for standard RNN architectures, the range of context that can be in practice accessed is quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections. This effect is often referred to in the literature as the vanishing gradient problem (Hochreiter, 1991; Hochreiter et al., 2001a; Bengio et al., 1994). The vanishing gradient problem is illustrated schematically in Figure 4.1},
  booktitle = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  series = {Studies in {{Computational Intelligence}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-06-09},
  date = {2012},
  pages = {37-45},
  keywords = {LSTM},
  author = {Graves, Alex},
  editor = {Graves, Alex},
  doi = {10.1007/978-3-642-24797-2_4}
}

@article{hochreiter_long_1997,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number = {8},
  journaltitle = {Neural Comput.},
  urldate = {2019-06-09},
  date = {1997-11},
  pages = {1735--1780},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen}
}

@book{hochreiter_gradient_2001,
  title = {Gradient {{Flow}} in {{Recurrent Nets}}: The {{Difficulty}} of {{Learning Long}}-{{Term Dependencies}}},
  shorttitle = {Gradient {{Flow}} in {{Recurrent Nets}}},
  abstract = {Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take too much time to be feasible or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long. Although theoretically fascinating, they do not provide clear practical advantages over, say, backprop in feedforward networks with limited time windows (see crossreference Chapters 11 and 12). With conventional "algorithms based on the computation of the complete gradient", such as "Back-Propagation Through Time" (BPTT, e.g., [22, 27, 26]) or "Real-Time Recurrent Learning" (RTRL, e.g., [21]) error signals "flowing backwards in time" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error ex},
  date = {2001},
  author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, Jürgen},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AQPQYQM6/summary.html}
}

@article{bengio_learning_1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  volume = {5},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{$<>$}},
  number = {2},
  journaltitle = {IEEE Transactions on Neural Networks},
  date = {1994-03},
  pages = {157-166},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,learning (artificial intelligence),long-term dependencies,LSTM,Neural networks,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/65ECEXNV/Bengio et al. - 1994 - Learning long-term dependencies with gradient desc.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ZU2LSDGD/279181.html}
}

@article{pascanu_difficulty_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1211.5063},
  primaryClass = {cs},
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  url = {http://arxiv.org/abs/1211.5063},
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  urldate = {2019-06-09},
  date = {2012-11-21},
  keywords = {Computer Science - Machine Learning},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/A2PSQEJZ/Pascanu et al. - 2012 - On the difficulty of training Recurrent Neural Net.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/DG53RAJ8/1211.html}
}

@article{greff_lstm_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.04069},
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  volume = {28},
  issn = {2162-237X, 2162-2388},
  url = {http://arxiv.org/abs/1503.04069},
  doi = {10.1109/TNNLS.2016.2582924},
  shorttitle = {{{LSTM}}},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\$\textbackslash{}approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  number = {10},
  journaltitle = {IEEE Trans. Neural Netw. Learning Syst.},
  urldate = {2019-06-09},
  date = {2017-10},
  pages = {2222-2232},
  keywords = {68T10,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1,LSTM},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YIHLFQ88/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/BNQ6L9PJ/1503.html}
}

@article{sutskever_sequence_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.3215},
  primaryClass = {cs},
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  url = {http://arxiv.org/abs/1409.3215},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  urldate = {2019-06-09},
  date = {2014-09-10},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,LSTM},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3S3EI25D/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CYWGJ28R/1409.html}
}

@article{rumelhart_learning_1986,
  langid = {english},
  title = {Learning Representations by Back-Propagating Errors},
  volume = {323},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/323533a0},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  number = {6088},
  journaltitle = {Nature},
  urldate = {2019-06-09},
  date = {1986-10},
  pages = {533},
  keywords = {LSTM},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/APZZGL3Q/323533a0.html}
}

@inproceedings{kalchbrenner_recurrent_2013,
  title = {Recurrent {{Continuous Translation Models}}},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {$>$} 43\% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  booktitle = {{{EMNLP}}},
  date = {2013},
  keywords = {Experiment,Language model,LSTM,Perplexity,Recurrence relation},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/8JPKCFKG/Kalchbrenner and Blunsom - 2013 - Recurrent Continuous Translation Models.pdf}
}

@article{gers_lstm_2001,
  title = {{{LSTM}} Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages},
  volume = {12},
  issn = {1045-9227},
  doi = {10.1109/72.963769},
  abstract = {Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.},
  number = {6},
  journaltitle = {IEEE Transactions on Neural Networks},
  date = {2001-11},
  pages = {1333-1340},
  keywords = {Backpropagation algorithms,Bridges,Computational complexity,context-free language,context-free languages,context-sensitive language,context-sensitive languages,Delay effects,Hidden Markov models,learning (artificial intelligence),Learning automata,long short-term memory,LSTM,Neural networks,recurrent neural nets,recurrent neural networks,Recurrent neural networks,regular languages,Resonance light scattering,State-space methods},
  author = {Gers, F. A. and Schmidhuber, E.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/NAFKSTHZ/Gers and Schmidhuber - 2001 - LSTM recurrent networks learn simple context-free .pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/WWS7UUF3/963769.html}
}

@article{jean_using_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.2007},
  primaryClass = {cs},
  title = {On {{Using Very Large Target Vocabulary}} for {{Neural Machine Translation}}},
  url = {http://arxiv.org/abs/1412.2007},
  abstract = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English-{$>$}German translation and almost as high performance as state-of-the-art English-{$>$}French translation system.},
  urldate = {2019-06-09},
  date = {2014-12-05},
  keywords = {Computer Science - Computation and Language},
  author = {Jean, Sébastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/6WWLTU82/Jean et al. - 2014 - On Using Very Large Target Vocabulary for Neural M.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/GECE2XJY/1412.html}
}

@article{luong_addressing_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.8206},
  primaryClass = {cs},
  title = {Addressing the {{Rare Word Problem}} in {{Neural Machine Translation}}},
  url = {http://arxiv.org/abs/1410.8206},
  abstract = {Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.},
  urldate = {2019-06-09},
  date = {2014-10-29},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,LSTM},
  author = {Luong, Minh-Thang and Sutskever, Ilya and Le, Quoc V. and Vinyals, Oriol and Zaremba, Wojciech},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/YRJQY387/Luong et al. - 2014 - Addressing the Rare Word Problem in Neural Machine.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SNXLRPTP/1410.html}
}

@incollection{graves_unconstrained_2008,
  title = {Unconstrained {{On}}-Line {{Handwriting Recognition}} with {{Recurrent Neural Networks}}},
  url = {http://papers.nips.cc/paper/3213-unconstrained-on-line-handwriting-recognition-with-recurrent-neural-networks.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  publisher = {{Curran Associates, Inc.}},
  date = {2008},
  pages = {577-584},
  author = {Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, Jürgen and Fernández, Santiago},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.}
}

@article{graves_novel_2009,
  title = {A {{Novel Connectionist System}} for {{Unconstrained Handwriting Recognition}}},
  volume = {31},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2008.137},
  abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
  number = {5},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2009-05},
  pages = {855-868},
  keywords = {Algorithms,Automatic Data Processing,bidirectional long short-term memory,Character recognition,connectionist system,connectionist temporal classification,Connectionist temporal classification,Databases,Handwriting,handwriting recognition,Handwriting recognition,handwritten character recognition,hidden Markov model.,hidden Markov models,Hidden Markov models,Image Enhancement,Image Interpretation; Computer-Assisted,image segmentation,Information Storage and Retrieval,Labeling,language modeling,Long Short-Term Memory,Models; Statistical,offline handwriting,Offline handwriting recognition,online handwriting,Online handwriting recognition,overlapping character segmentation,Pattern Recognition; Automated,Reading,recurrent neural nets,recurrent neural network,recurrent neural networks,Recurrent neural networks,Reproducibility of Results,Robustness,Sensitivity and Specificity,Size measurement,Speech,Subtraction Technique,Text recognition,unconstrained handwriting databases,Unconstrained handwriting recognition,unconstrained handwriting text recognition},
  author = {Graves, A. and Liwicki, M. and Fernández, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/SE4V9W3S/Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/ABE6VF7K/4531750.html}
}

@article{graves_framewise_2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  volume = {18},
  issn = {0893-6080},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608005001206},
  doi = {10.1016/j.neunet.2005.06.042},
  abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright.},
  number = {5},
  journaltitle = {Neural Networks},
  series = {{{IJCNN}} 2005},
  urldate = {2019-06-10},
  date = {2005-07-01},
  pages = {602-610},
  keywords = {vanilla lstm},
  author = {Graves, Alex and Schmidhuber, Jürgen},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/EA8ZNM5Z/Graves and Schmidhuber - 2005 - Framewise phoneme classification with bidirectiona.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/BHADLVVL/S0893608005001206.html}
}

@article{zeiler_adadelta_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1212.5701},
  primaryClass = {cs},
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  url = {http://arxiv.org/abs/1212.5701},
  shorttitle = {{{ADADELTA}}},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  urldate = {2019-06-10},
  date = {2012-12-22},
  keywords = {Computer Science - Machine Learning},
  author = {Zeiler, Matthew D.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/CWKI9GR5/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/LR2UZLAE/1212.html}
}

@inproceedings{auer_dbpedia_2007,
  langid = {english},
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  isbn = {978-3-540-76298-0},
  shorttitle = {{{DBpedia}}},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  booktitle = {The {{Semantic Web}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2007},
  pages = {722-735},
  keywords = {Open Dataset,Relational Database Table,Sophisticated Query,SPARQL Endpoint,Triple Pattern},
  author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and Cudré-Mauroux, Philippe},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/3WJMDPXE/Auer et al. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf}
}

@inproceedings{anacleto_can_2006,
  langid = {english},
  title = {Can {{Common Sense}} Uncover Cultural Differences in Computer Applications?},
  isbn = {978-0-387-34747-9},
  abstract = {Cultural differences play a very important role in matching computer interfaces to the expectations of users from different national and cultural backgrounds. But to date, there has been little systematic research as to the extent of such differences, and how to produce software that automatically takes into account these differences. We are studying these issues using a unique resource: Common Sense knowledge bases in different languages. Our research points out that this kind of knowledge can help computer systems to consider cultural differences. We describe our experiences with knowledge bases containing thousands of sentences describing people and everyday activities, collected from volunteer Web contributors in three different cultures: Brazil, Mexico and the USA, and software which automatically searches for cultural differences amongst the three cultures, alerting the user to potential differences.},
  booktitle = {Artificial {{Intelligence}} in {{Theory}} and {{Practice}}},
  series = {{{IFIP International Federation}} for {{Information Processing}}},
  publisher = {{Springer US}},
  date = {2006},
  pages = {1-10},
  keywords = {Common Sense,Common Sense Knowledge,Cultural Difference,Cultural Knowledge,Semantic Network},
  author = {Anacleto, Junia and Lieberman, Henry and Tsutsumi, Marie and Neris, Vânia and Carvalho, Aparecido and Espinosa, Jose and Godoi, Muriel and Zem-Mascarenhas, Silvia},
  editor = {Bramer, Max},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/HSIDVBID/Anacleto et al. - 2006 - Can Common Sense uncover cultural differences in c.pdf}
}

@article{kuhn_hungarian_1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  volume = {2},
  number = {1-2},
  journaltitle = {Naval research logistics quarterly},
  date = {1955},
  pages = {83--97},
  author = {Kuhn, Harold W.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/QJPKDZM8/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/AYFDMLFW/nav.html}
}

@article{dijkstra_note_1959,
  langid = {english},
  title = {A Note on Two Problems in Connexion with Graphs},
  volume = {1},
  issn = {0945-3245},
  url = {https://doi.org/10.1007/BF01386390},
  doi = {10.1007/BF01386390},
  number = {1},
  journaltitle = {Numer. Math.},
  urldate = {2019-06-11},
  date = {1959-12-01},
  pages = {269-271},
  keywords = {lapjv,Mathematical Method},
  author = {Dijkstra, E. W.},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/83IUIL3F/Dijkstra - 1959 - A note on two problems in connexion with graphs.pdf}
}

@article{kantorovitch_translocation_1958,
  title = {On the Translocation of Masses},
  volume = {5},
  number = {1},
  journaltitle = {Management Science},
  date = {1958},
  pages = {1-4},
  author = {Kantorovitch, Leonid},
  publisher = {{INFORMS}}
}

@article{sinkhorn_concerning_1967,
  langid = {english},
  title = {Concerning Nonnegative Matrices and Doubly Stochastic Matrices.},
  volume = {21},
  issn = {0030-8730},
  url = {https://projecteuclid.org/euclid.pjm/1102992505},
  abstract = {Project Euclid - mathematics and statistics online},
  number = {2},
  journaltitle = {Pacific J. Math.},
  urldate = {2019-06-13},
  date = {1967},
  pages = {343-348},
  author = {Sinkhorn, Richard and Knopp, Paul},
  file = {/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/DIW4LPL3/Sinkhorn and Knopp - 1967 - Concerning nonnegative matrices and doubly stochas.pdf;/home/yigit/.zotero/zotero/p8h92vwx.default/zotero/storage/EJWH3LZN/1102992505.html}
}

@book{bird_natural_2009,
  langid = {english},
  title = {Natural {{Language Processing}} with {{Python}}: {{Analyzing Text}} with the {{Natural Language Toolkit}}},
  isbn = {978-0-596-55571-9},
  shorttitle = {Natural {{Language Processing}} with {{Python}}},
  abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.Packed with examples and exercises, Natural Language Processing with Python will help you:Extract information from unstructured text, either to guess the topic or identify "named entities"Analyze linguistic structure in text, including parsing and semantic analysisAccess popular linguistic databases, including WordNet and treebanksIntegrate techniques drawn from fields as diverse as linguistics and artificial intelligenceThis book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.},
  pagetotal = {506},
  publisher = {{"O'Reilly Media, Inc."}},
  date = {2009-06-12},
  keywords = {Computers / General,Computers / Programming Languages / General,Computers / Programming Languages / JavaScript,Computers / Programming Languages / Python,Computers / Software Development & Engineering / General},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  eprinttype = {googlebooks}
}

@inproceedings{artetxe_robust_2018,
  title = {A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  date = {2018},
  pages = {789-798},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko}
}

@inproceedings{artetxe_learning_2016,
  title = {Learning Principled Bilingual Mappings of Word Embeddings While Preserving Monolingual Invariance},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  date = {2016},
  pages = {2289-2294},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko}
}


